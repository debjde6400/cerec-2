A specification of the application's functional and nonfunctional requirements.
First, it is anticipated that the SRS will be used by the application designers.
Designers will use the information recorded here as the basis for creating the application's design.
Second, the client for the project, the library manager in our case, is expected to review this document.
The SRS will serve to establish a basis for agreement between the client and development team about the functionality to be provided by the application.
The purpose of this software development project is to create a new application called: DLS SYSTEM.
The client for this project wishes to enter the PC-based internet environment.
The Library Management System will be PC-base with a internet, allowing library users to search for books,seminars and library staff members to manage the book inventory and user database.
The application will be access via a internet on a PC at any place.
Library staff will be able to manage library user accounts including remove, change, and add.
Library staff will be able to manage the book inventory database including remove, change, and add.
The application will generate reports for administrative purposes.
The application will provide search function on books based on ISBN, subject, title, or author.
Provide additional flexibility and convenience to the library users.
Provide better reliability and security of the library information.
Provide a more productive environment for the library staff member.
The availability of information at any time in any place.
DLS SYSTEM is used for Library Manager, Librarian, and Library User.
However, it is possible to exchange data with other system through external interface if required.
The high level summary of functions in DLSSYSTEM System is described in the following concept map.
Detail functional requirements will be described in section 3.
The following table describe general users characteristics that will affect the functionality of the software product.
This system is Web based, there will be a need to provide PC Server hardware connected to the internet.
DLS System can potentially have more than hundreds of users.
It is unrealistic to provide training for everyone.
Therefore, the system should be designed for easy to use, providing help instructions, and appropriate error messages for invalid user inputs.
Library user is allowed to use the DLSSYSTEM only for searching book records.
User should never be able to break into the system and to perform any modification.
The DLSSYSTEM should not have any unscheduled down time during library operation hours.
Any down time in operation hours has significant impact to the operation and cause inconvenience to everyone in library.
The following is a list of assumptions and dependencies that would affect Users have basic understanding to PC and Windows and internet.
There is a method to convert all book records and library user records from the existing system into the DLSSYSTEM.
In this section, the users of "Search Book Record" are refereed to librarians and patrons (library users).
Interfaces are a critical class of components within the DML that will provide the means by which users interact with the system.
As such, all interfaces should provide easy access to help as well as clearly indicate the current state of the user’s transaction when the user isn’t idle.
Transaction and error status MUST be displayed within each interface component.
Cut and paste of text within interfaces and into and out of the interfaces MUST be supported.
Administrative interfaces will assist Library Staff in building/maintaining collections and controlling access to them.
Because of the complexity of the data model, Library Staff will need to be able to edit multiple records simultaneously and create links between them.
Administrative MUST be able to have multiple records open for editing Administrator MUST be able to create links (references) between records without needing to type in record identifiers.
Additionally data represented in the administrative interface may be in a differentstate than that stored in the repository.
For example, after a record has been edited, but before it has be en “saved” into the repository two versions of the record exist.
The interface should clearly indicate the state of the locally edited record relative to the version stored in the repository.
All editors MUST clearly indicate the state of the edited record (new,saved, and modified/not yet saved).
The system shall display the user account information including user ID, last and first name, and user position, privilege.
The system shall use a graphic user interface which allows librarians to choice actions including removing, changing and adding user account and account information..
Within the system, logging will be used to provide a trail of transactions that have taken place.
This might either be for developer debugging purposes, administrative checks on usage, or research on the usability of interfaces.
Transaction logs MUST be kept for each service provided.
Sufficiently detailed client session logs MUST be generated to SRS-005.
The user’s password MUST never be exposed to compromise.
SRS-006 User session logs stored for usability and other research MUST be anonymous.
SRS-008: When download the books, the system shall display the information of the e-book which is just being downloaded including: ISBN, title, location.
The system shall allow a user to enter his/her data via choose an item via a mouse.
SRS-011: Whenever the "date" data is needed, it shall be entered only by choose date from a online calendar.
The system shall allow the user to enter the library card number and ISBN both by typing or scanning.
The system shall allow the user to enter book borrowing, recalling data as frequently as required.
The system shall allow the user to add or change information in an account including: last name, first name, user ID, user position, user privilege.
SRS-016: the system shall allow the user to delete an entire account.
The system shall allow the user choose language option which the SRS-019: If the search result are a list of books, the system shall allow the user to choose any one of them to see the details.
SRS-021: the system shall allow the user to put "delete" for a existing e- book and specify the deleting reason.
The system shall have a report feature that will allow the user to generate a report showing the information of a particular patron.
The system shall have a report feature that will allow the user to generate a report showing the information of book purchase information in a period including the book titles, category, the author, the publisher, the price.
It also shall give statistic data about the total number of books purchased, the money paid by category.
The system shall be generate those reports to the display, a file or a printer which is linked to the system.
The system shall be installed in a windows-NT network.
The Patron information report shall be generated by users who have librarian account.
The book purchase report shall only be generated by managers or users with defined privileges.
SRS-7: Database update data shall be committed to the database only after the managers have approved.
The system shall be recovered within 10 minutes if it is down.
The system shall be recovered without intervention at user terminal if it is down.
The system shall show appropriate messages at terminal when system is down.
The system shall have 99% reliability during library operating hours.
SRS-012: Scheduled down time after library operating hours shall not be more than 1 hour per day.
The system shall generate error messages when the user attempts to enter invalid data.
System must be able to extend to store and deliver new content media types.
SRS-015 System must be able to extend to support synchronization of media based on shared work/item structure.
System MUST be able to extend to include music thesaurus in later versions.
System MUST be able to extend support to MMTT components built in later versions.
System MUST be able to extend to support data sharing between records.
SRS-019System MUST be able to extend to support more sophisticated bookmaking including additional context (e.g. size and configuration of viewer) and book marking of other record types.
Instructors will be using the system in collaboration with Concourse to provide access to course materials and content stored within the DML.
Library Staff will populate the repositories with both metadata records and media content.
They must necessarily be able to generate and modify metadata and content in records as well as create references between the records.
Additionally data represented in the administrative interface may be stored in a different database .
For example, after a record has been edited, but before it has be en “saved” into main database .
The RDB administrator, who will be able to select an existing word or add a new one (hence it is a less controlled list than the Subjects, suggests keywords.
This document includes the system requirements jointly defined by the project partners.
In this document, system requirements defined by the partners are presented.
System requirements are classified into two categories: funtional and non-functional system requirements.
Non-functional system requirements are classified into various categories: accessibility; accuracy; audit, control, and reporting; availability; backup and restore; capacity, current and forecast; certification; compliance; compatibility of software, tools, standards, platform, database; concurrency; configuration management; dependency on deployment; documentation; disaster recovery; efficiency (resource consumption for given load); effectiveness (resulting performance in relation to effort); error handling; exploitability; extensibility (adding features); failure management; interoperability; legal and regulatory; localizability; maintainability; modifiability; network topology; operability; performance/response time; privacy; portability; quality; recoverability; redundancy; reliability (mean time between failures); reporting; resource constraints (processor speed, memory, disk space, network bandwidth, etc.
System requirements are defined taking into account the user requirements.
Therefore “D1.1.2 - Use Case Scenarios and Requirements” is used during this task.
This deliverable wil be used during system design phase.
The urban spaces are full of stand-alone sensor based installations of different services designed according to their own purpose and requirements.
For example, the municipalities provide several services to the citizens: safety and security services of citizens in the streets, traffic management, etc.
These services rely mostly on street-implemented infrastructures, such as cameras, sensors, induction loops.
In addition, local businesses have their own illumination systems, advertising infrastructure including neon signs, public displays etc.
They also monitor customer behavior using various sensor systems.
The INSIST project proposes an integration of these sensor-based systems into a wider perspective.
The role of WP1 is to investigate the state-of-the-art and gather the requirements on the smart systems targeted by the INSIST project, and their development flow.
The main objective of the second activity (Activity 1.2: System requirements) of WP1 is the definition and the consistency evaluation of the generic INSIST system requirements, taking into account the use cases and user requirements which were determined in the first activity (Activity 1.1: Use case description and user requirements) of WP1 and were provided in the first deliverable (D1.1.2 - Use Case Scenarios and Requirements).
To achieve this objective, a set of system requirements for INSIST platform are defined and presented in this document.
These requirements will meet user requirements and platform’s general requirements.
System requirement is a requirement at the system level that describes a function or the functions which the system as a whole should fullfill to satisfy the stakeholder needs and requirements.
System requirements are expressed in an appropriate combination of textual statements, views, and non-functional requirements.
System requirements express the levels of safety, security, reliability which will be necessary [1].
Based on this definition, a set of system requirements are defined in WP1, taking into account the user requirements.
System requirements describe what the system shall do whereas the user requirements (user needs) describe what the user does with the system.
System requirements are classified as either functional or non-functional (supplemental) requirements in terms of functionality feature of the requirement.
A functional requirement specifies something that a user needs to perform their work.
For example, collecting traffic – related data and processing them is a functional requirement for INSIST project.
For example, a system may be required to enter and print cost estimates; this is a functional requirement.
Non-functional requirements specify all the remaining requirements not covered by the functional requirements.
The plan for implementing functional requirements will be detailed in the system design phase.
The plan for implementing non-functional requirements will be detailed in the system architecture phase.
System requirements are classified into three categories: (i) general system requirements, (ii) security requirements, (iii) initial capacity requirements.
General system requirements are composed of major capabilities, major system conditions, system interfaces, and system user characteristics.
Major system conditions contain of assumptions and constraints.
System interfaces describe the dependency and relationship requirements of the system to other enterprise and/or external systems.
System interfaces include any interface to a future system or one under development.
System user characteristics identify each type of user of the system by function, location, and type of device.
Aggregated number of users should be specified and the nature of their use of the system should be explained.
Security requirements are related to the security requirements for users of the system.
Initial capacity requirements should be specified for the system.
An initial estimation can be established using current data amounts, planned number of users, and estimated number of services.
General system acceptance criteria should be specified and agreed upon by the project partners and the potential customers who will be used to accept the final end product.
Requirement: Performance requirements should be defined.
Performance requirements should be specified and defined to measure the performance of the INSIST Platform.
A list of performance criteria of the platform should be defined.
These performance criteria will be used during the acceptance of the project output.
The performance of the platform will be measured by using simulation and instrumentation tools.
Obtained results will be included into performance analysis report of the platform.
The elapsed time between the request of traffic density at a specific location and the calculation of the value.
Quality criteria of the UIs of the INSIST Platform should be defined and quality of the UIs should be measured.
Quality of the UIs of the INSIST Platform should be measured and should meet required quality level.
Documentation, complexity, integration, and usability criteria will be considered during the measurement of the UIs.
Requirement: Software quality attributes should be defined.
Software quality attributes should be specified and defined to measure the quality of the INSIST Platform.
The quality charactreistics of the INSIST Platform should be specified.
These characteristics are the items which can be important to either platform customers or platform developers.
The important characteristics include adaptability, availability, reusability, robustness, testability, and usability.
Requirement: Operating environment should be described.
Operating environment in which the software will operate should be described.
Operating environment in which the software will operate should be described.
Operating environment requirements include the hardware platform, operating system and versions, and any other software components or applications.
Requirement: Limitations and contraints should be defined.
Limitations and contraints should be defined and the platform developers should be informed.
Any items or issues which will limit the options available to the platform developers should be described.
Requirement: User documentation should be prepared.
User documentation should be prepared for all types of the platform users.
User documentations include user manuals, on-line help, and tutorials should be prepared.
All documents should be delivered along with the platform or applications.
User types of the INSIST Platform and the main characteristics of the users should be defined.
Requirement: Assumptions and dependencies should be defined.
Assumptions and dependencies can affect the development process.
Platform developers and users should be informed about them.
Assumptions and dependencies which could affect the development of the system should be defined.
These could include third-party or commercial components, issues around the development or operating environment, and constraints.
Requirement: External interface requirements should be defined.
All interface requirements should be specified and defined.
External interface requirements include user interfaces, hardware interfaces, software interfaces, and communication interfaces.
User interface requirements such as sample screens, and GUI standards are the logical characteristics of each interface between the platform and the users.
The logical and physical characteristics of each interface between the platform and the hardware components should be defined.
Hardware components include the supported device types.
Requirements of the hardware interfaces contain required data and control interactions between platform and the hardware, and the communication protocols to be used.
The connections between the platform and the other used software components should be described.
The used software components include databases, operating systems, tools, libraries, and commercial components.
Name and the versions should be identified in the requirement document.
The requirements associated with any communications functions required by the INSIST Platform should be described.
These communication functions include web browser, network server communications protocols, and so on.
The communication standards (such as ftp, http) that will be used should be identified.
The initial capacity requirements should be specified.
The initial capacity requirements of the INSIST Platform should be specified.
The initial capacity requirements of the INSIST Platform should be specified.
The initial estimation will be established using current test data amounts, planned number of users, and estimated number of transactions.
The highest and lowest numbers of the transactions, and processing frequency expected usage will be estimated.
These will be used for capacity planning for storage and memory requirements for INSIST Platform.
Two different main user types are defined in INSIST platform.
Any kind of user type can be chose when setting up your users in the INSIST system.
A system user is a person who interacts with the INSIST system, through an interface to define the behaviour of the system and how the end users would interact with the system.
A maintenance user who has the role of Administrator can access the maintenance console of the INSIST project.
A maintenance user can develop applications on th INSIST platform, create / update the database and test the system.
A database manager performs the management functionalities, such as creation and maintenance of the database.
A database manager has also abilities to back up and restore, attach and detach, create and clone the database.
A developer can develop new applications or services for the INSIST platform.
A test user can run the test scenarios on the INSIST platform.
A data analysis user can access to the INSIST platform to analyze platform data and obtain results.
System admin user can access to all areas of the INSIST platform.
B2B users use INSIST Platform for their production and/or service process.
They need INSIST Platform's functionalities for their operational reasons.
The overall volume of B2B user's transactions is much higher than the volume of B2C user's transactions.
B2C users in INSIST Platform are all road users such as pedestrians and drivers.
A list of abbreviations used in this document is presented below.
The Co-ReSyF project will implement a dedicated data access and processing infrastructure, with automated tools, methods and standards to support research applications using Earth Observation (EO) data for monitoring of Coastal Waters, levering on the components deployed SenSyF (www.sensyf.eu).
The main objective is to facilitate the access to Earth Observation data and pre-processing tools to the research community, towards the future provision of future Coastal Waters services based on EO data.
Through Co-ReSyF‘s collaborative front end, even inexperienced researchers in EO will be able to upload their applications to the system to compose and configure processing chains for easy deployment on the cloud infrastructure.
They will be able to accelerate the development of high-performing applications taking full advantage of the scalability of resources available in the cloud framework.
The system’s facilities and tools, optimized for distributed processing, include EO data access catalogues, discovery and retrieval tools, as well as a number of pre-processing tools and toolboxes for manipulating EO data.
Advanced users will also be able to go further and take full control of the processing chains and algorithms by having access to the cloud back-end, and to further optimize their applications for fast deployment for big data access and processing.
The Co-ReSyF capabilities will be supported and initially demonstrated by a series of early adopters who will develop new research applications on the coastal domain, guide the definition of requirements and serve as system beta testers.
A competitive call will be issued within the project to further demonstrate and promote the usage of the Co-ReSyF release.
These pioneering researchers in will be given access not only to the platform itself, but also to extensive training material on the system and on Coastal Waters research themes, as well as to the project's events, including the Summer School and Final Workshop.
Define technical requirements for the system Tools, tracing the functional extent and feasibility defined in support of the Co-ReSyF Research Applications user stories.
Within the Co-ReSyF platform it can be identified two major components that support the operation of the research activities performed within the platform.
One component is the Framework, which is composed of all the things that support the environment where the applications are defined and executed, and the other component are the Tools which are things that can be used to build an application and to analyse/visualize the results of the application.
The Framework includes the Cloud back-end, which is the infrastructure that runs the applications in the cloud and is in charge of coordinating and creating the VMs for distributed processing and collection of input and output data.
The other part of the framework is related to the user interaction and it is the part that directly interfaces with the user, this includes the Front- end (GUI that provides the connection to all the platform functionalities) and the Expert Centre and Knowledge Base (wiki with relevant information for newcomers of the platform to start using it).
The Tools live within the Framework and are a set of executables or libraries that can be used by the researchers to build and manage their applications or handle the data.
It includes the Automated Orchestration which is a set of tools designed to configure and monitor the execution of the sequence of tasks that compose one application.
The Image Inter-calibration, Atmospheric corrections, Data Co-registration and Fusion and Other tools, which are tools used to process the data commonly used by several applications and provided in a default tool-kit available to all users of the platform.
Finally there is also a set of Visualisation tools, which are provided as default by the platform that allow the users to visualise and manipulate the data is commonly used data visualization tools (different from the main front-end data visualization provided with the platform).
This document focuses solely on the Tools part of the Co-ReSyF platform.
The Co-ReSyF platform is envisaged to help researchers analysing EO data.
One of the biggest advantages of using Co-ReSyF is when the data to be analyzed has a significant size, where the researcher will be able to access the cloud assets in order to process the data.
This means that the data is not being processed locally and it can also take some time before the processing is complete.
It then becomes important to have mechanisms that can provide feedback to the user of the current status of the processing and that also ensure that the processing is correctly executed.
ARGANS background in remote sensing image inter-comparison and inter-calibration derives from involvement in radiometric calibration of medium resolution optical multi-spectral sensors (DIMITRI (https://dimitri.argans.co.uk)) and involvement with MERIS match-up in-situ database (MERMAID (http://mermaid.acri.fr/home/home.php)).
Inter-calibration provide a practical means of identifying and correcting relative biases in radiometric calibration between instruments.
One approach to inter-calibration is to use a well-characterized instrument as a reference to compare with other instruments viewing the same target under near-simultaneous conditions.
But there are problems for coastal Type II waters since PICS tend to be land sites, e.g. desert, salt lakes, snow; and both Rayleigh and glint procedures requires low chlorophyl and low aerosol.
The coastal zone is dynamic and the marine signal measures by satellite sensors strongly affected by land contamination effects.
Another approach is vicarious ground-based calibration in which surface measurements and radiative transfer models are used to estimate theoretical at-sensor radiance and correct biases in measured Top-of-Atmosphere (TOA) radiances, provides absolute calibration enabling direct inter-calibration of sensor images with relatively high accuracy.
For this in-situ data is needed (http://mermaid.acri.fr/home/home.php)) it is expensive and labour intensive to acquire.
It also requires inversion of Radiative Transfer Models (RTMs) which is sensitive to climatological and atmospheric parameters.
A possible approximate solution would be to perform relative calibration between sensors based on a set of invariant sites: PICS, Rayleigh and sun glint; and generalise that the bias and coastal test site.
For Sentinel-2 Multi-spectral Imager (MSI) (https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi) these correction coefficients are not provided with the image metadata and would need to be generated within the framework.
This may result in significant uncertainty since the spectral signature at the coast is not invariant.
A better approach would be to directly compare sensors by matching up images of the AOI based on viewing angle and illumination and normalise the images based on the sensor spectral response function (SRF).
But this requires a long time series so that sufficient match-ups between images are available.
And it does not consider that cross-sensor wavelength variations in SRF that can lead to measurement discrepencies between sensors.
A compensation for differences in SRF can be made by integrating the SRF of the sensor with the spectral signature of the target, the Spectral Band Adjustment Factor (SBAF) but as stated before the dynamic nature of the coastal spectral signature complicates the characterisation of the reference sensor simulated TOA reflectance.
In the wider context of the strategic fit to the framework it is important to be able to combine data from multiple satellite sensors and data from the same satellite sensor acquired over a period of time.
It is possible to create a composite image simply by normalising the image set using an image manipulation package but if the aim is to derive geophysical measurements care is needed to ensure inter-calibration considers the differing spectral response of the sensors, the observing and environmental conditions, and the spectral signature of the target.
In dynamic and heterogenous coastal zones absolute calibration and sensor inter-comparison is a challenging problem.
Inter-calibration, e.g. normalizing multiple images to a common radiometric scale, is distinct from sensor calibration, a process performed during Ground Segment processing and monitoring to ensure baseline data quality requirements are met.
Implicitly that quantifying and correcting relative bias between monitored and reference sensors for collocated data can be generalised to measurements by the same pair of satellites even when they are not being directly compared.
Geometrical calibration will have been performed on data retrieved from repositories.
Atmospheric correction will be provided by the framework [see CORESYF-TOOL-ATCORR].
A reference standard has been identified i.e. a stable well-calibrated satellite instrument that provide temporal, spatial, spectral and geometric collocation with the sensors to be inter-calibrated or an appropriate stable pseudo-invariant calibration site (PICS) or a well- characterised instrumented in situ site.
From a series of data from a reference sensor and target sensor(s) derive calibration coefficients to align the radiometry of the target sensors with the reference sensor.
By re-scaling all sensors to the same radiometric scale a consistent set of acquisitions over a given site, covering a broad range of geometries, is generated.
Absolute calibration of images is probably out-of-scope considering the resources allocated to this tool (5 months) and the requirement for a reasonably long time series of Sentinel-2 data to ensure there are a significant number of matchup images sharing geometrical and temporal viewing and illumination characteristics.
If there is a necessity for inter-calibration of SAR images input is required from another partner with the relevant knowledge.
Detection using remote sensing is based on geometry and radiometry.
Using radiometry often requires to apply a solid atmospheric correction algorithm, especially regarding water colour as the signal is quite low.
Indeed, up to 90% of the signal sensed by the satellite can be related to the atmosphere so it is primordial to correct that in order to clear any noise affecting the water colour.
For the second version of the Co-ReSyF platform it is envisaged to explore the potential of synergies between the different applications.
In order to be able to exploit these synergies, the applications that will combine the data from other applications, will need generic tools for co- registration and fusion of the data.
For the development of the Co-ReSyF applications several tools are identified as needed for the data processing that are useful to many other applications and are generic enough to be applied to a certain type of data in many use cases.
When analysing the data, either output or input data, the users may need some tools for data visualization and manipulation that are generic enough to consider them as useful to all users of the platform.
These tools are then identified as being tools for visualization that should be included in the Co-ReSyF default tool-kit.
The USDA and Agricultural Research Service (ARS) have supported hydrologic research since the managed and made available independently at each research location, greatly reducing the accessibility and utility of these data for policy-relevant, multi-site analyses.
The ARS is developing and implementing a data system to organize, document, manipulate and compile water, soil, management, and economic data for assessment of conservation practices.
While primary responsibility for data will continue to reside at individual watersheds, the new data system will provide one-point access to data from the sites in well-documented, and standardized formats.
The purpose of this document is to describe the technical and operation requirements that meet the needs of the CEAP Watershed Assessment Studies, as well as additional needs of researchers at the watershed sites and diverse outside users, in adequate detail to provide the basis for the system design.
The system will be called STEWARDS: Sustaining the Earth's Watersheds – Agricultural Research Data System.
The system requirements specification document describes what the system is to do, and how the system will perform each function.
The audiences for this document include the system developers and the users.
The system developer uses this document as the authority on designing and building system capabilities.
The users review the document to ensure the documentation completely and accurately describes the intended functionality.
This version – version 1.0 - provides general descriptions of the system.
The system developer should review the document to ensure there is adequate information for defining an initial design of the system.
The users should review the document to affirm the features described are needed, to clarify features, and to identify additional features needed within the system.
The next version – version 2.0 – will be the result of more detailed requirements analysis.
When version 2.0 is written, the system developer and users will be asked to review this document.
The document is structured to follow IEEE 830-1998 standards for recording system requirements.
As part of the Conservation Effects Assessment Project (CEAP), twelve ARS Benchmark Watersheds will support watershed-scale assessment of environmental effects of USDA conservation program implementation.
The ARS Benchmark Watersheds represent primarily rainfed cropland, although some of the watersheds also contain irrigated cropland, grazingland, wetlands, and confined animal feeding operations.
Three additional ARS watersheds may be added in the future to represent additional land uses.
Conservation practices to be emphasized will include NRCS CORE 4 practices for croplands (conservation buffers, nutrient management, pest management, and tillage management), drainage management systems, and manure management practices.
Environmental effects and benefits will be estimated primarily for water and soil resources, with some assessment of wildlife habitat and air quality benefits in some watersheds.
The goal for the watershed-scale research is to provide detailed assessments of conservation practices and programs in a selected watershed, provide a framework for improving the performance of the national assessment models, and support coordinated research on the effects of conservation practices across a range of resource characteristics (such as climate, terrain, land use, and soils).
The basic requirement for STEWARDS is to deliver consistent high quality data in support of the CEAP watershed-scale assessment from a one-stop data portal to the CEAP clients and, in time, the public at large.
The data system consists two main parts -- a central database management system for the storage and management of data, and a client application to allow users access and interact with the data.
The data stored in the database can be viewed and downloaded using graphical and tabular interface tools.
The data system will serve as a repository where diverse end-users can access, search, analyze, visualize, and report various types of integrated watershed data contributed from the multiple locations.
Types of data will be diverse, including biophysical data (i.e., point-based in time and/or space, spatially variable data, time series), data about land use, management, and conservation practices; and economic data.
Where applicable, data from the NRCS, Economics Research Service (ERS) Agricultural Resources and Environmental Indicators database, other ERS sources of data on costs of conservation practices, and land use and management data of NASS or other agencies will be utilized.
The intention of the team is that ARS researchers at watershed locations retain primary responsibility for data collection and management.
Sites may chose to retain existing data management protocols or change location protocols to those of STEWARDS.
However, STEWARDS must have the capability to translate data from location-specific formats into the standardized STEWARDS formats.
Data on STEWARDS will not be available to the public in real time.
Access to real-time data will remain at the discretion of each location research team.
Instead, annual or more frequent updates from location databases to STEWARDS will be made by the watersheds’ staffs after the locations have completed their quality assurance procedures.
Source data for this system are the long-term ARS watersheds, with those participating in the CEAP project expected to participate in this data system first.
The data system consists two main parts -- a central database management system for the uploading, storage, and management of data, and a client application to allow users access and interact with the data.
Diverse end-users can access, search, analyze, visualize, download, and report various types of integrated watershed data contributed from the multiple locations.
Types of data will include biophysical data (i.e., point-based in time and/or space, spatially variable data, time series), data about land use, management, and conservation practices; and economic data.
This includes the role commonly thought of as a Database Administrator (DBA).
UCC-4: ARS users: ARS scientists, engineers, and support staff who require ARS watershed data for research purposes.
These would likely be conversant with the general characteristics of the data content, and to a lesser extent have abilities manipulating views of the data using database tools such as envisioned for this database system.
These users will need user/password authentication to enable user settings management and to have access to data protected through the confidentiality agreements among agencies participating in CEAP.
These would have similar characteristics and needs as ARS users, but because of the agency-level confidentiality issues, could not access the password-protected sensitive data.
These users would have access to the public data without need for authentication.
The rule for selecting hardware and software is that the components/application must be functionally efficient, capable of interfacing with other software, and easy to maintain.
The Data Management portion of the system shall permit user access from the corporate Intranet and, if a user is authorized for outside access through the corporate firewall, from an Internet connection at the user’s home.
The goal of the web application is to be platform independent on the client side wherever possible.
Therefore, the web applications will be implemented to run on the server side as much as possible.
Also, it is required to test the application using different platforms, connection speeds, screen settings, colors/graphics, and browsers.
CO-3: Data uploading and management will likely be done as infrequently as once a year.
This fact greatly limits the complexity of the user interface and the learning curve needed for completing this task.
The testing plan will be based on user roles, modules or use cases, required tasks and expected outcomes.
User documentation will consist of the several components usually expected of a modern web-based software application, including a tutorial, help pages, FAQ’s with an online request form, and a complete user’s manual.
Frequently asked questions will be screened for the FAQ pages.
It is anticipated that each watershed location will provide human and fiscal resources required to prepare metadata files, data files, work with the data team on initial import of data, perform subsequent data uploads, and provide site-specific support to users of the system.
If a location has inadequate resources, the minimal data required for CEAP analyses will be identified and the data team will provide additional support to that location.
This may result in a location’s data coming into the system later than scheduled.
Team leaders anticipate availability of the operational platform at the ARS OCIO level.
Pilot and, as a contingency, operational data systems may be developed and maintained at El Reno or other research locations.
It is anticipated that funding from NRCS will partially support this activity through FY07.
If that funding were not available or insufficient in future years, base funds at the locations and/or discretionary funds from the Area and Headquarters funds would support the activity but the timelines would be adjusted.
Help desk funding will be determined from staffing needs identified during the design phase and a proposal will then be made to NPS.
The standardized data structure will facilitate database development, storage, and maintenance; metadata development, storage, and maintenance; and support database functionality, including tabular and spatial data queries, evaluation, visualization, and transfer (downloading) to off-site locations.
The database management system should be stable and secured, have high performance (in terms of speed and efficiency/effectiveness), and be easy to maintain.
This component is central to the effort and is thus of highest priority.
The team will elicit user requirements from a cross-section of user class members (see section 2.3) to accurately and completely describe the expected uses and functionality of the system.
To elicit requirements, the user class members will be asked via conferencing, interviews, and email, to provide a detailed description of the user actions and system responses that will take place during execution of the use case under normal, expected conditions.
The summation of responses will lead to an accurate and complete inventory of user requirements.
Establish a database in support of CEAP that will house the collective data assembled or generated during research activities.
The database will support a variety of data types and formats, including but not limited to: spatial data - vector, raster, imagery, and tabular; tabular data – static and time-series; spreadsheets; documents; reports; photographs; and URL links.
The use of agricultural models (SWAT, AnnAGNPS) is a core element of the CEAP effort.
Several modeling-related database topics will need to be addressed: Maintenance and reporting of uncertainty or error in measured data, in spatial components of GIS data, and in modeling output.
Reporting of uncertainty may take place on an individual sample basis, on a method or procedural basis, or on an entire database.
Models will require specific input data and will generate output data during the calibration and validation phases, sensitivity analyses, and exploratory scenarios.
The current scope of the STEWARDS system is to provide measured data for input to the model and for comparison to output.
FR-2.1: View entire universe of watersheds from top-level screen for selection Design tools to navigate the individual watershed sites and their data.
Summary descriptions of research watersheds, stations, and instruments that are useful in describing the research activities should be accessible.
This information would not be considered ‘formal’ metadata.
Provide access to the data via browsing of sites, stations, and instruments; allow for simple queries to individual datasets; provide a metadata search tool to query dataset parameters; and allow for downloading of datasets (full or partial).
Users may wish to examine time-series data, e.g. stream discharge data, over a user-specified time frame in order to select only those data desired for download.
Charting tools in association with the query engine are desirable.
Provide access to CEAP-related reports, tables, and project documents.
Agricultural research data are inherently spatial in nature.
STEWARDS will provide web-based geographic information tools to allow site-specific data to be viewed within their spatial context.
These tools will provide browse and query functionality and support links to download spatial data and their associated tabular data.
Much of the initial effort in CEAP will focus on the use of agricultural models (e.g. SWAT, AnnAGNPS) in the CEAP watersheds.
Users will be able to examine the data used in the modeling effort, visualize the results in their spatial context, and download the model input data and results.
Develop and maintain a metadata database that provides browse and query access to formal descriptions of the database elements.
Provide tools for database contributors to create and upload metadata compatible with the STEWARDS metadata database.
Develop search tools that query watershed, station, and instrument metadata across the STEWARDS database.
Metadata query tools will support query by location, theme, and keyword.
The STEWARDS database will maintain a metadata report for each database in STEWARDS.
The metadata of individual datasets should be created at the local watershed sites for local use and for populating a CEAP metadata database.
Adopt a metadata standard that is compliant with Federal regulations.
The current Federal standards for spatial data are the Content Standard for Digital Geospatial Metadata (version 2.0), (http://www.fgdc.gov/metadata/contstan.html), FGDC-STD-001-1998.
There are some indications that the Federal government may implement the ISO 19115 geographic information/metadata standard in the future.
If this occurs, the CEAP database team will evaluate conversion of metadata.
Implement a metadata input tool that supports creating and editing of metadata in the chosen format.
The metadata tool will allow for local and web-based input and editing.
A likely scenario involves a web-based wizard which allows users to specify particulars of a data set, and then save those answers and relate them to a particular project, so that these selections can be re-used in future uploading operations.
Implement a web-based browse-and-query tool for searching STEWARDS metadata.
Queries by location, theme, and keyword will be supported.
A likely scenario involves a web-based wizard which allows users to specify particular querying parameters and then save them for reference later such that time-based comparisons are facilitated with minimal user training.
Implement a database of STEWARDS metadata elements.
The database will provide browse and query functionality.
To make data electronically available any place and any time through a web server-client service, the web service will be platform independent as much as possible.
Where there is an exception, the software/hardware requirement should be specified for a particular application.
Users whose computing environment precludes downloading the data via the internet will have the opportunity to acquire data, reports, model results and other STEWARDS information via alternative media.
The system needs to be able to store hundreds of megabytes of data on demand.
The potential for needing Gigabytes of data storage capacity is also in the realm of possibilities.
Further requirements gathering is needed to get real-world estimates of such data storage needs.
HR-1.2: Near-line Storage – All User and Application data as well as software installation and configuration files must be fully backed up week-nightly.
Further, secure, off-site storage will be performed on a weekly basis with 24-hour retrieval times.
HR-2.1: Load Balancing - Application Servers and Database servers must be load balanced at the application level to ensure maximum stability and availability.
Specific scenarios, such as fail-over versus session-managed load balancing need to be addressed in the functional requirements specification.
The system requires a “back-end” private network environment in order to ensure that end-user operations (on the front-end) are not impeded by database backups, index propagations, or other large back-end data transfers.
The system will use, where appropriate, the standard hardware and data communications resources provided by the ARS OCIO at the ARS George Washington Carver Center in Beltsville, MD.
This includes, but is not limited to, the general Ethernet network/T1 connection at the server/hosting site, network servers, and network management tools.
SR-2: HTTP and GIS Server Applications – As the Web will be the primary delivery protocol for the application, HTTP and related GIS server applications will be required to support system functionality.
The use of Browser plug-ins will be judiciously restricted on an as-needed basis.
The system will use, where appropriate, the standard software resources provided by the ARS OCIO.
This includes, but is not limited to, MS ASP/Java Scripts, C++/C#, MS SQL server and IIS, or the use of PHP/Perl, JB scripts, C++/C#, MySQL server, and Apache server.
Data will be subject to quality assurance prior to either initial or recurring uploading to STEWARDS.
Initial population of STEWARDS will require that transition filters be developed jointly between the STEWARDS team and the individual watersheds.
These filters will provide the same function for recurring, probably annual, uploads.
Local watershed staff will be responsible for performing necessary quality assurance/quality control (QA/QC) procedures prior to data being uploaded to the STEWARDS database.
Measurement methods and QA/QC procedures established by the CEAP Quality Assurance Team will be used by local data producers and recorded in the data provided to STEWARDS.
The purpose of quality assurance is intended to provide added assurance of data integrity.
Quality assurance checking will be carried out at two different levels – local site level and central site level.
First, the guidelines for quality assurance, including parameter-specific ranges, threshold values, quality flags etc., will be determined.
Quality assurance checking at the local sites may be performed before or during the standard exchange file conversion phase.
A general quality assurance checking, not as specific as at the local sites, will be carried out during the data uploading stage.
Note that a) the primary responsibility for data quality assurance rests with the individual sites and b) the quality assurance ranges, criteria, etc., used here should be consistent with the quality assurance information specified in metadata (and may be a component of the data dictionary).
Basic information regarding individual watersheds will be made available to STEWARDS by local watershed sites.
Basic information may include spatial data in a GIS-compatible format; descriptive data about stations, instruments, methods, or procedures; reports; modeling results; and other data.
Data will be reformatted to be compatible with the STEWARDS data schema and CEAP modeling requirements.
Watershed data to be used in modeling that does not conform to model requirements will be processed in STEWARDS for inclusion in the STEWARDS databases.
As part of the initial population of the database, time-series data (measurement data, annual land use information, weather/climate data) that has been collected by local watershed staff will be processed through local and CEAP QA/QC procedures and uploaded to the STEWARDS database by watershed staff per obligations to CEAP Objective 1.
Protocols for data imports from local sites via input files with different formats will be developed and implemented jointly between the STEWARDS team and the local watersheds.
The standard exchange file format will have been jointly determined first.
A single import filter will be written to parse the standard formatted (e.g. tab-delimited) files from the local watershed sites.
In this manner, all the data files from the local sites will have consistent formats when they are stored in the central site.
The filter developed for the intial population of the database will be available for use in later periodic uploads.
Time-series data (measurement data, annual land use information, weather/climate data) and other data (spatial data, metadata) will be collected by local watershed staff, processed through local and CEAP QA/QC procedures, and uploaded to the STEWARDS database on an annual basis using the filter developed during the initial population of the database.
Requests will initiate from the central system per schedule negotiated with individual watersheds.
Compliance by watershed staff is understood to be expected per obligations under CEAP watershed project plans revised to accommodate the CEAP project plan objective 1.
The central database will allow users to access the data of individual watersheds, sub-watersheds on a station-by-station basis, or a collection of stations if the entire dataset is desired.
Multiple levels of data aggregation of time-series data (daily, monthly, or yearly) will be supported.
A great flexibility of export data should be also available in terms of data volume (the number of data entity and time coverage) and output format [for example, for tabular data – comma separated values (csv) or database format (dbf); for spatial data – shape files (shp) or ascii grid files (grd); for reports – Adobe page definition format (pdf)].
The user interface will be simple and consistent, using terminology commonly understood by the intended users of the system.
The system will have a simple interface, consistent with industry standard interfaces, to eliminate the need for user training of infrequent users.
The STEWARDS team will evaluate the user interface of similar systems and apply appropriately.
For additional details see Appendix E. User testing will be used to ensure the user interface is clear (simple, commonly understood vocabulary, intuitive to use without training), complete (users can perform all functions from the interface), and consistent (buttons and wording are the same throughout the system).
The system will use the standard hardware and data communications resources provided by the ARS OCIO at the ARS George Washington Carver Center in Beltsville, MD.
This includes, but is not limited to, the general Ethernet network/T1 connection at the server/hosting site, network servers, and network management tools.
This system will include a warning message when a low transmission speed is detected, and a non-graphical interface option will be available.
The system will use the standard software resources provided by the ARS OCIO.
This includes, but is not limited to, MS ASP/Java Scripts, C++/C#, MS SQL server and IIS, or the use of PHP/Perl, JB scripts, C++/C#, MySQL server, and Apache server.
The system will use the communications resources provided by the ARS OCIO.
This includes, but is not limited to, HTTP protocol for communication with the web browser and the web server and TCP/IP network protocol with HTTP protocol.
Response times seen by end users for querying metadata should be on the order of a few seconds or less.
Response times seen by end users for retrieving the actual images may take much longer, anywhere from a few minutes to several hours.
If the user requests a large image with a short response time, it is acceptable for the ARS data system to advise the user that the target response time cannot be met.
In that case, the person would be referred to an alternate method of getting the data.
Data on the server should be protected from power loss but data in transit from server to requester could be lost.
Given that these data will also remain on the watershed site system, rather than expend resources to prevent this loss, such failures will be monitored and the uploading process will be repeated.
STEWARDS will have reasonable controls consistent with ARS OCIO practices, in compliance with agency, Dept and Govt regulations.
STEWARDS security requirements will have four primary components.
They are authentication, confidentiality, integrity, and availability.
STEWARDS will follow industry best practices for authentication, using single-sign-on systems like Microsoft Active Directory to perform authentication.
Authentication addresses security requirements to ensure those using system are who they say they are.
This is of greatest concern when data are being changed or updated.
This is primarily done through userids and passwords.
Confidentiality security requirements describe the need to protect the data appropriately.
STEWARDS will use the user classes described in section 2.3 above to define boundaries of information sharing to ensure confidentiality as appropriate.
Any data that should be viewed by a restricted audience must be protected with appropriate security features.
The integrity of STEWARDS data will be critical to its success as a product.
Scientific research and publications will be based on the data obtained through the system.
Therefore, extensive data validation and review will be performed both before data are uploaded to the system and as part of the upload process.
The system will need policy and procedures protecting the data from intentional or unintentional modifications, and to ensure accurate data are made available.
The fourth consideration for security requirements is availability.
The system must be available to the intended audience 24 hours per day, 7 days a week with, 99% availability and a tolerance of -5% (not less than 50% of working hours in any week).
For this system, availability will be concerned with the reliability of the software and network components.
Intentional “denial of service attacks” is not foreseen as a significant concern.
This database will be built for a particular system and may not be portable but results to queries will be portable between many environments.
Implementation of the application software/code and design of database structure should be flexible enough for the necessary change in the later phase.
Availability is defined here to mean the ability to use the system during its intended period of operation as defined in SCR-4 above.
ArcGIS - ArcGIS is an integrated collection of GIS software products for building a complete GIS developed by ESRI.
The procedure (essentially approval) used by the approval authority in verifying that specification content is acceptable.
Authentication does not imply acceptance or responsibility for the specified item to perform successfully.
Benchmark watersheds - is used to differentiate the larger scale ARS watersheds from field scale ARS research activity or from other non-ARS watersheds.
C++ - "C plus plus" is a programming language that was built off the C language.
Core 4 - NRCS's Core 4 is a common-sense approach to improving farm profitability while addressing environmental concerns.
The approach is easily adaptable to virtually any farming situation and can be fine tuned to meet your unique needs.
The net result is better soil, cleaner water, greater on-farm profits, and a brighter future for all of us.
Database - A collection of related data stored in one or more computerized files in a manner that can be accessed by users or computer programs via a database management system.
Database management system - An integrated set of computer programs that provide the capabilities needed to establish, modify, make available, and maintain the integrity of a database.
Data dictionary - A collection of data definitions, each identified by the name of a piece of data, describing the meaning and purpose of that piece of data in the system, the type of the data, its components and any other relevant attributes (range, precision, storage size and so on).
Data Schema - A data schema is a grammar that describes elements and their types, attributes and possible relations.
Functional requirement: A statement of a piece of required functionality or a behavior that a system will exhibit under specific conditions.
These include inputs, outputs, calculations, external interfaces, communications, and special management information needs.
Functional requirements are also called behavioral requirements because they address what the system does.
Intranet - An intranet is an internal or private Internet used strictly within the confines of a company, university, or organization.
JavaScript - A programming language designed by Sun Microsystems, in conjunction with Netscape, that can be integrated into standard HTML pages.
Metadata -- "data about data" describe the content, quality, condition, and other characteristics of data.
Module – (1) In software, a module is a part of a program.
A single module can contain one or several routines.
Performance - A quantitative measure characterizing a physical or functional attribute relating to the execution of a mission/operation or function.
Performance requirement -- A system/software system requirement specifying a performance characteristic that a system/software system or system/software component must possess; for example, speed, accuracy, and frequency.
Portability - (1) A term used to describe an object that can be easily moved, such as a portable computer; (2) When referring to computer software, portability refers to how easy a software program can be moved between computer Operating Systems.
Software requirement – (1) A software capability needed by a user to solve a problem to achieve an objective; (2) A software capability that must be met or possessed by a system or system component to satisfy a contract, standard, specification, or other formally imposed document.
System - A composite of equipment, skills, and techniques capable of performing or supporting an operational role or both.
A complete system includes all equipment, related facilities, material, software, services and personnel required for its operation and support to the degree that it can be considered a self-sufficient item in its intended operational environment.
System Requirement - A condition or capability that must be met or possessed by a system or system component to satisfy a condition or capability needed by a user to solve a problem.
Use cases - A task analysis technique often used in software engineering.
For each module of a system, common tasks are written up with the prerequisites for each task, the steps to take for the user and the system, and the changes that will be true after the task is completed.
Use cases are especially useful for making sure that common tasks are supported by the system, that they are relatively straightforward, and that the system architecture reflects the task structure.
User interface – A user interface is what you have to learn to operate a machine.
For examples, the graphical user interfaces (GUIs) -- windows, icons, and pop-up menus have become standard on personal computers.
User requirements - address what the users need to do their jobs.
SWAT - Soil and Water Assessment Tool, a river basin, or watershed, scale model developed by Dr. Jeff Arnold for the USDA Agricultural Research Service (ARS).
SWAT was developed to predict the impact of land management practices on water, sediment and agricultural chemical yields in large complex watersheds with varying soils, land use, and management conditions over long periods of time.
AnnAgNPS - AGNPS is a tool to help evaluate the effect of management decisions impacting a watershed system.
The AGNPS system is a direct update of the AGNPS 98 & 2001 system of modules containing many enhancements.
The term "AGNPS" now refers to the system of modeling components instead of the single-event AGNPS, which was discontinued in the mid-1990's.
These enhancements have been included to improve the capability of the program and to automate many of the input data preparation steps needed for use with large watershed systems.
New to AnnAGNPS Version 3.42 are many minor enhancements to algorithms and more output options.
The AGNPS Arcview interface has been better integrated with the components needed to develop AnnAGNPS datasets.
The capabilities of RUSLE, used by USDA-NRCS to evaluate the degree of erosion on agricultural fields and to guide development of conservation plans to control erosion, have been incorporated into AnnAGNPS.
This provides a watershed scale aspect to conservation planning.
This section is reserved for open requirements issues that remain to be resolved, including TBDs, pending decisions, information that is needed, conflicts awaiting resolution, and the like.
The pages will be designed for screen resolution of 800 x 600 pixels as a minimum.
The title tag and the Meta description tag in the web pages will be carefully constructed to include the focused keywords so these keywords will be highly visible in the search engine results pages, making the STEWARDS product more useful to the general users after the public rollout.
Completion/Confirmation messages should be displayed when the application processes the data successfully.
Messages generated shall be clear, succinct, and free of jargon.
The purpose of system design is to translate the system requirements into more technical specifications.
Through a user case survey and a logical analysis of user’s expectations and system functionalities, the system components for STEWARDS are identified (Fig.
System design provides a detailed description for each identified component and produces a physical data system design (with documentation) that allow the system developers to construct each component (layer) accordingly.
At the end of the system design phase, hardware, software tools and skill requirements, and other resources will be identified and a system developer team/plan will be drawn.
After that, the construction phase will begins (Task flow of can be seen in Fig.
The system design process can be divided into three main phases: conceptual system design, logical system design, and physical system design.
In this appendix, only conceptual system design and logical system design for STEWARDS are documented.
Physical system design (how the logical structure of STEWARDS will be implemented) is still in a planning stage and will be described later.
As depicted in the conceptual model of STEWARDS (Fig.
The database design and interface/application design parallel each other within the system development process.
The total number of system components will depend on the level of detail.
For example, the data component of STEWARDS can be further identified as watershed climate, image, soils, management, pollutant source, hydrology, socio-economic, water quality data, data search, data downloads etc (Fig.
Description for hardware infrastructure design will be provided separately.
During the logical system design phase, a logical data system (all system components and their relationships) will be constructed and validated.
In other words, the Entity-Relationship (ER) diagrams for measurement data and user graphic interfaces (UGIs) to connect all components will be developed.
For example, an entity-relationship diagram for measured climate data at South Fork watershed, Iowa, is given in Fig.
The tables – STEWARDS_Units_lookup_Table, [WS]_Topic_Table, and [WS]_Site_ID_Table are supporting data tables to the [WS]_Climate_Daily table.
The latter is the basic data table where climate data are stored.
Note that the characteristics (topic) of a theme (e.g. climate) in Fig.
F3 become the fields (min_temperature, max_temperature, etc.).
The data descriptors for the basic data table and supporting tables are shown in Tables F1 and F2 respectively.
A generalized data flow and table relationship is given in Fig.
Three major system application components are identified: overview/summary, system search, and data access/visualization/downloads.
For next level of detail, the system component diagram for access login component, system/summary system component, metadata search component, site summary search, component, watershed specific search component, topic search component, site specific search component, pre-existing search component, characterizing watershed GIS component, and data access/visualization component are given in Figs.
The tasks for importing data from the local sites into the centralized site required the following considerations.
Therefore, translating empirical data is the only need.
Translation will be local site specific and done there.
There are three types of empirical data we will need to address.
Time series data (Type 1 data) with columns for different data collected at the same time.
A generalized data structure for this type of data is given in Fig.
A generalized data structure for this type of data is given in Fig.
The mapping method allows a great flexibility in terms of data heterogeneity, table size, data type, etc.
Physical System Design: Build the specific data system for STEWARDS in terms of softwares and tools.
For software requirements, the programming skills in developing Windows and Web applications using Microsoft Visual Studio .NET (BASICS, ASP.NET, C++, C#), XHTML, XML programming languages/scripts, Microsoft VISIO 2003 under MS Internet Information Services (IIS) and Microsoft SQL 2000/2005 servers will be pursued.
F1 Logical model of STEWARDS Data System with four system components – System infracture, databases, applications/interfaces, and data uploading.
Each block represents a data entity (a topic object).
An example Entity-Relationship (ER) diagram of climate data at South Fork Watershed, Iowa.
The definition text at the top of the table with gray color represents the context of an entity (e.g. [WS]_Climate_Daily) while the texts under the top gray area represent the field names.
PK represents primary key of a table and FK(x) is foreign key.
The line with an arrow shows the relationship between two tables.
A foreign key is the field in a relational table that matches the primary key column of another table.
The primary key of a relational table uniquely identifies each record in the table.
The indexes, j,k,m, and n represent the number of topics, sites, variables, and tables, respectively.
The lines with arrows show the relationships between tables.
The numbers represent different application /interface system component at different levels.
The 2nd level of access login component of STEWARDS.
The first digit of the leading number of each subcomponent represents the first level of the system components and the second number represents the second level and so forth.
Data query - Create queries to derive specific information.
Data extraction - Download geographic data sets from the server.
Characterizing watershed GIS component of STEWARDS.
As an ARS watershed data system to support SWAT and AnnAGNPS modeling in CEAP, ultimately the STEWARDS data system could be integrated with other similar ARS data/modeling systems such as OMS (Object Modeling System), iFARM (Integrated Farm And Ranch Management), and other general models (Fig.
The OMS is a Java-based modeling framework that facilitates simulation model development, evaluation, and deployment.
In general, the OMS consists of: 1) a library of science, control and database modules; 2) a means to assemble selected modules into a modeling package customized to the problem; and 3) automatic generation of a user- friendly interface.
The framework employs the latest Java-based software technology for all the components, and is supported by data dictionary, data retrieval, GIS, graphical visualization, and statistical analysis utility modules.
G1 Linkage of STEWARDS to Other USDA/ARS Data/Modeling Systems.
H1) of STEWARDS shows that the task components in the development process.
The prototype has been implemented but is still under tuning.
The number in parentheses represent the system component number as shown in Fig.
Type 1: Time series data with columns for different data collected at the same time.
I1 shows a generalized data structure for this type of data.
Type 2: Time series data, usually more sparse than the above, with an id field that explains what the parameter field contains.
I2 show a generalized data structure for this type of data.
I1 Data structure for time series data (Type 1 data) with columns for different data collected at the same time.
Data structure for time series data (Type 2 data), usually more sparse than those shown in Fig.
RD 12 Review of SLCCI SRD v1.0, Issue 1, Revision 0, Bruno Lucas, 08/08/2011.
RD 13 Review of SLCCI SRD v1.1, Issue 1, Revision 0, Bruno Lucas, 25/01/2012.
RD 14 Review of SLCCI SRD v1.2, Issue 1, Revision 0, Bruno Lucas, 18/04/2012.
The System Requirements Document (SRD) serves to provide a complete set of system requirements for the SLCCI operational system, as requested in the Statement of Work (SoW), geared to the Sea Level ECV.
The SRD will function as the primary input to the design of the operational system, as to be described by the System Specification Document (SSD).
This document is a revised issue of SRD v1.2, following ESA feedback [RD 14].
The System Requirements Document (SRD) defines the requirements of the Sea Level Climate change Initiative (SLCCI) operational system (henceforth the system), and so acts as the foundation to the System Specification Document (SSD).
The SRD is a living document, and it is intended that the document undergoes the necessary pressures of review and subsequent revision, during the lifetime of Phase 1.
Where change takes place in the SRD, those deliverables which depend on the SRD must be appropriately updated.
The readership of this document is comprised of the SLCCI consortium parties and ESA.
There may also be scope, following further investigation, as to the use of this document for the Software Engineering Working Group (SEWG), towards finding and forming common ground with other ECV projects as is encouraged by the Statement of Work (SoW).
This document is based on issue 1.4 of the Data Access Requirements Document (DARD), issue 1.0 of the Preliminary System Analysis Document (PSAD), Issue 1.1 of the Product Specification Document (PSD), and Issue 1.3 of the User Requirements Document (URD); refinement of this document will be necessary of catchment of future issues of these documents.
One of the types of product of the Sea Level Climate Change Initiative project.
One of the types of product of the Sea Level Climate Change Initiative project.
The phrases operational system, operational SLCCI system, SLCCI system all dethe SLCCI operational system being developed.
Where the word system is used, it will similarly refer to the SLCCI operational system, unless it is clear from the context that another system is being referred to.
The Sea Level Climate Change Initiative project is one of eleven parallel Climate Change Initiative (CCI) projects, each focussing on a specific Essential Climate Variable (ECV).
The CCI Statement of Work extrapolates from, and refines, this axiomatic objective into a series of constituent requirements and desirabilities, within a phased framework for realisation.
The effort towards realisation of a system to enact the ultimate objective of the CCI is split into three phases, with Phase 2 implementing the operational system.
Phase 1, within which this System Requirements Document (SRD) resides, declares the system requirements for the operational system of the Sea Level ECV, that is the specification of what the system should do, prior to the subsequent design deliverable describing how the requirements should be realised.
The importance of the CCI Statement of Work (SoW) is acknowledged by its integration into the systems requirements elicitation, from which a series of business goals are formed.
One of the reasoned objectives elicited is the reuse of existing functionality towards the development of each CCI operational system.
To this end, the System Requirements document (SRD) reasons for the apt adoption of certain requirements from an existing operational system, towards a measured realisation of an SLCCI system.
The scope of the operational system from which the SLCCI system will be founded is therefore mature and well defined, given its proven validation over time in an operational context.
The CCI Statement of Work does not directly preclude inclusion of any type of environment, but does encourage reuse of existing similar operational systems, as practiced in Chapter 6 (Requirements Elicitation), which may therefore indirectly constrain the environment(s) which may be considered to those adopted by the existing similar system.
The CCI Statement of Work declares the desirability to a unified multi-ECV system, whereby all, or sets of, ECV operational systems share resources and infrastructure.
The CCI System Engineering Working Group (SEWG) has been set up as a vehicle towards satisfying this goal.
To this end, acknowledgment and consideration is taken in the System Requirements Document (SRD) with the aim of exploring commonality between the SLCCI and other CCI projects.
Additionally, one of the advantages of adoption of the existing system chosen, namely DUACS, is that it has been proven to reside in a broader, multi-variable, system similar to a possible pan-ECV CCI system.
The lifecycle of the System Requirements Document (SRD) starts via a Preliminary System Analysis Document (PSAD) identifying the candidate system for reuse for SLCCI, namely DUACS.
A set of system requirements is elicited and refined based on a judicious mapping with DUACS, prior to other inputs being considered.
The SRD thereafter undergoes an iteration of review and subsequent modification towards reaching a stable system requirements baseline at each iteration.
Requirements Elicitation – the attainment of system requirements.
Requirements Analysis – the analysis and refinement of system requirements attained.
As will be seen in the following chapter (Requirements Elicitation), special consideration is given to the judicious absorption of an existing similar system into the SLCCI system requirements baseline, namely DUACS as realised for MyOcean SL TAC, identified by the Preliminary System Analysis Document (PSAD).
Re-use of the system for this System Requirements Document (SRD) is undertaken in the context of ECSS standard Q-ST-80C for Space Product Assurance ([RD 10], section 6.2.7), describing good practice for the re-use of existing software.
A macro functionality is a distinct high-level area of functionality, representable by a use case, and ultimately mappable to a sub- system.
The unrefined, raw, mapping elicited is analysed with the aim of outputting refined requirements to the System Requirements Baseline (Chapter 8).
Description is made as to how each SLCCI equivalent system requirement need be modified and whether the mapping itself is valid.
The output of the Requirements Analysis is synthesised as a formal System Requirements Baseline.
The system requirements are arranged via two dimensions, the SLCCI macro functionalities distilled from the Requirements Analysis, and (ii) the ECSS requirements groups.
Performance - Each requirement should be quantifiable.
Identifiability - Each requirement should be associated with a unique identifier, associated in the context of where it sits in the system.
Singularity - Each requirement should be separately stated.
These ECSS criteria are compatible with IEEE 830 Standard on the characteristics of valid requirements.
Initial requirements are directly elicited via (i) the CCI Statement of Work [RD 7] from which business goals and subsequent design requirements are extracted, which includes encouragement for system reuse (ii) a Preliminary Systems Analysis Document [RD 3] describing an adopted existing system significantly similar to SLCCI, namely DUACS, and (iii) relevant documents pertaining to how DUACS is realised for MyOcean in the form of the MyOcean SL TAC.
The argument towards reuse of DUACS is aided by the context of system reuse good practice as promoted by ECSS standard Q-ST-80C on Software Product Assurance [RD 10, pg 27].
As depicted by Figure 6-2, the steps undertaken during Requirements Elicitation start with the extraction of business goals from the Statement of Work, the axiomatic needs for the SLCCI project towards an operational system (Section 6.2).
The pertinent business goal regarding system reuse is pursued through the identification and adoption of an apt system for re-use (Section 6.3); a line of argument is posited towards adoption of DUACS based on significant similarities, though also identifying differences for accountability of argument.
The ESA Climate Change Initiative Statement of Work [RD 7] represents the source from which the business goals of the system are herein elicited.
The objective of Phase I Task 5 is the “preperation for Phase 2 of the CCI Programme” [RD 7, pg 26].
The CCI Phase I is defined largely as a scientific exercise by the CCI Phase I SoW [RD 7], given the Tasks defined therein.
Tasks I to Tasks 4 relate to the scientific endeavour of engaging with the latest scientific understanding, with the ultimate aim of creating a prototype production system which reflects that understanding.
Task 5 is a specification of the operationalisation of that prototype production system for the outside world.
The CCI Phase I SoW [RD 7] is here analysed to extract a series of business goals pertinent to the operational system.
These goals represent certain axiomatic needs of the operational system prepared by Phase I Task 5, and due to be built by future Phase II implementers.
We scope our analysis for arriving at business goals of the operational system, by first defining operational system.
The operational system is the operationalisation of the Task 3 prototype production system, that is a production system and the engineering in of capability onto the production system to allow it to be used in the outside world, following its construction by CCI Phase II.
The operational system is specified via Phase I Task 5 and built by future implementers via Phase II.
Our aim in analysing the Statement of Work is to infer a series of goals which are directly related to the operational system.
We are not directly concerned, therefore, with the activities leading up to the prototype production system, nor are we directly concerned with the activities at a meta-level associated with the CCI programme organisation, such as the listing of other CCI ECV programmes.
To that end, we have analyised the whole Phase I SoW, inferring business goals, which we illustrate with supporting evidence.
It is imperative to note that for each business goal such evidence is one item of supporting evidence as labelled, and does not necessarily reflect all items of evidence used to reason the associated business goal.
Therefore, a business goal cannot be necessarily inferred by observation of the associated supported illustrativetext alone, but rather by absorption and analysis of the whole document.
Business goals SLCCI-SRB-BUSINESS-GOAL_#1 to SLCCI-SRB-BUSINESS-GOAL_#14 regard the technical constraints listed in SoW §2.8 related to “planning and implementing the CCI project”; these inhabit section SoW §2 (CCI Programme Background) prior to commencement of the description of the tasks to be executed (§3).
These §2.8 constraints apply to CCI projects as a whole, and irrespective of the Task at hand.
We distil these in context to the operational system alone.
With SLCCI-SRB-BUSINESS- GOAL_#1, for example, we propose that since the scientific community represent the end users of the operational system, their expectations with regards to the performance of the operational system must be taken into account.
Development of the operational system shall be undertaken with apt consideration for scientific consensus on performance specification of the operational system.
Development of the operational system shall be undertaken with apt consideration for availability of input data from EO archives.
Development of the operational system shall be undertaken with apt consideration for quality of input data from EO archives.
The supporting evidence from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#3 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#2 [RD 7, pg 11].
Development of the operational system shall be undertaken with apt consideration for availability of associated metadata, cal/val data and documentation.
Development of the operational system shall be undertaken with apt consideration for quality of associated metadata, cal/val data and documentation.
The supporting evidence illustration from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#5 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#4 [RD 7, pg 11].
Development of the operational system shall be undertaken with apt consideration for compatibility of data from different missions.
The supporting evidence illustration from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#6 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#4 [RD 7, pg 11].
The supporting evidence illustration from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#7 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#6 [RD 7, pg 11].
Development of the operational system shall be undertaken with apt consideration for trade-off between cost, complexity and impact of new algorithms to be developed and validated during the project.
The supporting evidence illustration from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#8 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#6 [RD 7, pg 11].
Development of the operational system shall be undertaken with apt consideration for advance planning for data from new missions to be integrated during the project.
The supporting evidence illustration from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#9 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#6 [RD 7, pg 11].
Development of the operational system shall be undertaken with apt consideration for end-to-end throughput of ECV production systems.
The supporting evidence illustration from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#10 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#6 [RD 7, pg 11].
Development of the operational system shall be undertaken with apt consideration for re-use of existing capabilities within Europe.
The supporting evidence illustration from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#11 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#6 [RD 7, pg 11].
Development of the operational system shall be undertaken with apt consideration for compliance of ESA standards.
The supporting evidence illustration from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#12 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#6 [RD 7, pg 11].
Development of the operational system shall be undertaken with apt consideration for availability of external validation data.
The supporting evidence illustration from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#13 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#6 [RD 7, pg 11].
Development of the operational system shall be undertaken with apt consideration for avoidance of duplication of activities covered by existing operational projects.
The supporting evidence illustration from the Statement of Work for SLCCI-SR-BUSINESS-GOAL_#14 is the same as that described above for SLCCI-SR-BUSINESS-GOAL_#6 [RD 7, pg 11].
Similarly, in CCI Phase I SoW §2, which offers CCI Programme Background, we extract further information relating ESA’s vision of the operational system.
Before commencement of description of the Phase I Tasks (SoW §3), SoW §2.5 briefly describes the three phases of the CCI programme.
We take advantage of this Phase I SoW introductory section, to pre-empt future ESA expectations for Phase II, which concerns the building of the operational system.
As described above for Business goals SLCCI-SRB-BUSINESS-GOAL_#1 to SLCCI-SRB-BUSINESS- GOAL_#14, we observe Phase I content general to the phase, and infer from that the goal specific to the operational context.
As similarly described earlier, supporting evidence offered is illustrative and does not necessarily form all roots of the reasining.
Similarly, as with the content quoted below, for example, we draw on modernity and adaptability of the operational system through other goals inferred.
We recognise the matter of cost effectiveness across the ECVs as imperative to achieving cost effectiveness.
We acknowledge the urgency of delivery of the operational system to the climate change community.
Full advantage shall be taken of the latest developments in computing architectures in the development of the operational system.
Again, in the Phase I “CCI Programme Background” section, we interpret ESA’s vision of Phase I prior to the Phase I SoW Task specification; we assume this SoW visionary information underlies in part the motivation of content in the Phase I SoW prior Task definition.
Supporting evidence – “CCI Phase 1 […] Full advantage must be taken of the latest developments in computing architectures, data management and communications technologies.
Innovative structures for large-scale data sharing, data (re)processing and user access, need to be investigated and traded off alongside the associated cost models.
Full advantage shall be taken of the latest developments in data management in development of the operational system.
The description and supporting evidence illustration from the Statement of Work for SLCCI-SR- BUSINESS-GOAL_#19 applies above to SLCCI-SR-BUSINESS-GOAL_#18 [RD 7, pg 8].
Full advantage shall be taken of the latest developments in communications technology in development of the operational system.
The description and supporting evidence illustration from the Statement of Work for SLCCI-SR- BUSINESS-GOAL_#20 applies above to SLCCI-SR-BUSINESS-GOAL_#18 [RD 7, pg 8].
The operational system development should include cooperation with other consortia producing ECV products.
We dehere the importance of pan-ECV collaboration across Task 5 endeavours, towards cost effectiveness, as is being realised by the System Engineering Working Group (SEWG).
The operational system shall have provision for future data set updates.
The operational system must be flexible enough to be capable to acquire, absorb and process the future data sets being created by scientific endeavour.
Supporting evidence – “address the need for establishing data service systems that ensure ongoing accessibility to the Climate Data Sets into the future as well as the required capacity to update these data sets periodically by addition of new data or by reprocessing complete records when calibration improves or ECV generation methods evolve” [RD 7, pg 27].
The operational system shall allow algorithm change.
Similarly to the previous CCI Programme Background reasoning above for SLCCI-SRB-BUSINESS- GOAL_#22, the operational system needs to be flexible enough to accommodate algorithms being developed by science, as well as new data (SLCCI-SRB-BUSINESS-GOAL_#22).
The operational system shall have an archiving facility.
Of all the ingredients listed for inclusion in the SRD, our preliminary analysis points to these as already being considered as part of our system requirements engineering effort, leading to the system requirements baseline.
We take explicit here of archiving, as this is pointed to as a functional need of the operational system.
Supporting evidence – Appendix 2, describing content of deliverables, portrays the system requirements as needing to include “archiving requirements” (baseline data and interim products and outputs and their safeguarding to allow for reprocessing).
The above elicited business goals are included as part of the Design Requirements & Implementation Constraints section of the System Requirements Baseline.
Therefore, the axiomatic needs of the SLCCI system are made concrete directly in the System Requirements Baseline, so mitigating risk of loss of vision of the system during development.
The business goals earlier elicited make it clear that adoption of an existing system for reuse for SLCCI is desirable.
ECSS standard Q-ST-80C for Space Product Assurance ([RD 10], section 6.2.7), which describes relevant good practice for the re-use of existing software, from which this System Requirements Document (SRD) will observe where apt for SLCCI.
The Preliminary System Analysis Document (PSAD) identified DUACS as a suitable system for adoption.
The input to the DUACS system and the SLCCI system are significantly similar.
The output to the DUACS system and the SLCCI system are significantly similar.
The users, the audience, of DUACS and the SLCCI system are significantly similar.
The climatology research community are the primary target audience for SLCCI, and represent a significant subset of the DUACS user base.
We aim for SLCCI to be subsumed with other sibling ECVs in a higher-level system; such an objective has already been proven with DUACS in the context of the (pan-variable) MyOcean Endeavours are being made, and will continue to be made, through the CCI System Engineering Group (SEWG) towards reaching commonality with sibling ECVs towards system design.
To accommodate this endeavour in an apt manner during the requirements stage, it is envisaged that the SLCCI system will hook in to higher level Central Information System (CIS) [RD 11], available at the CCI level across all ECV projects (including, in this content, SLCCI), with each ECV implementing its own INSPIRE compliant interoperability with the Central Information System.
Metadata will be associated with SLCCI in order to describe all output sea level products.
It is anticipated that this SLCCI metadata will take the form of static and dynamic metadata, describing the characteristics and quality of the associated SLCCI product, respectively, as is similarly the case with DUACS metadata.
This is recognised in context of the business goals associated to quality and availability of metadata, namely SLCCI_SRB_BUSINESS_GOAL_#4 and SLCCI_SRB_BUSINESS_GOAL_#5.
As has been pointed out, both DUACS and SLCCI systems are both associated with a common and significant user type, namely the climatology research community; the complete user type for SLCCI represents a significant subset of DUACS users.
DUACS NRT services, are irrelevant to SLCCI which is concerned exclusively with the climatology research community.
DUACS generates Near Real Time (NRT) products , Delayed Time (DT) or Updated mode, and entire reprocessed time series (REP).
SLCCI only addresses DT and REP specifically for climate applications.
DUACS as realised for MyOcean SL TAC links to a higher level system, namely MyOcean.
Although the envisaged Central Information System (CIS) for SLCCI is analogous to the MyOcean Information System (MIS), the CIS is intentionally flexible but nonetheless well scoped, rather than a concrete, existing, high level pan-variable system as is the case with MyOcean.
No Acquisition Chain and Production Chain, as used by DUACS documentation, exist on the SLCCI.
Instead, the term production pipeline is used to refer to the sequential macro-functionalities necessary in producing the data products.
We further demonstrate that the DUACS system can be used for SLCCI product generation, by mitigating development risk in adoption of DUACS.
To this end, it is pertinent to eliminate or manage any constraints to the SLCCI inherited from DUACS.
On the first of these, the DUACS system was originally tailored for independent operational use rather than as a general (sub) system hookable elsewhere for re-use.
However, use of DUACS towards reuse has been proven in the context of MyOcean, a large scale multi- variable processing and dissemination system, so proving that DUACS is capable of serving for reuse.
On the second point, the Design Requirements & Implementation Constraints (Section 8.19) section absorb such matters.
Description of differences between DUACS and SLCCI (Section 6.3).
To this end, the SRD declares the differences between the original DUACS system and the target SLCCI system (Section 6.3) to legitimise and reason the mapping.
These differences are observed during Requirements Analysis (which takes as its input the raw requirements mapping reasoned during Requirements Elicitation) in order to reason all changes necessary for those requirements which are derived based on the mapping from the original system.
We acknowledge the good practice on system reuse (Section 6.3) as promoted via the ECSS standard for software reuse as elicited by the ECSS standard ECSS-Q-ST-80C (Space Product Assurance, Software Product Assurance), Section 6.2.7 (Reuse of Existing Software), and implement where apt and suitable for our system reuse of DUACS towards SLCCI.
The elicitation of business goals from the CCI Statement of Work not only highlight the welcoming of system reuse, directly (as SoW explicit content on system reuse) and indirectly (via SoW content on cost efficiencies) , but also the adoption of latest technologies; to this end, the adoption of an existing, mature, system needs to legitimise the balance undertaken of reuse of existing technology with adoption of, or integration with, more novel technologies towards provision of a system of apt quality and timeliness.
In the context of the System Requirements Baseline, an explicit list of constraints are offered in section 8.19 (Design Requirements & Implementation Constraints).
Such constraints are important during SLCCI design, in meeting apt and reasoned reuse of the existing DUACS system.
During the design stage, the output of which is communicated in the form of the System Design Document (SDD), this pertinent matter will be attended to at the very earliest opportunity.
The following diagram portrays the boundary of the DUACS system, and describes the interaction between the system and its external environment.
Upstream Systems from which data is collected for the production chain.
Support Contributor, users internal to the DUACS system.
Product Contributors, users internal to the DUACS system.
While interacting with the system, he is to be considered as an actor and uses specific system capabilities.
While performing additional manual or procedural tasks and eventually storing his results in the System.
Monitoring (and follow-up of monitoring) System & Service & Products Solving incidents on the system”.
Note that every contributor type is involved in each of NRT, DT and REP production ; each type has a higher frequency of activity involved in the NRT production only because NRT productions are daily compared to other productions which are of a lower frequency (REP,DT).
The SLCCI system will adopt the same system boundary and high level interaction with users, considering the aforementioned operational equivalencies.
The DUACS system, as realised for MyOcean SL TAC, is part of the higher level MyOcean system, so dependencies with the Top Level, for instance the MyOcean Information System (MIS), are not directly included in the SLCC equivalent.
Acknowledgement of this potential pan-ECV usage is of the very utmost importance, considering the business goals elicited which relate to system reuse, namely SLCCI- EB-BUSINESS-GOAL_#15, SLCCI-EB-BUSINESS-GOAL_#16, and SLCCI-EB-BUSINESS- GOAL_#21.
To this end, a Central Information System (CIS) is adopted for SLCCI.
The external users quoted for DUACS are a superset of those associated with SLCCI, considering the DT/REP-only nature of SLCCI products.
The SLCCI users relate to climatology research and so do not require degraded-quality NRT data products.
SLCCI Contributor – the highest level internal user type.
SLCCI Product User – the highest level external user type.
The diagram below (Figure 6) portrays all SLCCI user types.
The SLCCI Product User is considered to be associated with external users of the SLCCI outputted products (Climate modelling communities represented by the CMUG group within the CCI Phase 1), namely the SLCCI FCDR and SLCCI ECV as described by the PSD.
The SLCCI contributor, or Worker, des an internal user to the system.
Moreover, the diagram represents the relationship between different types of user via the generalisation relationship.
Therefore, for example, there are two direct sub-types of SLCCI User, namely an SLCCI Contributor and an SLCCI Product User; in other words, an SLCCI Contributor is an SLCCI User and an SLCCI Product User is an SLCCI User.
Indeed all user types in the diagram are SLCCI Users since the SLCCI User type is the ultimate base type of all user types.
Following on, an SLCCI Product Contributor (responsible for the quality and management of products) and an SLCCI Support Contributor (representing SLCCI Contributor types supporting operational services), are both direct sub-types of SLCCI Contributor.
Furthermore, there are different types of SLCCI Product Contributor (Scientific Expert responsible for modelling, Product Expert responsible for establishing and assessing the quality of products plus providing scientific judgement, Product Manager maintaining the product database, product catalogue content and managing metadata) undertaking the work done on the production chain, and different types of SLCCI Support Contributor (Service Desk interacting with external users, Service Manager overseeing the operational running, Support Operator supporting the systems operations internally) undertaking the support of operational running of the system.
These four packages exhaustively realise the operational scenarios explored by SL TAC [RD 9, pg 12], and are operationally significantly similar to SLCCI to warrant adoption, given the argument and evidence offered earlier.
To this end, the figure below offers an illustration of the SLCCI equivalent Use Case packages adopted.
The following four sections will argue for the apt mapping of use cases and their associated system requirements to the SLCCI system, with each section representing one of the four use case packages, thus covering the entire use case package spectrum.
As argued earlier for DUACS, and indeed also by the PSAD, the above use cases are mirrored by macro functionalities describing the pipeline between data input and end product [RD 3].
Each of these high level use cases maps to a macro functionality.
The SL TAC SRD [RD 9, section 8.1.1] fully describes the responsibility associated with each DUACS macro functionality.
The high level DUACS use cases for production are each associated with a macro functionality, with each macro functionality associated with lower level requirements completely expressing the associated macro functionality.
Given the evidence and argument so far composed, we may judiciously define a mapping of use cases and subsequently a mapping of the system requirements contained therein, from DUACS to SLCCI.
We therefore envisage the same use case to macro functionality parallel for the SLCCI as is the case with DUACS.
For convenience we explicitly map between SLCCI and DUACS macro functionalities rather than between their associated use cases.
Given the evidence provided thus far, we argue that the DUACS production Use Cases, and therefore their associated macro functionalities, map to SLCCI equivalents.
The View Product use case relates to visualisation of the product under two guises – a “Preview” of products, and a “Full View” of products [RD 9, pg 48] composed of generated static images.
External tools are used to generate these views, namely THREDDS, OPENDAP, and WMS for such visualisation, which expect content in NetCDF format.
As explored as part of the Requirements Analysis, these will not necessarily be adopted for SLCCI.
With regards to the Get Product DUACS use case, the product is downloaded by means of ftp or OPENDAP [RD 9, pg 50].
The product is also available via the MyOcean web portal.
Again, the SLCCI equivalents, described in the Requirements Analysis, will illustrate where changes are likened or necessary in order to accommodate the mapping, for instance the adoption of ftp and OPENDAP.
Regarding the Support Users use case, users’ demands for support and information are satisfied via a DUACS web portal supported by a service desk.
A high level service desk distributes requests to an SL TAC specific service desk [RD 9, pg 51].
Four relevant user types pertain, the top level and SL level service desks, service manager and support operator.
As stated in earlier, whereas DUACS hooks into an existing higher level infrastructure for the MIS, the SLCCI equivalent will be connected to a Central Information System at the CCI level.
An answer may be returned to an information request via different communication means and within a variable time span depending on information type or availability.
For example, user might be directed towards online system capabilities for general information.
As a single point of contact, the MyOcean Web Portal, is offered to users to get automated information, integrated system capabilities allow Support Contributors to automatically manage and publish information on this Web Portal.
New dissemination interfaces (associated to "Products").
View Product relates to viewing details of product, as described by its metadata.
Get Product is the downloading of the product by an external user; this is accomplished via an SLCCI web portal.
Support User is a ticketing system similar as that earlier referred to for DUACS.
However, where the mapping does not hold is that no higher level service desk exists for SLCCI, therefore the SLCCI service desk (i.e that at the level of the SLCCI rather than a higher level system) is referred to instead; it is d however that there is the potential to glue SLCCI to a higher level pan-ECV system, given that DUACS, in the frame of MyOcean SL TAC equivalent, already resides in a pan-variable infrastructure.
Under SLCCI, it is envisaged that users referring to the SLCCI Support User use case are (i) Service Desk, (ii) Service Manager, (iii) Support Operator, that is the complete three types of SLCCI Support Operator.
These three user types will enact actions in the SLCCI system requirements on which the support user use case envelopes.
The Use Case package describing the management of products in the DUACS system is described in the SL TAC SRD [RD 9, section 8.1.2].
Product information is supplied by static metadata (the Update Static Metadata use case) and dynamic metadata (the Update Dynamic Metadata use case), dependent on the kinds of product updates.
A Maintain the Products Database use case subsumes both these metadata update use cases.
The Maintain the Product Database use case for DUACS, as realised for MyOcean SL TAC, is related to registering products in the product database and the maintenance of the static and dynamic metadata associated with the products.
Actors involved pertain to the Product Manager, Product Expert, which are both Product Contributors, i.e internal users of the system.
As earlier indicated, it is proposed that the SLCCI system have a Central Information System (CIS) equivalent to the MyOcean Information System (MIS) component, and therefore the mapping to an SLCCI equivalent during Requirements Analysis (Chapter 7) will accomodate the SLCCI equivalent accordingly.
Authorization of the "Product" (so that MyOcean can start ?logging?
Again, an Information System is not proposed for the SLCCI system, as it is not currently warranted.
However this could be a valuable solution when relying on (at least partly) existing systems.
As is the case with DUACS, a differentiation is made for SLCCI between two flavours of metadata, namely static and dynamic metadata, describing the characteristics and quality of the associated SLCCI product, respectively.
It is anticipated, again, that the SLCCI equivalent will justifiably refer to both static (definition of the product characteristics) versus dynamic (definition of product quality) metadata types.
The FTSS (Fast Track Service Specification) is a MyOcean document cataloguing products, containing descriptions to all products generated by MyOcean.
These matters are considered further during Requirements Analysis (Chapter 7) where the raw systems requirements mapping is exercised to produce and refine SLCCI equivalents.
Each update (creation/deletion of a Product Line/Product specification, authorization of a Product) of the Products Database by the SL TAC Product Manager has to be reported to the Top-Level Product Manager for a validation (particularly regarding the state of a Product: operational or not).
Such Product Lines won't have associated Product Specifications and are necessary to the product dependencies.
Again, strengthened by the business goal requirement towards consideration of SLCCI metadata defining availability and quality of product, a semantic equivalency is proposed between DUACS metadata and SLCCI metadata.
As earlier indicated during the analysis of the DUACS / MyOcean SL TAC product management, the SLCCI equivalent assumes similar distinction between static and dynamic metadata.
The DUACS use case package containing all use cases therein related to monitoring of the system is the Monitoring package, are described in the SL TAC SRD [RD 9, section 8.1.4].
Certain DUACS requirements, as realised for MyOcean SL TAC, must be refined in order to reach their SLCCI equivalents, for instance the defining of raw measurements, and depiction of the CRM tool to be used.
Top Level dashboard shows consolidated indicators for all Top level Services.
This dashboard allows Service Desk members (Support Operators, Service Managers), at all levels, to know the "health" of the Top Level Services at a glance.
Note that the SLCCI equivalent will not (necessarily) have a top level tier, and so an equivalent at the SL level should be adopted for the SLCCI equivalent.
Several level of automation might be implemented from fully manual to advanced instrumented monitoring.
As earlier indicated, ITIL Processes are irrelevant to SLCCI and therefore fall out of scope of the SLCCI system requirements.
As earlier indicated, the SLCCI equivalent system will have a Central Information System (CIS), residing at the CCI level, equivalent to the MIS component, and therefore SLCCI requirements will reflect this equivalency.
The monitoring can be either an automated procedure or human action.
Deferment of certain DUACS characteristics to a Service Management Plan, necessitates examination of the Service Management Plan during Requirements Analysis.
The production is monitored in the SL TAC Production Center.
Athough fitting neatly into the MyOcean structure at a high level, the concept of the production centre is not established in the SLCCI equivalent.
This information is produced and validated by the Product Expert (see "Perform output checks and QC", "Do measures and build indicators on products" Use Cases).
The exact nature of the metrics to be monitored will be detailed in the Service Management Plan.
Request monitoring at the SLCCI level, as required for Requirements Analysis cannot assume inclusion of a higher level system tier to SLCCI, but should flexibly observe a higher level tier where possible.
The list is described in the Service Management Plan.
DUACS monitoring requests are enacted through a top level providing information on the user relevant to a request, and information regarding the products retrieved by that user.
Such assumptions cannot be assumed to be realised for the SLCCI.
The SLCCI equivalent of the Monitoring package is as illustrated below, which mirrors the DUACS use case arrangement as realised for MyOcean SL TAC.
Therefore, the three types of monitoring proposed in the DUACS system are similarly adopted for the SLCCI system.
The main respect in which the equivalence between DUACS, as realised for MyOcean, and SLCCI does not hold is the presence of a higher level tier for the former.
As earlier argued, a higher level presence for SLCCI does not (necessarily) occur, but can be accommodated through well scoped provision where apt for this stage, such as through inclusion of a CCI-level Central Information System.
The Requirements Analysis takes as its input the argued equivalency between DUACS and the SLCCI operational system, in readiness for argued mapping at the system requirement level (§7.2).
The analysis also draws in other documents as referenced below, with a subsection similarly associated with each.
As its input, therefore, the Requirements Analysis accepts a reasoned, though unrefined, system requirements between DUACS and SLCCI, based on argued equivalences of user hierarchy, scope, use case packages, use cases, and their associated system requirements during the Requirements Elicitation.
The following four sections (Sections 7.2, 7.3, 7.4, 7.5) portray the judicious mapping from DUACS to SLCCI, given all supporting evidence and argument thus offered.
Where the mapping is not possible to an SLCCI equivalent, this is reasoned.
Similarly, where a mapping needs to be such that a pertinent difference must hold, then an explicit reference is made to the difference, in order to allow the mapping; the set of differences between the DUACS system, as realised for MyOcean SL TAC, and SLCCI have been given.
The following table lists the tables housed by each of these four sections.
Each section table represents the mappings for a use case pertaining to the associated use case package.
The above categories pertain to the complete set of ECSS requirement groups apt for SLCCI [RD 5], other than (i) Functional Requirements and (ii) Performance Requirements, which are more aptly housed within each of the SLCCI macro functionality sections.
We introduce the concept of a Central Information System (CIS) to our design, to represent our interfacing with other ECVs ; the consensus and specification of such an interface are still under discussion within the SEWG, but we want to make the maintain the concept of such an interface as part of the SLCCI design.
Portal, but also make them available to our conceptual Central Information System.
The System Requirements Baseline should include requirements derived from the consortium experience in operational systems within Earth Observation, and other sectors where applicable.
Such requirements will draw on the operational demands to be put upon the Sea Level CCI system, as well as expectations the operational system should contain for the assessment of its operational health.
Establishment of macro functionality performance expectations, to each of the production chain stages.
Each relevant stage of the production chain should have associated performance expectations.
Inclusion of requirements related to operational matters, not directly connected to the production chain but nonetheless required for the running of an operational service, such as asset and inventory management, backups.
To help maintain operational integrity to the system, transparency to the performance gathering aspects of the system should also warrant consideration, such as in the gathering of operational statistics.
The expectations of the system may bind to particular standards.
Use of such standards differs to those to be adopted within the System Specification Document, where candidate standards are identified and a chosen standard reasoned towards helping satisfy system requirements.
Analysis towards inference on, and refinement of, relevant expectations pertaining to the DUACS operational system, for example expansion of operational demands for user registration, download verification, and transaction accounting, linkage to a higher level system.
Analysis on elaborating on the user perspective towards reasonably capturing future user demands, e.g concerning reasonable demands of user of operational system, for instance allowance of simultaneous downloads, and the associated storage resources required.
At a meta-level to specifying requirements of the system, specification of requirements to do with the conduct of system specification and development itself, such as apt considerations arising from ECSS.
Analysis on system configuration, installation and delivery.
The system should, for instance, be able to anticipate Earth Observation data from the Sentinel cluster.
Our analysis of the URD [RD 1] captures the list of user requirements which the operational system should accommodate in a system context.
We do so by expressing the accommodation of these desired user requirements.
These system focus centres on the SLCCI production chain, and preserve the user consensus on what should be accommodated by the operational system.
This synthesis within the URD proposes the following categories of user requirements, which for the system specification we will bundle to declare that the SLCCI should accommodate such matters.
Length of data set time series ([RD1] §5.1.1, UR-SLCCI-GEN-01).
Satellite coverage and overlap over the dataset ([RD1] §5.1.1, UR-SLCCI-GEN-02).
Acknowledegement of tidal influences ([RD1] §5.1.1, UR-SLCCI-GEN-06).
These matters do not directly point to the scientific practice undergone to arriving at the input data or product generation, but rather user-driven characteristics which the system requirements must be in keeping with.
Our analysis of the PSD [RD 6] allows us to recognise the production chain product types, to be outputted by the operational system.
In particular, declares the formats of these two product types, given both the PSD ([RD6] §2.2&3.2) and the endeavours of the Data Standardisation Working Group (§7.10).
Identification of the spread of satellites, missions, instruments and products necessary for application to the operational system, irrespective of the outcome of the Task 2 Round Robin ([RD2] §5).
This version of the SRD attains these identified necessary inputs, with a later version of the SRD introducing further products as input, following identification of the winning algorithm and associated data for Task 2 ([RD2] §5).
Recognition of types of auxiliary data input by analysing ([RD2] §5).
Recognition that the system must understand data as well as metadata ([RD2] §5).
The system needs to understand future data needs, not just accommodate existing data needs, so pointing towards the need for scalability of the operational system.
The system should, for instance, be able to anticipate Earth Observation data from the Sentinel cluster ([RD2] §1).
Absorption of business goals within the Statement of Work, representing the axiomatic needs of the system2.
These relate to expectations of the system directly relevant the operational system scope ([RD7] throughout); see §6.2 for mapping detail per business goal.
We offer a direct mapping from these attained business goals to the System Requirements Baseline, so preserving ESA’s vision of the operational system.
These sit at a meta-level to other system requirements, describing the design requirements and implementation constraints of the operational system; [RD7] throughout, see §6.2 for mapping detail per business goal.
Although these system requirements for a Phase II system are derived from a Phase I CCI, there is still a good distillation of information possible in terms of pointing to system requirements for the Phase II system ([RD7] §2.5).
We draw on ECSS-Q-ST-80C (Space Product Assurance) towards establishing reasonable expectations for software quality which should be satisfied by the future implementers during Phase II; these are not only beneficial in order for the Phase I SSD to be satisfied, but of benefit to Phase II implementers needing to formally attain and verify good practice.
An analysis should be carried out as to pros and cons of reusing software ([RD10] §6.2.7.2).
Appropriate attention should be directed to reflect corrective actions ([RD10] §6.2.7.9).
We analyse the PSAD [RD 3] to support the construction of the System Requirements Baseline.
The objective of the PSAD was as an early analysis of how the Task 3 prototype may be used operationally, and conducted by each of the CCI ECV parties.
We also subsequently infer that a Reprocessing (REP) mode is required.
The PSAD defines two manners by which the operational system should absorb input data – passive and active acquisition.
This information, together with the DARD analysis, allows the introduction of system requirements pertaining to the manner in which each of the expected operational system inputs should be acquired ([RD3] §3.3).
Interpreting the PSAD together with the DARD, allows the formation of system requirements pertaining to the data and database, such as the size of particular foreseen databases.
Given the need to design an operational system needing to be capable of accommodating future mission data, points to consideration of scalability of the system ([RD3] §3.3, §3.4, §3.5).
We are active participants in the SEWG, and support ESA in their leadership of the group.
A vehicle for collaboration which we have introduced into discussion is the concept of a Central Information System, a potential mediator across ECV systems and proxy to the ECV portfolio from the outside world.
To that end, we have produced a Technical to the SEWG relating to pan-ECV collaboration in the context of such a mechanism [RD 11], and will continue to endeavour towards exploration of a reasonable solution for attaining cost-effectiveness across the ECVs.
This document recognises the efforts of the Data Standardisation Working Group (DSWG), towards reaching consensus on a common format of data product across the ECVs, targeting NetCDF and CF, archiving standards, and other operational matters as described in [RD 11].
Moreover, the argument for defining the concept of the Central Information System is earlier elaborated in the §7.2 introduction, in reference to the concept through §6.3 point (7), §6.5.1, §6.5.2, §6.8.2, §6.9.2 and §6.10.3.
This chapter represents the System Requirements Baseline of the SLCCI operational system, derived from the earlier requirements elicitation (Chapter 6) and subsequent analysis (Chapter 7), as illustrated by Figure 8-1.
Further requirements other than those directly elicited via DUACS system re-use are also added.
The system requirements are arranged across two dimensions, garnered from macro functionalities and ECSS standard E-ST-40C ([RD 5], Annex D).
Of the ECSS engineering discipline branches, E-40 (Software Engineering) is observed as being the most considerably apt for exhaustive, complete, SLCCI requirement categories in relation to other ECSS discipline branches, and also appropriately envelopes relevant considerations in providing a software related system requirements realisation of ECSS-E-ST-10C (System Engineering General Requirements) clause 5.2, establishing linkage between E-ST-10C and E-ST-40C.
On one dimension lies a complete listing of relevant ECSS standard requirements groups (Figure 8-3).
The second dimension houses the complete list of macro functionalities proposed (Figure 8-2).
Applying one dimension to another allows a complete, exhaustive, view of ECSS standard requirements involvement across the SLCCI macro functionalities.
The application of all macro functionalities across the Functional and Performance ECSS requirements.
Functional and Performance requirements differ significantly across the macro functionalities.
That is, the SRD macro functionality system requirements groupings will ultimately map to SSD sub-systems, and for the design of each subsystem it is imperative to have the functional and performance SRD considerations treated per sub-system.
The application of all ECSS requirements groups other than Functional and Performance (i.e the remaining ECSS requirements groups) to the SLCCI system as a whole (i.e across macro functionalities as a whole); these requirements groupings sit more comfortably neutral to each macro functionality, so applicable to the system as a whole.
The result of these two grids is that all ECSS standard requirements groups are aptly measured against the SLCCI system.
Grid 1 observes Functional and Performance requirements across all ECSS requirements groupings.
Grid 2 observes the macro functionality neutral ECSS requirements groupings across the whole system.
The system requirements are formed in tables as illustrated below, with the following columns and associated abbreviated column names.
A list of traced items for the same system requirement is delimited with a comma character.
It is often the case that a system requirement is inferred from a number of sources – for example a system requirement related to the storage of certain processed mission data, may use the PSAD to infer the data identity, DARD to attain mission references and operational experience to attain the formation of the storage constraint specified.
The form of verification for the system requirement.
Also, with regards to the system requirements pertaining to the macro functionalities of the production chain, we have declared a single higher level system requirement for each macro functionality section which semantically composites all system requirements contained within its associated section.
We offer such higher level system requirements across the production chain as a convenient vehicle to describing the expectation of the associated macro functionality stage.
Such high level production chain system requirements are ded are satisfiable if and only if their constituent system requirements are entirely satisfied.
For each macro functionality section, we declare the high level functionality and its associated constituents diagrammatically.
The following functional system requirements have been positioned in this section for clarity and convenience as they essentially reflect the scoping of the SLCCI operational system production chain through (i) the inputs to the system, the (ii) outputs from the system, and the (iii) user expectations.
The Data Acquisition system requirements associated with functionality are comprised below.
Note that the required course of operational action for anomalous data acquisition events is as described by the Operational Requirements (§8.16).
If the application detects that it lacks at least one item of required before product generation can commence.
The Level2+ Cal/Val requirements associated with functionality comprise of the following.
The specific temporal threshold for an automatic quality check is calculated as a multiple of the amount of input data and a calculation weighting; for instance, if the calculation weighting is 0.1, and the amount of data flow is flow.
There are no Product Assessment system requirements associated with the performance of product assessment.
Product Management is associated with no performance requirements.
Production monitoring is further explained by SLCCI-SRB-REQ_6-019 and SLCCI-SRB-REQ_6-020.
Request Monitoring expectations are specified by SLCCI-SRB-REQ_12-010.
SLCCI-SRB-REQ_1-420 Data backup operation shall be automated where possible.
The system shall make provision for at least <x> TBs of disk storage dedicated to the product pipeline.
The system shall make provision for at least <x> TBs of disk download by SLCCI Product Users.
The system shall make provision for at least <x> TBs of disk storage dedicated for SLCCI product archiving.
These system requirements sit at a meta-level to others as they provide stewardship for the development of the system via the maintaining of the ESA vision of the operational system, and also include system requirements referring to approach of design and implementation which are appropriate at this early stage.
Note that system requirements associated with configuration control and system reuse are contained within §8.22.
SLCCI-SRB-REQ_15-010 Development of the system shall be undertaken with apt specification of the operational system.
SLCCI-SRB-REQ_15-020 Development of the system shall be undertaken with apt archives.
SLCCI-SRB-REQ_15-030 Development of the system shall be undertaken with apt consideration for quality of input data from EO archives.
SLCCI-SRB-REQ_15-040 Development of the system shall be undertaken with apt cal/val data and documentation.
SLCCI-SRB-REQ_15-050 Development of the system shall be undertaken with apt data and documentation.
SLCCI-SRB-REQ_15-060 Development of the system shall be undertaken with apt missions.
SLCCI-SRB-REQ_15-070 Development of the system shall be undertaken with apt sensors.
SLCCI-SRB-REQ_15-080 Development of the system shall be undertaken with apt validated during the project.
SLCCI-SRB-REQ_15-090 Development of the system shall be undertaken with apt missions to be integrated during the project.
SLCCI-SRB-REQ_15-110 Development of the system shall be undertaken with apt Europe.
SLCCI-SRB-REQ_15-130 Development of the system shall be undertaken with apt consideration for availability of external validation data.
SLCCI-SRB-REQ_15-140 Development of the system shall be undertaken with apt covered by existing operational projects.
The Security and Privacy system requirements are comprised as follows.
Note that the provision for authentication has been made in §8.13.
The Software Quality system requirements are derived from ECSS standard ECSS-Q-ST-80C.
The following table summarises the mapping between system requirement ID and associated traceability.
The system required ID is coloured red if the associated system requirement refers to a value yet to be determined; these cases are also listed under the “Lists of TBD”.
The following table represents Appendix B (« List of Requirements ») of [RD 9], summarising the list of DUACS system requirements as realised for My Ocean SL TAC.
The system detects new data flows (input altimetry level 2 data and auxiliary data).
The system checks the synchronization between the input data and the auxiliary data (the dependency between input data and auxiliary data needed for the processing).
In nominal case, input and auxiliary data are present and the system acquire them.
The Support Operator analyses the source for the detection of the abnormal flow.
Degraded level 2 data will degrade the quality of the output products.
The decision to take into account the new level 2 data must be taken by the Support Operator and approved by the Service Manager.
The acquisition process is forced by human intervention.
The products of degraded quality are not reprocessed but new products will use the new Level 2 data once they have been acquired.
The auxiliary data needed for the input data may not be present.
The input data are supposed to be acquired on a given date.
The system can wait for the acquisition until a defined delay.
When the delay is expired, the support operator is automatically warned that the data flow is too late.
The unavailability of the input data is temporary (incident on an altimeter): the output products can be generated but with a degraded quality.
The information on the quality of the products has to be reported to the Top Level Service Desk through the SL TAC Service Desk.
It may also be reported in the dynamic metadata of the products TBC, upon Service Desk Manager decision.
The unavailability of the input data is definitive (loss of one altimetry mission): the products of the corresponding mission will not be produced anymore.
The incident has to be reported to the Top Level Service Desk through the SL TAC Service Desk since it impacts the products available for the users.
The data are homogenized by using the suitable geophysical corrections to calculate the Sea Level Anomaly for each altimetry mission.
He warns the upstream data centers of the anomaly detected on the input data flows.
This error can be corrected by a new acquisition of the input flow that was initially corrupted (not detected at the acquisition level but at the step of the input checks).
The information on the quality of the products has to be reported to the Top Level Service Desk through the SL TAC Service Desk.
It may also be reported in the dynamic metadata of the products TBC, upon Service Manager decision.
The SL TAC system has to be modified to cope with the new configuration of input data.
The decision for the modification must be validated by the Service Manager.
The organization for such a scenario is described in the Service Management Plan.
The statistics are gathered in a synthesis report to be checked by the Product expert twice a week.
The validation loop may need to run several times the production chain.
If the anomaly can be corrected in the SL TAC, the production chain is run again (the Use Cases which are impacted).
If needed, the Support Operator reports the anomaly to the Service Manager.
The system calculates the different KPI for the SL TAC.
The list of the KPI is described in the Service Management Plan.
The KPI are available on a daily basis, for NRT products only in V0 and V1.
The system calculates the different Ocean Indictors for the SL TAC.
The list of the Ocean Indicators is described in the Service Management Plan.
The system calculates interpreted indictors for the SL TAC, which are derived from KPI and scientific metrics.
The product is distributed, even if the metadata are not updated.
If the FTP server fails, an incident has to be reported to the SL TAC Service Manager.
Once it is repaired, the products are available to the users but with delayed time.
The decision and the archive procedure is automatic.
Authorization of the "Product" (so that MyOcean can start ?logging?
Each update (creation/deletion of a Product Line/Product specification, authorization of a Product) of the Products Database by the SL TAC Product Manager has to be reported to the Top-Level Product Manager for a validation (particularly regarding the state of a Product: operational or not).
The Product Manager must also register as Product Lines the upstream data that are delivered by providers external to MyOcean v1.
Such Product Lines won't have associated Product Specifications and are necessary to the product dependencies.
The requests may be addressed via different communication means (for example through a form), and processed by a dedicated service (Service Desk, at top level) and are cascaded to the SL TAC Service Desk in case of requested information about SL TAC.
An answer may be returned to an information request via different communication means and within a variable time span depending on information type or availability.
For example, user might be directed towards online system capabilities for general information.
As a single point of contact, the MyOcean Web Portal, is offered to users to get automated information, integrated system capabilities allow Support Contributors to automatically manage and publish information on this Web Portal.
Top Level dashboard shows consolidated indicators for all Top level Services.
Several level of automation might be implemented from fully manual to advanced instrumented monitoring.
The monitoring can be either an automated procedure or human action.
The production is monitored in the SL TAC Production Center.
This information is produced and validated by the Product Expert (see "Perform output checks and QC", "Do measures and build indicators on products" Use Cases).
The list is described in the Service Management Plan.
The input data and products of SL-TAC are archived by CNES.
The following physical interfaces and conventions are required by the SL TAC from the MIS: v1.
The SL-TAC system shall comply with the security policy of all projects CLS carries for CNES, as SL-TAC activities are also carried out in the DUACS framework of SALP, the multi-mission center of the French space agency.
The SL-TAC system shall prevent any disclosure of information to unauthorized individuals or systems.
This requirement is applicable to the Production Unit (limited access to internal documents, products and internal data) and to the Distribution Unit (limited access to Service Desk documents, database, and MIS Gateway configuration).
The SL-TAC system shall prevent modification without authorization of any data.
The SL-TAC system shall prevent any conflict between concurrent operator requests.
The SL-TAC system shall be activated with scripts and command lines on Linux OS.
Automated sequences shall be defined for routine operations.
Internal or confidential documents shall be centralized in an access restricted area.
Accessibility shall remain a practical concern of the SL-TAC.
The SL-TAC system shall be configured for strict monitoring, by favoring false alarms to undetected errors.
The SL-TAC system shall be able to detect problems and to send an automated warning within <<one hour>> of the event.
The SL-TAC system and contributors shall try and provide a level of service availability as close to nominal in working hours as possible.
The backup system shall be located either in CLS’ disaster recovery data center located within the CNES campus, or in CLS’ ARGOS backup data center in Washington.
The SL-TAC system shall be designed to minimize the risk of central failure and to create, as many workaround solutions as possible if sub systems are unavailable.
The only exception to this rule are major incidents and new versions the SL-TAC system.
In both cases, the intervention must be discussed and controlled by system and scientific experts.
All possible measures to ensure integrity is not compromised shall be taken.
The end-to-end Near Real Time production shall be able to process an innovative data flow of up to <<4 days from 4 altimeters>> every day.
The processing sequence shall also include up to <<40 days>> of older data not changed since the previous production.
The SL-TAC shall be compliant with INSPIRE European Directive.
The SL-TAC shall be compliant with CNES’ SALP project rules and guidelines.
The SL-TAC shall be compliant with French and European laws on the following topics: privacy and right to own data about one’s account (respect to France’s CNIL regulations), intellectual property and copyrights, IT fraud control, cryptography and electronic identity, legal duration of data retention.
The SL-TAC production system shall expand on the pre-existing CNES/CLS DUACS system (both in DT and NRT) and be compliant with its design interfaces and constraints.
The SL-TAC distribution system shall expand on the pre-existing CNES AVISO system and be compliant with its design interfaces and constraints.
The overall goal of the DESSIN ESS software system is to support users implementing the DESSIN ESS evaluation framework and sustainability assessment, as described in the DESSIN Cookbook (D11.2).
This document describes how the software should do this, from the end-user perspective.
The document is organized into so-called “user stories”.
Each user story describes a task that a user would like the software to perform, and explains why performing the task provides value to the user.
The users stories are organized into “epics”, which are groups of similar stories.
There are five epics, one for each of the five parts of the DESSIN Cookbook (D11.2).
Most of the user stories are written from the perspective of an evaluation lead carrying out an ESS assessment, as this was thought to be the most likely user of the system.
The software framework presented here was developed by DHI, ECOLOGIC, SINTEF, and IWW in collaboration with the DESSIN user group, which consists of the demo site representatives who will be the end users of the software.
The overall goal of the DESSIN ESS software system is to support users implementing the DESSIN ESS evaluation framework and sustainability assessment, as described in the DESSIN Cookbook (D11.2).
This document describes how the software should do this, from the end-user perspective.
The document is organized into so-called “user stories”.
Each user story describes a task that a user would like the software to perform, and explains why performing the task provides value to the user.
User stories are accompanied by acceptance criteria that define when a story is complete.
The users stories are organized into “epics”, which are groups of similar stories.
There are five epics, one for each of the five parts of the DESSIN Cookbook (D11.2).
There is not a one-to-one correspondence between the user stories and the “steps” of the cookbook because some steps were too complex to fit into a single coherent user story.
A number of potential user types were considered in the development of the user stories presented here, from scientists to SME representatives to technical specialists with computer programming expertise.
However, most of the user stories are written from the perspective of an evaluation lead carrying out an ESS assessment, as this was thought to be the most likely user of the system.
The software framework presented here was developed by DHI, ECOLOGIC, SINTEF, and IWW in collaboration with the DESSIN user group, which consists of the demo site representatives who will be the end users of the software.
Although the software is targeted to this user group, it is hoped that the software will go on to be used by others performing ESS and sustainability assessments after the conclusion of DESSIN.
This document describes what the DESSIN ESS software valuation software should do, from the end-user perspective.
The purpose of the document is not to provide technical details for the software implementation, but rather to outline what the software should be able to do, and how the software should appear.
The software requirements presented here are the result of consultations with individuals involved in developing the DESSIN ESS and sustainability assessment methodologies developed as part of DESSIN work package 11.
In addition, the software requirements were refined through consultation with the end-users of the software (i.e., representatives of the DESSIN demo sites).
This document presents software requirements in a series of “user stories”.
Each user story describes a task that a user would like the software to perform.
In addition, each user story explains why performing the task provides value to the user.
Note that the example states what the user would like to do (“order and item”) and why the user would like to do it (“receive what I want”).
User stories are accompanied by “acceptance criteria”, which outline what the functionality described in the story should be able to do when implementation is complete.
In other words, acceptance criteria describe when a user story is complete.
In this document, user stories are organized into “epics”, which group similar stories.
Together, the different epics support the overall goals of the software system.
A conceptual diagram of the organizational structure is provided in Figure 1.
The overall goal of the software system is to support users implementing the DESSIN ESS evaluation framework and sustainability assessment, as described in the DESSIN Cookbook (D11.2).
The user stories represent the different tasks that should be carried out to implement the methodology in the DESSIN Cookbook (D11.2).
There is not a one-to-one correspondence between the user stories and the “steps” of the cookbook because some steps were too complex to fit into a single coherent user story.
As part of the development of user stories, the WP23 partners identified potential users of the software.
All of the user stories are written from the perspective of one or more of these users.
A list of user profiles and a brief description of each user is provided in Table 1.
Most of the user stories presented in this document are written from the perspective of the evaluation lead, as it was thought this profile is representative of the most likely user of the software.
Because resources available to the DESSIN project do not allow for implementation of all of the user stories described in the document, it is also necessary to prioritize.
Must: Stories labeled as “must” are stories that must be included in the Final solution.
All stories labeled as “must” will be implemented in the final version of the software tool (D23.2).
Should: Stories labeled as “should” are stories that should be included if possible.
These stories are not essential for fulfilling the purpose of the software; however, not including them may force users to undertake time-consuming work-arounds.
All stories labeled as “should” will be implemented in the software tool if resources are available after implementation of the stories labeled as “must”.
These stories are not essential for fulfilling the purpose of the software, and work-arounds are relatively easy.
All stories labeled as “could” will be implemented in the software tool if resources are available after implementation of the stories labeled as “must” and “should”.
User stories are also given an estimate of the level of effort required to implement each story.
The purpose of the first epic is to prepare the evaluation by delineating general basic characteristics of the study area including: the geographical location and spatial extension; the intended audience and expected results of the assessment; and to gather economic and demographic information.
The software must provide the possibility of entering information on the geographical location and spatial extension of the study area.
The user is prompted to select from a list of Eurostat-defined geographical areas.
The user is able to download population and density data from Eurostat for the region selected in 1.3.
The user is only able to download data for the most recent year available (2014).
The user is able to download GVA data from Eurostat for the region selected in 1.3.
The user is only able to download data for the most recent year available (2014).
The user is able to download employment data from Eurostat for the region selected in 1.3.
The user is only able to download data for the most recent year available (2014).
Each text entry possibility should offer some links to information sources.
The links should be to datasets that cover all of Europe with appropriate local-scale detail.
The links are the same as the links defined in 1.3 and 1.4.
This can be an alternative solution if 1.3 and 1.4 cannot be implemented because of resource constraints.
Each text entry possibility should offer some guidance on what should be entered.
I would like a separate entry possibility for text describing each characteristic needed to provide an overview of the study area.
Refer to Table 1, row 3 of Cookbook for complete list.
This section presents mock-ups of the visual appearance of the stories outlined in Epic 1.
Epic 2 represents the first step in the core evaluation and is the entry point towards describing the entire DPSIR cycle.
Here the relevant Drivers and Pressures are identified in order to understand the full picture of the system under study.
This enables the user to decide which Pressures to focus the rest of the evaluation on, and provides initial insight on what appropriate measures could be.
The purpose of this epic is to produce a qualitative overview of the Drivers present in the study area, relate these to resulting Pressures, and describe the latter.
As a rule within the DESSIN assessments, Pressures should be described qualitatively.
In specific cases where the proposed measures are expected/found to influence Pressures, then changes in those Pressures should be quantified.
The list of drivers in the DESSIN Cookbook (D11.2) is available to the user.
The user selects from the list and is prompted to enter a specification for each selected driver.
A specification is made by entering text in a text field.
The software should provide the possibility of entering additional driver types as text.
The user should then be prompted to enter a specification for each additional driver.
The user must provide a specification about each case-relevant driver.
The user is provided with examples from the mature sites.
The user receives a message to consider using information from Part I – Study description (e.g. economic activities found to be taking place in the study area).
A list of associated pressure categories is generated for each driver.
The user is presented with the list and is prompted to select those relevant in the study area and enter a specification for each selected pressure.
For those additional drivers that were inserted by the user in 2.2, the software should present the full list of pressure categories for the user to associate the additional drivers to one or more resulting pressures.
The user is presented with Table 2 of the DESSIN Cookbook (D11.2).
The software should provide the possibility of entering additional pressure types as text.
The user should then be prompted to enter a specification for each additional pressure.
The user is prompted to associate the new pressure to the list of drivers.
The user must provide a specification for each case-relevant pressure.
The user is provided with examples from the mature sites.
The user is prompted to use information from Part I – Study description (e.g. economic activities found to be taking place in the study area).
The software should provide the possibility of navigating back to this screen from the response analysis screen in order to include additional quantitative information to the description of case-relevant pressures.
This section presents mock-ups of the visual appearance of the stories outlined in Epic 2.
Epic 3 describe the Responses (i.e. the proposed measures) that can be implemented to address the problems in the study area, as identified in Part II.
It also aims to identify the case-relevant ESS (i.e. the ESS hypothetically affected by the proposed measure).
Finally, ESS are linked to Beneficiaries, and this information is used to categorize the case-relevant ESS as Final ESS or Intermediate ESS.
I can provide an expected lifetime that is known to the system in a number format.
I can describe the capability qualitatively in a text field.
I can describe if the capabilities are theoretical or tested.
I can select drivers from the list identified in part II.
I can select pressures from the list identified in part II.
I can select state parameters from the list on worksheet “State indicators” in the supplementary material file for each measure.
The list is grouped using the same system that is used in the supplementary material file.
I am able to read a definition of each state parameter.
I am able to enter the name of an additional state parameter.
I am required to provide a description of the parameter.
A list of case-relevant ESS is generated based on my selection in 3.5.
The list is generated by the links provided in the State-Impact I Provision table in the supplementary material.
The list consists of ESS classes in the CICES system.
For each additional state parameter identified in 3.6, the user is provided with the full CICES list and prompted to select ESS classes affected by the additional state parameter.
The software should provide text entry possibility to allow for a description of each case-relevant ESS that is more detailed than the CICES ESS class titles.
For each case-relevant ESS, I select beneficiary types that benefit from that ESS.
I select from the list given in column C of the worksheet DESSIN Beneficiaries-Final ESS in the supplementary material.
The list also includes the information provided in columns D, E and F in order to assist me with the selection.
The software must allow the user to compare each entry in the stakeholder list created in Part I to each beneficiary type from the ones selected in 3.8 (i.e. the ones associated to each case-relevant ESS).
The user must then assign a beneficiary type to each stakeholder in the study area.
The choice can include “none”, as some stakeholders might not fall within any of the categories listed in the subset of beneficiary types.
Each case-relevant ESS that has a beneficiary type that could be associated with a stakeholder is classified as Final.
I am presented with a list of case-relevant ESS classified as Final and Intermediate and select the ones that will be analysed further in Part IV.
This section presents mock-ups of the visual appearance of the stories outlined in Epic 3.
The purpose of Epic 4 is to assess the effect of the proposed measure (Response) on the system under examination by quantifying the state of the ecosystem, Impact I (ESS provision) and Impact II (ESS use).
State, Impact I and Impact II have to be estimated for two scenarios: a baseline scenario (before) and one where the proposed measure is already implemented (after).
Finally the scenarios are compared and the change in the elements of the DPSIR is evaluated.
The software must retrieve the parameters of State previously found to be affected by the proposed measure.
These must be presented in relation to the case-relevant ESS for clarity.
The user is presented with a list of examples including those in column D of worksheet “State indicators” and columns C and D of worksheet “Impact I Provision indicators” in the supplementary material file.
The user is prompted to assign State indicators to each of the case-relevant parameters of State based on the examples presented or the creation of custom indicators.
The user is prompted to create custom indicators using text entry.
The user is presented with information on the existing State indicator scripts from the indicator script library in MIKE WORKBENCH.
The user can select existing State indicators from the library and use them in his model.
The user has access to the scripting capabilities of MIKE WORKBENCH.
The user can load existing scripts or script new state indicators to be used in the analysis.
The user is presented with a list of examples including those in columns C and D of worksheet “Impact I Provision indicators” in the supplementary material file.
The user is presented with information on the existing Impact I indicator scripts from the indicator scripts library in MIKE WORKBENCH.
The user can select existing Impact I indicator scripts from the library and use them in the analysis.
The user has access to the scripting capabilities of MIKE WORKBENCH.
The user can load existing scripts or script new Impact I indicators to be used in the analysis.
The user is presented with the list of case-relevant Final ESS to be further analysed (stories 3.9 and 3.10), showed according to CICES section, class and class type.
As guidance, for each listed ESS class type, the user is presented with examples from column G (Examples of Impact II (ESS Use) Indicator(s)) of the Impact II Use indicators worksheet in the Supplementary Material File.
In level I, the user is presented with a list of valuation method examples including those in column H (Valuation Method(s)); and references from column I (Data/Literature) of the Impact II Use indicators worksheet in the Supplementary Material File.
In level II, the user is presented with extended information for any given study of his/her choice from column I (Data/Literature) of the Impact II Use indicators worksheet in the Supplementary Material File.
The extended information about each specific study can be found in the Impact II Monetization worksheet in the Supplementary Material File.
This is the studies table (all columns) and abstracts table per study.
A table similar to Table 11 in the DESSIN Cookbook (D11.2) is generated with the first two columns populated with the Final ESS and Beneficiaries identified in previous steps.
The user has the capability to load the models, datasets, etc.
The user should have the capability to access and adapt existing datasets and tools from the MIKE WORKBENCH libraries.
The user must be able to run simulations and compute the previously selected indicators using the loaded data.
The user must be able to define the time range for which the indicators are quantified.
The user interface should be targeted at the Evaluation Lead or SME (i.e. non- expert users).
The user must be able to compare between the results of the baseline and after implementation scenarios with ease.
I can export results to Excel so that I can make custom plots and other reporting tools.
This section presents mock-ups of the visual appearance of the stories outlined in Epic 4.
The purpose of this chapter is to supplement the ESS evaluation by advising how to conduct an additional sustainability assessment (SA) of innovative solutions aimed at mitigating water scarcity or water quality issues.
The SA allows the user of the DESSIN ESS Evaluation Framework to widen the analysis, putting the evaluated changes in ESS into perspective by considering multiple dimensions.
These multiple dimensions include wider social, environmental, financial, governmental, and asset performance aspects of the examined solution.
This allows for the consideration of potential disadvantages like costs and environmental effects (e.g. additional greenhouse gas emissions) and their comparison with the advantages in terms of benefits expected from implementing the solution.
I am able to make use of the data inserted in part I (e. g. system boundary and information about economic activities).
The SA uses the same Eurostat region definition that was selected in Epic 1.
I am able to specify the number of technologies that will be compared.
I am able to specify whether the system is a water supply or wastewater system.
I am able to specify a lifetime for each technology under consideration.
I am able to specify a start-up time for each technology under consideration.
I am able to define a common start time for the analysis.
I am able to define one or more times in the future when I would like to take a snapshot of the performance of each technologies under consideration.
All times are time periods are defined in units of years.
The indicator list should contain all indicators from the sustainability indicator list grouped by dimension, objectives and criteria.
I am presented with an indicator list that is filtered based on whether I am analysing one technology or comparing more than one.
I am presented with an indicator list that is filtered based on the water system type.
Each indicator is populated with a set of properties.
I am able to indicate data availability for each indicator by choosing between “yes” or “no”.
All indicators rated “yes” will be assessed quantitatively.
All indicators rated “no” will be assessed qualitatively.
An overview of my current sustainability indicator list and all specifications made up to this point is available.
I can add a regulatory threshold value where one exists.
I can populate all necessary properties for each indicator.
I am able to assign new indicators to a certain dimension, objective and criteria.
Two or more fields should be presented for each indicator: one for inserting a “before” value and one or more for the “after” values.
The number of "after" values should be the same as the number of snapshots of the future defined in 5.3.
The “after” values are labelled using the snapshot times defined in 5.3.
I can add a reference to each value inserted in a text field.
I can compute indicator values from indicator scripts.
The indicator scripts can accept time series input.
I am able to extract data from a model that I connected to MIKE Workbench.
I am able to select and take over single result values from parts III and IV.
I am able to calculate indicators based on data and time series inserted in parts III and IV.
Data that is taken from the ESS Evaluation part should be marked in the table with an automatic reference provided.
I can calculate GHG emission in three different ways: (1) those emitted directly from fossil fuel consumption during the solution use such as water pumping to the atmosphere; (2) those emitted indirectly from electricity consumption in the solution such as water pumping, water treatment to the atmosphere; (3) those emitted indirectly from material flux (resulted from embodied energy of materials) and chemicals used for treatment processes.
I am asked to insert the total fossil fuel consumption, the electricity consumption and the material and chemical fluxes for each solution.
I can select from a database the conversion coefficient as kg of CO2 equivalent per consumption unit and is specific for different energy types, chemicals and materials.
OR I am asked to specify the conversion coefficient, to be applied, as kg of CO2 equivalent per consumption unit and is specific for different energy types, chemicals and materials.
All the aforementioned GHG emissions are calculated by multiplying the amount of energy, chemical and material consumed by a conversion coefficient for that specific energy, chemical and material.
Results of the life cycle assessment are directly allocated to the respective indicator of En213.
I can select indicators from those calculated in the financial dimension.
I can define specific interest rates, a discounting rate and the time horizon of the analysis.
I can introduce the values for "r", the discount rate, such as the rate of inflation, and for "t", the number of compounding periods, such as years into the future.
If the number is higher than 0, then cost coverage is guaranteed.
I can describe each indicator qualitatively in a text field.
It is possible to add a score value to each indicator referring to a scale from 1 to 5 (strong negative impact – some negative impact – neutral – some positive impact – strong positive impact).
I can compare the performance per indicator of each solution for the baseline scenario and the after implementation scenario (to the regulatory threshold) in a bar chart.
The after implementation scenario can be represented by one or more of the periods for snapshot analysis selected in 5.3.
It is possible to view results at two or more points in the future.
I can compare the performance level of each solution for several indicators for the baseline scenario and the after implementation scenarios by comparing the indicator values from different points in each time series.
The indicators should be normalized to a regulatory threshold.
I can choose which indicators should be presented in the graph(s).
I can export results to Excel so that I can make custom plots and other reporting tools.
I can select dimensions for each scenario (before and after), and then select relevant indicators for each dimension.
All indicator values are normalized automatically after the user defines if the highest or the lowest value is the best for each.
I can add a weighting on the indicator level, the criteria level or the dimension level.
I can select dimensions for each scenario (before and after), and then select relevant indicators for each dimension.
I do not only aim at ranking those indicators derived directly from the SAT but also anything else that is incorporated into the host environment.
For each scenario the calculated values of each k-th indicator (fk(x)) are available.
I can select the indicators, from those calculated in monetary terms to estimate the cost part of the analysis.
I can select the indicators, from those calculated to be used to estimate the benefits side of the analysis and convert them in monetary terms.
I can define specific interest rates, a discounting rate and the time horizon of the analysis.
I can introduce the values for "r", the discount rate, such as the rate of inflation, and for "t", the number of compounding periods, such as years into the future.
I can calculate the discounted value of each cost and benefit using the following net present value formula: NPV = value / (1 + r)^t.
I am able to add each present value of cost and benefit.
I can divide the present value of benefits by the present value of costs.
If the number is less than 1.0, then the cost- benefit analysis is negative.
If it's greater than 1.0, then there's a positive return.
This section presents mock-ups of the visual appearance of the stories outlined in Epic 5.
The software specifications document presented here represents the consensus of partners involved in DESSIN Work Package 23: Software Framework for ESS valuation.
Partners include parties involved in the development of the DESSIN ESS evaluation framework (WA1) as well as partners who will apply the software tool as part of the implementation of the Demo cases (WA3).
Because the software requirements have been developed in close consultation with the developers of the ESS evaluation framework as well as the end user group, it is anticipated that the resulting software tool will be useful to the Demo site partners when they apply the DESSIN evaluation framework at their sites.
Furthermore, the specifications team have been mindful of how the tool will be used after the conclusion DESSIN, and have attempted to create a tool that will be broadly accessible to researchers and practitioners in Europe.
This document relates to the Phase 1 SKA Signal Transport and Networks Domain Element and its Sub‐elements.
It is of a maturity commensurate with a Concept level of definition of the STaN Domain and the SKA Observatory as a whole.
It also forms the working basis of the Domain Requirements Document to be prepared for the future System Requirements Review, and its Table of Contents is intended to be subject to the present Review.
The purpose of this document is to provide a summary of all flowed, derived, allocated and introduced Requirements pertaining to the full life cycle of the Domain.
The following documents are applicable to the extent stated herein.
In the event of conflict between the contents of the applicable documents and this document, the applicable documents shall take precedence.
The following documents are referenced in this document.
In the event of conflict between the contents of the referenced documents and this document, this document shall take precedence.
The interconnecting lines define the association between the blocks in the diagram.
Quality: A quality requirement is to change the way something is done.
Constraint: Constraints are restrictions or limitations on possible solutions.
Each of these requirements is to satisfy a goal owned by a particular stakeholder that plays a role in a scenario of the system.
Scenarios are associated with the modes and configurations of the system and define the dynamic behaviour.
This document includes Use Cases to provide a path to discovering associated requirements.
As detailed in Figure 2 a requirement comprises of more information the just the requirement description.
This information in effect forms attributes for the requirement.
The Requirement ID provides a unique identifier for each individual requirement.
The sting provides a unique descriptor identifying the item within the systems hierarchy that the xxxx is a four digit decimal number uniquely identifying the requirement within the requirement set.
The requirement should be a single active sentence as short as possible.
The requirement should focus on naming a single desired result Requirements should avoid conjunctions such as: “and”, “or”, “with” and “also” as these tend to wrap multiple requirements into one which is not desirable.
Requirements should not specify the design envelope.
As stated in the requirement description, all requirements are to be verifiable.
The method of verification is to be attached as an attribute to the requirement.
The priority of the requirement is to be attached as an attribute to the requirement.
Requirements are not static statements but have a life‐cycle.
The originator of the requirement should be attached as an attribute.
The date that the requirement was created should be attached as an attribute.
Making assumptions explicit and connecting them to an argued rationale enables decisions to be re‐ visited without starting all over again.
An understanding of rationale enables accurate prioritisation and is an aid to preventing essential requirements from being deleted.
The requirements for STaN form part of the overall system hierarchy as illustrated in Figure 3.
Each tier in the hierarchy has its own set of requirements which are derived from the next hierarchical tier above.
There is also a feedback path via the architectural design process to inform the requirements at the higher tier whether there are any issues.
The flow‐down and feedback is an on‐going process iterating towards a stable and eventually base‐lined requirement set.
The initial requirements for the concept phase of STaN Sub‐Element level is the scope of this document.
It identifies the subset of concept phase system requirements that are applicable to STaN and presents additional requirements where there are gaps.
This process forms part of an iterative feedback path to the system level.
In the next phase these requirements will be refined so that they can be utilised by each of the STaN Sub‐Elements.
The aim of the next phase in the project is the definition of the requirements.
The quality, design, development and other requirements will be developed in the next phase and the constraints identified.
These will be presented at the STaN System Requirements Review.
In this phase requirements analysis and validation are undertaken in order to ensure that the complete set of requirements is understood and is present.
Gaps will be identified and actions to address these shortcomings will be initiated.
The result of these activities will be captured in the relevant Requirement Specifications to be reviewed at the conclusion of this phase.
Architectural design activities will also be initiated with the aim of producing a first draft design document at the end of the phase.
Interfaces will be refined and finalised as far as possible (especially functional interfaces).
This phase will be concluded by the (Sub) System Requirements Review (SRR).
It shows how the STaN Sub‐Elements fit within the STaN domain.
This section provides an Applicability Matrix showing the Applicability of AD[1] Requirements to the STaN Domain and whether the requirement has been analysed in the scope of the STaN CoDR.
SKA1 shall be able to measure electromagnetic radiation in a frequency range from 70 MHz to 3 GHz.
The SKA Phase 1 shall be designed so that the fractional instantaneous bandwidth is comparable to the observing frequency.
It shall be possible to position this band anywhere within frequency band is a contiguous (TBC) band selected from the total frequency range.
The resolution with which the 500 MHz and 1 GHz bands can be selected shall be TBD or less.
It shall not be possible to select different digitized bands for the two polarizations of a single dish/antenna/array.
The subband bandwidth after station level beamforming shall be less than TBD Hz.
The digital processing capacity shall be sufficient to process all sub‐bands (Q: and beams, and polarizations, or should there be exchangeability).
The phase relations between the sub‐bands and channels within a beam shall be known to such a precision that wider bands and corresponding time series can be reconstructed from sub‐bands and/or channels.
The SKA Phase 1 shall be designed so that the bandpass does not show ripples or systematic fluctuations, on scales smaller than a frequency corresponding to about 300 km s1, that are larger than twice the thermal noise level after an integration of 1000 hr.
SKA1 shall offer a spectral resolution in each polarization for Spectral resolution.
SKA1 shall offer a spectral resolution in each polarization for science processing of: 100kHz in the band 70 to 240 MHz; ‘This requirement follows directly from the radial resolution science requirement.
For reference, assuming the concordance cosmology, at these redshifts, the co‐moving length is given by ≈ 1.7 Mpc (ν/100 kHz).
Therefore, to match the angular resolution a frequency resolution of Spectral resolution.
SKA1 shall offer a spectral resolution in each polarization for science processing of: 1 kHz in the band 70 to 240 MHz; ‘In practice a more stringent requirement of 1 kHz in frequency resolution is required to identify and excise RFI, reduce bandwidth smearing, and calibrate ionospheric effects.’ Sub‐band and channel phase relations.
The signal processing performed on each sub‐band shall leave the relative phases of subbands and spectral channels intact or predictable.
SKA1 shall have a spectral dynamic range of: ≥61 dB in the Spectral dynamic range.
SKA1 shall have a spectral dynamic range of: ≥43 dB in the Sensitivity (Aeff/Tsys).
The SKA1 shall have a sensitivity of: 103 m2 K‐1 in the Sensitivity (Aeff/Tsys).
The SKA1 shall have a sensitivity of: 103 m2 K‐1 in the Sensitivity (Aeff/Tsys).
The SKA1 shall have a sensitivity of: 105 m2 K‐1 in the Survey speed.
The SKA1 survey speed requirement is: ~107 m4 K‐2 deg2 for the Survey speed.
The SKA Phase 1 shall be designed so that a major survey can be completed in 2 years of “on‐sky” observation time.
The SKA Phase 1 shall be designed so that a deep field can be completed in 1000 hr of integration time.
The magnitude and phase variations of any SKA1 compound beam over a 12 hours period at any point of its half‐power contour shall be less than 1% (TBC) relative to the beam peak.
The SKA Phase 1 shall have an attainable time resolution of at least as short as 50 μs.
Changing the beam former weights shall be possible every 60 seconds (TBC) in the case of scheduled switching sequences.
Changing the beam former weights shall be possible within 60 seconds in case of changes due to manual interaction or changes in schedule.
Observation data (specify: both uv(w)‐data and tied array beams) acquired during a change of beam direction shall be flagged.
The SKA shall be able to provide ‘near simultaneous access to Beam polarization stability.
External calibration measurements shall be necessary at a rate of no more than once per hour (TBC).
SKA1 shall provide visibility data in all four Stokes parameters.
The polarisation introduced by the calibration, shall be less than 0.5% of the total intensity.
The SKA1 shall have limited (TBD) susceptibility to bursty/spiky RFI (for pulsars, Transient RFI detection.
The post station level processing shall detect and flag invalid data.
It shall be possible to image the entire field of view Imaging dynamic range.
SKA1 shall be able to provide an imaging dynamic range for continuum imaging (thermal noise imaging to classical (micro Jansky (Jy)) confusion ‘studies of star formation at high redshift with a continuum deep field Dish beam absolute pointing accuracy.
The pointing accuracy of the dish beams is: AA beam absolute pointing accuracy.
The pointing accuracy of the AA beams is: TBD Dish beam pointing estimation accuracy.
The pointing estimation accuracy of the dish AA beam pointing estimation accuracy.
SKA1 shall provide a monitoring and control function.
The monitoring and control function shall ensure that all parts of the maintenance functions, are part of the M&C system.
The monitoring and control function shall ensure that failures in hardware, software or signal transport are detected and reported.
The monitoring and control function shall take autonomous action to ameliorate failures where possible and support a fail‐safe philosophy.
M&C shall take autonomous action in safety critical situations such as system power failure, over‐temperature, and storms (dish‐stowing).
The monitoring and control function shall give user transparent parameters.
The monitoring and control function shall be designed to operate the instrument fully remotely.
The monitoring and control function shall provide TBD performance monitoring data to users.
The monitoring and control function shall provide for a long‐term logging sub‐function with workflow support for the Operational Team and with sufficient information to relate system events to artefacts in the data.
It shall be possible to abort an observation if monitor parameters exceed user specified limits (including RFI mitigation performance indication parameters).
Individual element calibration information shall be available to the measurement function.
SKA1 shall have a control system that actively controls all system settings in the instrument.
The control system shall be capable of autonomously calculating system settings in response to changes in instrument status, environment or measurement results.
It shall be possible to activate the calculated system settings either automatically (autonomous control) or after explicit confirmation by the operator (manual control).
It shall be possible to specify when settings should be activated automatically and when they need to be confirmed by the operator.
It shall be possible to receive and accept updated schedules before the end‐time of the currently active schedule has expired.
It shall be possible to consolidate monitoring information to produce high‐level monitoring information from low‐level monitoring information.
Subsystems shall report completion of actions to M&C S&C summary reports.
It shall be possible for all user roles (specification of these roles TBD) to produce summarized historical monitoring information.
The results of control actions shall be verified with measurements made expressly for the purpose.
If the normal measurement sequence does not provide for control verification in a timely fashion, such measurements shall be made out of sequence.
It shall be possible to consolidate monitoring designated logical concepts like observation, correlator.
SKA1 shall provide a synthesis imaging mode where compound beams are correlated to form visibilities.
In synthesis imaging mode it shall be possible to form visibilities between all corresponding monochromatic compound beams (same frequency, same direction) from all dishes or all aperture arrays (stations).
This means that the central processing function should be able to handle the full data stream from the dishes or aperture arrays in synthesis imaging mode.
SKA1 shall provide a tied array mode where the signals from all dishes are phased up, after real‐time correction of instrumental effects, and transformed back into time series for pulsar processing.
In this mode the Autocorrelations of all single dishes / aperture (sub)arrays are recorded.
Each dish / sub‐array is tracking a different position on the sky.
SKA1 shall provide an aggregate mode in which bandwidth is exchanged for spatial coverage in the correlator.
SKA1 shall provide instrumental real‐time calibration functions in all observational modes.
It shall be possible to re‐process data retrieved from archive.
To which extent this will be supported needs further discussion.
SKA1 shall be able to produce final data products based on automated and interactive (manual) processing of acquired data.
SKA1 shall produce recordable intermediate data products, for example pulsar voltage time series and RFI statistics.
SKA1 shall be aimed to be operated continuously (7 days per week 24 hours per day).
It shall be possible for the operator to control and monitor the SKA1 instrument from the SKA station sites and core site.
The system shall provide security to prevent unauthorized physical access to facilities and resources.
Reconfiguration of SKA1 from one observational mode to another shall not take longer than 5 minutes (TBC) provided all software applications are present at their designated location.
It shall be possible to control all SKA1 functions from the operational centre, without requiring physical access to the instrument, including start‐up and shut down.
The start‐up of SKA1 functions shall follow a pre‐defined sequence taking not longer than: 10 minutes for a hot start (= restart) Start‐up sequence.
The start‐up of SKA1 functions shall follow a pre‐defined sequence taking not longer than: 24 hours for a cold start Start‐up and shut‐down individual antenna systems.
It shall be possible to start‐up or shutdown individual dishes or aperture arrays without disturbance [TBC] of routine operations.
The shutdown of SKA1 shall follow a pre‐defined sequence taking not longer than TBD minutes.
SKA1 shall also have an emergency shut‐down for wind (stowing dishes), lightning, and electric power anomalies.
Initialization of shut‐down and start‐up sequences shall be restricted to designated operators and engineers.
To be defined: security requirements on different access levels (e.g. engineering mode).
Any dependencies in the start‐up and shutdown sequences shall be automatically verified (so they do not depend on operator intervention).
The shutdown of pre‐defined parts of the SKA1 system shall have no (TBC) impact on SKA1 operations after appropriate re‐calibration performed automatically.
SKA1 shall be designed to enable an operational readiness check, including redundancies, prior to commencement of any SKA1 operations (initial check‐out).
The operational readiness check shall not take longer to complete than 5 minutes.
As far as possible, no single failure in the SKA1 shall lead to personnel safety hazards.
Failures in one of the SKA1 subsystems shall not lead to failures in other subsystems.
No single operator command shall cause catastrophic, serious, or major consequences.
No voltage‐transients or "cutoff" of lectrical power shall lead to catastrophic or serious consequences.
This includes voltage transients applied to the input of the receivers.
The absence of operator commands shall not cause catastrophic or serious consequences.
Single‐pointfailures in the design shall be listed.
Each‐single‐point failure in the design shall be justified, and assessed against alternative design(s) where this single‐pointfailure would not occur.
The correct functioning of each single‐point‐failure in the design shall be monitored by a watchdog function.
Failing equipment shall indicate the problem if power is on, and the control function shall take appropriate measures.
The status report of the functioning of a subsystem shall be available in 5 seconds.
The status report of a subsystem shall reflect the functioning of the subsystem at or after the operator request has been submitted to the system.
The status report shall display the status of a function, together with the system time the status was determined.
Each dish or aperture array system shall have the capability to answer to an operator interrogation, in case of detected failures at the dish, which antenna chain has failed.
The system shall have the capability to be operated by an operator in an autonomous mode, and in a manual control mode.
In the autonomous mode, all malfunctioning equipment and/or stations may be switched off autonomously, and a message with all details of this action shall be brought to the attention of the operator, and recorded in the systems log‐file.
In the manual control mode, the operator shall have the capability to switch on or off all equipment and/or stations.
Operator actions shall be recorded in the systems log‐file, in such a way that a complete picture of all correct functioning and/or all malfunctioning equipment, together with their operational and/or switch off statuses, can be achieved.
It shall be possible to take recovery actions without consequences for other parts of SKA1; the system shall minimize impact of recovery actions.
SKA1 shall be able to recover autonomously in case of failures that are classified as minor or negligible.
The SKA1 design shall ensure that disabled units do not corrupt the remaining system.
SKA1 shall be designed for a continuous operational period of 6 month.
After this time maintenance may be necessary, g. exchange/cleaning of airconditioning filters and refurbishment of cryogenic systems.
SKA1 shall be designed for a minimum life time of TBD years, including initial installation, testing and commissioning period.
The average availability of SKA1 during the operational period shall be better than 90% (TBC).
Availability is defined here as being available for scheduled observations in at least one of the supported operational modes.
Large scale maintenance and/or an upgrade shall give the possibility to reach a life time of 50 years (TBC).. Full fail rate.
SKA1 shall be designed to fully fail less than two times per year (TBC), the number determined as average over its operational period.
The maximum period of repair once a failure of SKA1 has been established, shall be 1 (TBC) week.
Here, a failure is defined as not being able to meet the scientific specifications due to (sub)system failure(s).
All users with scheduled measurements during the failure period shall be informed of the non‐availability of the system Data loss due to power outage.
All subsystems shall not lose more than 4 hours of acquired or processed measurement data (not yet permanently stored) as a result of an outage in the external power supply.
All subsystems shall have the capability to restart autonomously and without failures, after an outage in external power supply.
All subsystems shall be available within 5 minutes (TBC) after restart.
It shall be possible to replace all software/firmware configuration items in SKA1 through softwareupgrades, initiated by an engineer.
Software configuration items shall provide unambiguous inputs to allow the maintenance of a configuration management database.
The software identification shall be available to the operator within 10 seconds (TBC) after the request was made.
All subsystems shall include functions that allow maintenance of hardware and software.
The SKA design shall be fully compliant to all environmental rules applicable to the SKA site.
SKA shall be designed to have no lasting adverse environmental effects on the facility and site.
SKA shall be designed or protected against any deterioration leading to failure to meet the requirements specified herein caused by climatic and environmental conditions during its complete lifetime (both operating and non-operating).
The design of SKA shall be appropriate (TBD) for operation in the natural environment for the geographical deployment location of the SKA.
Buildings or parts of buildings containing central processing equipment and operator areas shall have a climatic conditioning system which can control the temperature within the range of 18 ºC to 23 ºC and the humidity within the range of 50 % to 70 % independent of weather conditions.
SKA equipment and operating facilities shall be adequately protected against intrusion by unauthorized persons or by “larger” wandering animals.
SKA equipment shall be able to operate without degradation of the performance during any type of precipitation (to be specified).
SKA equipment shall be adequately protected against performance degradation caused by contaminating particles (dust, sand etc), polluted air or any precipitation.
SKA equipment located at the dishes or aperture arrays or outside the central processing and operating facilities shall be able to withstand moisture and humidity levels up to 100 % RH.
SKA equipment located at the dishes or aperture arrays or outside the central processing and operating facilities shall be able to withstand (non-operating if necessary) an outside air temperature within the range of - Air temperature operation range.
SKA equipment located at the dishes or aperture arrays or outside the central processing and operating facilities shall be able to operate within specification if the outside air temperature is within the range of -5 ºC (TBC) to +50 ºC (TBC).
SKA equipment shall be able to survive wind velocities up to 160 km/hr (TBV), and shall operate within normal specification ranges for wind velocities up to 40 km/hr (TBC).
SKA shall not be damaged by RFI signals less than TBD V/m.
SKA shall not be susceptible to RFI signals, in-band or out-band, other than via the receptors.
The dynamic range of the ADC’s in the SKA shall be such that no clipping will occur.
Clipping occurs when the range of the input signal voltages to the ADC is larger than the ADC voltage range.
The number of ADC bits shall therefore be sufficient to prevent clipping due to strong interfering signals such as airplane DME and satellite signals.
The EMC safety margin, which is defined as the ratio between susceptibility threshold and the interference at any point within the system, shall be greater than TBD dB.
All "off-the-shelf" equipment applied within SKA shall posses as a minimum the host country EMC marking, including electrical and electronic supporting and infrastructural equipment.
A hybrid grounding concept as shown in figures TBD shall be used for EMC purposes.
Ground loops involving DC, and low frequency AC, currents shall be avoided inside the system.
Intentional currents through structure are not EMC design efforts.
Maximum effort (to be detailed) shall be put into designing signal interfaces to withstand noisy environments and to minimize the generation of excessive noise.
Interference due to selfgenerated RFI shall not degrade the performance of the instrument by greater than 1% by any measure (TBC).
SKA dedicated buildings and equipment located on sites shall be protected to minimize the effects of a direct lightning strike using certified methods (e.g. as described in NEN 1014).
Observation data taken during a lightning strike shall be flagged.
Electrical safety ground shall be designed according to the regulations imposed by the local government.
SKA equipment and buildings shall be protected against corrosion.
SKA electronics and connectors in areas with a higher air flow (for cooling) or outdoor environment shall be additionally protected against corrosion.
SKA equipment and buildings shall be protected against earthquakes with a magnitude up to 3.8 (TBV) on the scale of Richter.
The SKA1 shall be installed at the SKA core site and at the SKA station sites.
The SKA1 front‐end and cabling shall fit in the available feedboxes.
The total mass of any feed payload, including the RF cables to the ground, shall not exceed: TBD.
Each subsystem supplier shall establish, collect, review and deliver the Materials, Parts and Processes lists including all the Materials, Parts and Processes intended for use in the SKA1 equipment by his suppliers and himself.
Materials, Parts and Processes lists shall reflect the current design at the time of issue.
The estimated availability of the Parts and products obtained from Materials and Processes used shall be compatible with the final system’s life cycle (tests, storage, mission).
All materials used in the SKA1 design shall be fully compliant to all environmental rules applicable to the SKA1 core and remote sites.. Long‐term environmental effects.
Materials used in the SKA1 design shall not have any lasting effect on the site location.
Materials used for the parts subject to the outdoors environment shall be maintenance free.
Materials used for the parts subject to the outdoors environment shall be maintenance free.
Method of marking shall be compatible with the nature of the item and its use.
Identification numbers shall be marked on documentation and, where possible, on respective items.
The SKA1 shall connect to the available power distribution at the SKA core and remote sites.
The power consumption of all equipment at any AA or dish station, including the motors driving the dishes, shall be less than TBD kVA.
The total power consumption of the SKA1 observatory shall be less than TBD kVA.
SKA1 equipment and electronics shall be developed and produced according to the ISO9001 (TBC) quality standard.
The field return rate of equipment shall be less than 0.5% (TBC) during installation and the first year full usage.
General workmanship standards shall be applied as specified in the Product Assurance Plan (TBD) both for Software and Hardware production.
SKA1 dedicated workmanship standards shall be Scope of workmanship standards.
SKA1 dedicated workmanship standards shall and shall cover all phases of production, assembly and integration, testing, handling, and include clear requirements for acceptance/rejection criteria.
The SKA1 design shall possess design margins to cover all uncertainties in environment, analysis and properties of the materials and processes used.
It shall be possible to specify on a per user basis which SKA1 facilities and resources (both hardware and software) may be accessed by the user.
The reliability of SKA1 equipment to meet its performance requirements over a period of 10 years shall be greater than 99.4 % (TBC).
The SKA1 design shall require a minimum of special tools and test equipment to perform assembly, integration and repair and maintenance activities.
Inaccessible hardware or structures shall require no maintenance during operation and should have built in test capability when applicable.
Test and repair instructions shall be written for fault detection and maintenance of the SKA1 equipment.
It should be possible to execute regular maintenance jobs with not more than two (2) people per job.
The SKA1 design (hardware and software) shall have a modular approach.
The SKA1 design (hardware and software) shall provide flexibility and expandability to support anticipated areas of growth or changes in technology or mission.
The SKA1 design for both hardware and software shall provide self‐test capabilities.
All servicing and test points shall be clearly marked using TBD labelling standards.
SKA1 parts, test equipment or supporting equipment with transportation.
It shall be possible to disassemble SKA1 equipment for the reason of transportation or storage in its main parts.
It shall be possible to store SKA1 equipment (spare parts) for 10 years without any degradation of its function or performance If special storage facilities are needed they shall be supplied as part of the spares procurement.
Reusability of SKA1 equipment shall be ensured through design and by refurbishment and maintenance where this has been demonstrated as being cost effective.
SKA1 spare parts shall have a storage life consistent with availability and use during the full operational lifetime of the SKA1 equipment to which it applies.
SKA1 support equipment shall be designed to maintain SKA1 for 12 (TBC) years.
The power supplied to the SKA systems shall have the following Central facility UPS.
The power source to the central facility shall have back-up provisions for controlled shut-down (TBV).
Each SKA AA or dish system shall maintain an internal time standard with an accuracy of TBD nanosec.
All SKA subsystems shall synchronize their internal time standards to the central timing standard with an accuracy of TBD nanosec Limiting excessive currents.
SKA equipment circuitry shall be protected against excessive currents by a current limiting device, which shall not itself produce excessive currents.
SKA sub-systems shall be protected against power transients and surges.
SKA equipment circuitry shall be protected against the effects of inadvertent wrong polarity connections.
All dishes and aperture arrays shall time-tag received and processed data with the accuracy of their internal time standard.
SKA subsystems shall specify what special test resources they require in the operational phase.
Preventive maintenance of SKA1 hardware shall be performed in accordance with the maintenance program established for SKA.
SKA1 Dishes shall be designed, built and verified such that they can accommodate Phased Array Feeds.
SKA1 Dishes shall be designed, built and verified such that they can meet AD1 optical requirements up to 10GHz.
SKA1 feeds, receivers and digital processing subsystems shall be designed to provide the AD1 polarization purity requirement of 40dB.
SKA1 elements shall be designed to provide an imaging Spectral dynamic range.
SKA1 elements shall be designed to provide a spectral dynamic range of 67 dB.
The requirements traceability matrix in figure 1. shows the STaN functional requirements as a subset of the system level requirements described in AD[1].
The layout of receptors is defined by the configuration, which can be found in the SKA configurations design [3].
The parent in the SRS is TBD, so it is not clear where it comes from and what it refers to.
These are new system level requirements identified as important.
Are there any other transient requirement timings that need to be included at system level?
Upon switching do the receivers need to return to the same phase?
This requirement does not appear in the SPF Receiver requirements – should it?
Does this imply it should be possible to image the entire field of view at the same time or can this be achieved in increments?
They are broadly classified as External, namely those interfaces to entities outside the STaN Element, and Internal, between Sub‐elements of STaN.
Some of the External interface requirements, such as those stemming from interfaces to the environment, operations, monitoring and control functions (as opposed to fulfilling network functions for M&C), sustaining engineering and human actors, are found in their dedicated sections.
This section, as of STaN CoDR, is highly preliminary as this stage is in advance of formal Architecture development.
It is expected that, in addition to an analysis of the Architecture and negotiations between the System Engineering disciplines of the Elements and Sub‐elements, AD[13] will provide Requirements in this section.
These requirements are intended to be general, but where specific, reference will be made to Interface Control Documents (ICDs).
ICDs undergo a great deal of change until late in the development and therefore are the proper repository of such low level, changeable data.
These data should not be expressed as requirements for this reason.
It is expected that these requirements will be sourced primarily from ADs [5], [6] & [8].
It is expected that these requirements will be sourced primarily from AD[1] (Extensibility),[7] [8], [9] & [10].
It is expected that these requirements will be sourced primarily from ADs [4] [7] [8] & [10].
These requirements will be sourced primarily from ADs [5], [6], [8], [12] & [13].
These requirements will be sourced primarily from AD [12].
The sensing, measuring, controlling and business processes optimization technology networks approaching our daily life.
Various IoT platforms are emerging on the market to manage these networks and deliver new business models and services Nevertheless, these data-providing infrastructures are currently acting as isolated islands in the global IoT landscape.
Interconnection of these islands might bring significant added value driving IoT market development, but exploitation of these benefits is inhibited by various interoperability barriers that are present in the current IoT ecosystems.
The VICINITY consortium – together with stakeholders from energy, building, transport and health domains – thoroughly explored these barriers & drivers1 in parallel with the VICINITY demonstration sites survey (which results in extraction of pilot site needs – operational requirements)2.
Moreover, the IoT market will be likely influenced by new major EU legislative changes and proposals in the past year, such as the European Commissions’ Winter Package, the European Performance of Building Directive, the Energy Efficiency Directive, the Eco-Design Working Plan, the Renewable Energy Directive, the Electricity Directive, Electricity Regulation, and ACER Regulation, the Single Digital Market and the General Data Protection Regulation (GDPR).
Stakeholders’ drivers & barriers, operational requirements of demonstration sites together with EU legislative changes and proposals formed basis for business requirement context in which set of high- level user requirements for IoT interoperability of ubiquitous applications, services and other smart objects were identified (see Table 1: Business Requirement Context).
It spans between economics driven requirements, drive for new services provision on one hand and by compliance needs and response towards on-going disruptions in industry and public sector.
Secure and privacy-preserving infrastructure when exchanging data (especially people’s behaviour) in buildings, energy, transport and eHealth, through security policy of VICINITY and connected infrastructures; transparent auditable information sharing management; end- to-end data encryption; identification of data sources and smart objects behaviour; private data access based on data owners’ consents.
The business requirements conclude the VICINITY Objective 1.2 “IoT interoperability requirements and barriers are elicited, captured and analysed as principal drivers of the VICINITY research activities”.
Objective 5 “Value-added services explored and demonstrated” will be supported in exploration of potential new business models used to identify divers service across IoT domain with commercial benefit mind-set.
This document is prepared as a starting point for business audience to understand the basic concepts of the VICINITY solution.
The IoT paradigm and standards landscape is large and complex, and while the technology is still evolving, the adaption of standards is still immature.
Interoperability is a key element, and ensures cooperation between the different domains.
It is in this context that VICINITY may bridge the gap between domains by placing the ontology outside the physical domain (see task 2.1).
Requirement management process facilitates elicitation of stakeholders’ expectation through requirements definition, technical specification and architecture design followed by the detailed design, implementation and validation phase.
The business requirements establish consistent base- line stakeholders’ expectation reflecting realistic real-world needs.
The business requirements provide basis for solution validation.
The business requirements specification process5 has been tailored to the needs in VICINITY as described in D1.1 and detailed for business requirements in this section.
From the collected inputs the business requirements are extracted per each vertical domain (Figure .
All business requirements for each vertical domain as described in section 4.1, 4.2, 4.3 and 4.4, are consolidated and sorted.
Moreover, common business requirements are identified and clustered in separated horizontal fields, as described in section 4.5.
The business requirements will be transformed into technical requirement specification and architectural design, which drives the whole VICINITY solution definition and subsequent implementation.
Many requirements have been identified that will need to be fulfilled by VICINITYs proposed architecture and implementation.
The requirements identified in D1.3: “Report on pilot sites and operational requirements”, D1.4: “Report on VICINITY business requirements” and D1.5: “VICINITY technical requirements specification” need to be addressed.
Furthermore, the results of D1.6: “VICINITY architectural design” also have an impact on the overall model of requirements.
The process on how VICINITY will deal with these requirements is depicted in Figure 4: VICINITY requirement structuring process.
Partner UNIKL will use the inputs from D1.3 to D1.6 to create a SysML Model of all requirements VICINITY needs to address.
This model is on one hand used to track that these requirements are met by the VICINITY.
On the other hand, the model is used during WP6 to check and validate these inputs.
If some proposed requirements cannot be met, either the architectural design or the requirements themselves need to be changed as previously described in section 3.1.
Either way, problems and errors can be identified early in the design process.
A new iteration starts, of which the results are reexamined.
The same happens if during the lifetime of the VICINITY projects, some of the requirements will change.
Not only does this allow validating that all requirements are met, but also which of them may cause or are subject to threats and risk and thus need to be handled with special care.
In a business environment, the BRS describes how the organisation is pursuing new business in order to fit a new business environment for VICINITY, and how to utilize the system as a means to contribute to the business.
Each pilot has one or more domain focus - and hence differentiated business requirements.
Common assets in a neighbourhood need to be understood in order to identify the value of the assets.
When capacity is high and need is low, the price for sharing is normally low.
When capacity is low and need is high, the prices for sharing will be high.
A modern neighbourhood is equipped with a complex range of technical systems.
Interoperable with city IoT infrastructures, like LoRaWAN and NB-IoT.
Optimise sizing, monitoring of space and energy consumption; New rental models for sharing of assets need to be prepared; Standard process for emergency cases need to be adopted and adhered; Standard applications and business models used in telecom IoT infrastructures.
Buildings are some of the most expensive assets to operate and maintain used in our cities and urban centres.
It was found that building owners and managers see economic value through a potential improvement in efficient usage of resources and optimization of building maintenance as facilitated by IoT and VICINITY.
Building efficiency includes not only how and when the resources are used but also how many users need the same resource within the buildings neighbourhood.
A recently conducted study [16] in collaboration with Statsbygg, the Norwegian State Property owner, reports that a standard university building has an average energy cost optimization potential of about 15 – 20 % of the total energy cost.
However, a lack of visualisation of consumption of resources and lack of accountability prevents these savings to be made.
IoT gives the property-owner a high-resolution multichannel information-window, making possible to optimise the use of considerable resources inside the building as well as between buildings (cross assets) within a neighbourhood both virtually and physically.
As grids reach maximum use capacity or reach its exploitation limit, the importance of load management increases.
To be able to trade between buildings and grid owners, working markets with load pricing, as well as load shedding and shifting prices must be in place.
Facility managers and grid owners should support to interoperability between various IoT assets (i.e. load flexibility and constraints) that are needed for load management across various building (e.g. devices, charging station) and energy assets (such as RES, batteries,), to be able to trade loads in practice.
Transport: parking space occupancy, parking space prices payments.
Collecting and storing data related to people’s behaviour in buildings should be governed by transparent contracts and happen in accordance with relevant rules and regulations.
Encryption and/or aggregation of data to protect identities where relevant.
A modern building is usually equipped with a vast and complex range of technical system.
A communication flexibility and information architecture that can handle the speed and heterogeneity of current and future technical developments is needed.
The managers need incentives to share data from buildings, parking spaces, local grids, such as reduced peak loads, rent on parking spaces or sales of locally produced energy.
Availability of IoT based tools with simple management will change how facility management as a service can be delivered.
Dynamic systems, flexible to interconnect or extended various services and devices together, are expected to replace legacy systems.
IoT technology provides opportunities to obtain a new basis for digital data in the building industry.
This basis gives us completely new opportunities for monitoring, classification, analysis of the use and performance of a building and thus new opportunities for control and management.
Access to such data will change many of the existing business models and will push forward many new models and new types of services.
Facility owners, tenants, building managers should be supported in measurement and visualisation of the interior conditions and parameters to understand the resources consumption and their effects on wellbeing.
The facility owner, tenants and building managers should be supported in measurement the resource usage and / or occupancy to optimise resource maintenance or identify situations required manual or automatic intervention.
Aggregated energy consumption patterns of equipment usage can be offered to the building owners and managers, providing an opportunity to understand the impact of EV charging within a building and manage the process in an economic way avoiding peak consumption.
The service providers should have access to devices utilisations to be able to customise the device operation and maintenance leading towards “appliances as a service model”.
The facility owners, managers and tenants should have access to information of activities and resource consumption in order to identify and optimise the buildings operative parameters.
The facility owners, managers and tenants should have access to an overview of registered activities and resource consumption, in order to identify areas in actual need of cleaning, thus reduce the energy consumption and minimise the wear on other parts of the building.
The Energy sector has a high strategical value and there are high expectations as to potential impact of the IoT within the industry as a whole.
IoT overall and VICINITY as one of the particular solutions is expected to help in achieving RES energy penetration targets, energy efficiency goals and decarbonisation of economies on the high level, while increasing visibility of processes and improving quality of data resulting in higher quality of services.
The Energy ecosystem approach on the municipal level adapted by VICINITY is expected to facilitate data sharing and visualisation of impact on various stakeholder entities.
Continuous energy consumption monitoring and control should be supported by interoperability across standards and vendors of the connected devices.
Energy consumption systems should addresses functionality that extends beyond energy load balancing during peak periods (periods with high energy consumption).
The need for continuous monitoring through system analysis and integration in the building environment should also contribute to the data analysis.
Such services would open for B2B solutions and a B2C spectrum of services where a common platform could support the increasing in level of services driven by similar logic and approach.
Management energy source across different standards and appliances’ vendors enables management of various sources of production (electricity and heat) and sinks (consumption/ uses of energy) to meet demand as smoothly and sustainably as possible.
Management of energy sources should be supported by interoperability across standards and vendors of the connected devices.
Establishing and visualising the process of managing a wide range of occasionally used supplies and sinks enables create new business opportunities and add flexibility to the use of electricity.
Management of intermittent supplies and sinks should be supported by interoperability across standards and vendors of the connected devices.
Create Interoperable and secure IoT systems to facilitate and form the building blocks of supply and demand management systems, including: in-home communication between smart appliances and energy management systems; and integration to grid operators as well as auxiliary service providers.
The Synergies with other systems in buildings should also be incorporated in the energy management system (e.g. water).
Many resources provided within a building consume electricity in addition to the appliances and other devises.
Interdependencies of one resource consumption and the impact on the other resource are not well understood.
Increasing interoperability in consolidating this information is expected to create positive synergies.
Consumers with a B2C relationship will need “a system which should learn the user's preferences and optimize automatically without the need of user interaction.” To achieve this, hybrid fully- automated systems enabled by IoT solutions should be designed, demonstrated and tested.
Engaging consumers in both sectors, businesses and households to manage, produce and store energy and other available resources.
Ownership of data and information about ownership, access, and other related information about the device, should be made available for improved data management and transparency when installed.
The data in question could be e.g. heartbeats, value, volt detection, device ID code, registered ownership, placement etc.
Energy consumers would be anticipating potential economic gains, while transitioning from passive to active energy monitoring.
This also increases energy efficiency, because with advice the users might achieve a better rational use of energy, benchmark designed performance and actual performance of systems.
They are willing to give data access to service suppliers and intermediaries but have concerns as to: their privacy, level of data protection that can be guaranteed and the security of their data.
Energy consumers should provide access to data in secure and privacy-preserving way.
The facility owners and managers should be supported in monitoring of energy production/ consumption and facility resource occupancy and/or utilization and heat production over various devices in order to be able to improve the understanding of production, consumption and utilization and provide an opportunity to optimize billing for renters.
The facility owners, managers and tenants should be supported to measure, cluster and visualise of the energy and water consumption of devices to provide an opportunity to do assessment of alternative models of management reduce resource consumption and systems substitution decision making.
The data owners should be supported to share available weather data to energy producers from scientific equipment providing them an opportunity to optimise their renewable energy production.
EV charging enabled parking place may be booked by facility owners, thus offering the ability to manage energy production and consumption (demand-response) as well as EV charging utilization to facility owners and/or managers, to identify smooth positive and negative peaks in the electrical energy consumption of buildings taking into account energy price, parking place occupancy plan and price.
The facility owners should be supported in optimization of parking space, EV Charing utilization and management of energy production and consumption.
The household owners should be supported with information on resource consumptions and demand response services providing them opportunity to improve energy efficiency of their households and/or improve usage of excess renewable energy.
Plans for solar panel cleaning based on condition of soiling as opposed to periodic cleaning, could be offered by solar panel operators in order to improve panels’ quality of service and reduce costs for cleaning.
Solar panel operators should be supported in soiling monitoring.
Facility owners, manager and tenants should be supported to benchmark progress/compliance towards reaching energy efficiency targets with and without energy audits.
Sharing more data via VICINITY would increase the visibility of processes and enhance data quality resulting in better quality of services which might lead to higher financial returns in a medium to long term period.
Knowledge sharing at municipal level as well as increased competitiveness through deployment of new systems are positive drivers for early adopters.
A lack of internal resources, lack of commitment from the management, and regulatory compliance issues complicates this further, such that data protection and security by design will be a key driver.
The transport sector is recognised by a large amount cross-domain technologies and standards within fields such as road side technologies, car-to-car communication, car-to-road communication, tunnel technology, smart traffic light, smart signs etc.
The transport sector covers all means of logistics, being on the ground, rail, at sea or by air.
This is commonly part of ITS (Intelligent Transport System).
Solutions that offer a foundation for exchanging information between systems, opens for many opportunities.
Transportation technology can be considered the life blood of smart cities, as areas that suffer from pressure on the infrastructure reduce green areas, residents’ quality of life and financial status of the affected areas.
It is in this context that VICINITY focus on smart parking technology for demonstration purposes.
Smart parking is based on optimising the usage of areas in and around the parking facility.
Not just based on available space, but also on specifications that ranges from particular needs and demands to scheduling, environmental considerations, accessibility, security/privacy issues to available services.
For a more in-depth analysis and descriptions, see Annex II: Transport – market and demands.
Further descriptions of requirements for the transport domains are listed below.
Parking sites should abide to legislation for ownership models, income from rental, privacy6 and security7 agreements regarding access and authorisation to indoor parking sites8.
Contracts that define responsibilities for ownership of parking space, access and personal data should be defined.
Transaction and rental models should be introduced for long term, short time, subscription based and contract based usage.
Agreements should adhere to standards 9used for exchanging (semantic) data with ITS equipment, traffic control centrals, nearby vehicles, and digital signs.
Support for standardised communication protocols used for smart parking and relevant equipment needs to be anchored on a contractual level.
The European standard DATEX II should be the preferred choice.
The common rules in the field of data provision and publication are based on this standard, and it is the basics for public procurement and apply for all smart parking detection technologies, payment terminal and other transactions, traffic information and control centres data work.
The encryption and storage of data, should adhere to EU's Data Protection Directive 95/46/EC2 as well as national legislation.
This includes anonymising person data and other information that can identify an individual.
Integration with other transport related sectors – most notably car sharing and public transport should also be supported in order to establish the foundation for a healthy ecosystem, and also alleviate some of the pressure on the infrastructure, thereby reducing the climate footprint.
Support and access to updated information on available EV charging stations with technical information, current status and availability, should be supported.
Different ways of getting access – in particular to underground garage facilities – should be defined.
This would include authorisation equipment and systems like camera systems for reading licence plates, car based sensor, card reader, RFID, Bluetooth, geo fencing and biometric systems.
Other reader/detection units might be integrated when deemed necessary.
Demands on user experience design should be an integral part of the solution.
This should address devices and accessibility, in particular with an emphasis on supporting unified design.
Certain disabilities and lack of previous experience might affect the usability aspect.
Especially users on that handles the parking experience on small devices need to be receive specific attention, as there is little or no time for training, and the systems will be based on a on-demand approach.
This is even more important when the mobile devices will offer interaction with 3rd party services that will be built on top of the parking experience.
User experience should be continuously measured and evaluated by UX designers through anonymous tracking of user navigation on the panel.
Access to and visualisation of structural plans/layout for garage facilities should be included in the information about access points, charging stations, fire extinguishers, escape exists, alarm signals as this is necessary in order to provide a proper service that can be implemented and expanded on a longer term.
The transport domain contains challenges within areas such as logistics (e.g. assignment, allocation, optimising routes, history).
Transportation does also include such topics as authorisation/ authentication/access, integration of other services and visualisation of complex information.
This means that proper knowledge of priorities, special requirements for vehicles and users, historical data and ownership of transport related equipment and services will be one of many parameters that will influence logistics and other outcomes from the transport related issues.
Therefore, Interoperability on these areas including other value added services should be supported.
Interoperability on rulesets that support actions based on changing values and their relevance to the transport domain should be supported.
This includes data from manual booking, trigger mechanisms based on rulesets integrated with other domains (i.e. eHealth and building), request from technical or health care personnel, ground based sensors and visual data from other sources.
The Technical Specification standard CEN 16157-6:2013, Annex D (Data Dictionary), describes DATEX II data exchange specifications for traffic management and information, of which part 6 deals with Parking publication.
Standardisation of DATEX II establish a basis for common exchange between the traffic and the travel information sector, thus opening for cross-domain value-added services.
This standard serves for setting out the rules for parking management and should be part of the common framework for handling smart parking related activities.
Camera with Automatic Licence Plate (ALR) detection and reading capabilities affect authentication and authorisation processes, and is part of the entrance security system.
This operation also provides the facility owners and managers with information about how many and what registered vehicles that are present at any time.
Detection of parking space occupancy could be applied to optimisation of both short-term and long- term parking space usage.
These data can also be when correlated with data from ALR-detection and hence improve parking services processes.
The health sector is one of the core areas to be included to and benefit from the concept of smart cities.
Under the context of the VICINITY project, two health related scenarios were studied during the requirement analysis process; assisted living at home and preventive medicine.
In order for the demonstrations to be tested appropriately and effectively, a set of specific business requirements should be defined and followed.
Multiple sensors, mainly in the form of wearables but other activity tracking sensors as well, will provide several real-time health condition measurements by monitoring patients’ behaviour, ideally taking into account the “sensitivity” of health related data, a number of other challenges arises regarding ownership and usage of data, security and privacy, trust of authorized third parties and legal challenges as well.
Moreover, concerns regarding trust in data origin are principal business inhibitor.
Therefore, a business requirements framework is necessary for the case.
Therefore, as the demonstrations run in municipality level, the participating municipality should ensure the involvement of the necessary healthcare personnel.
The devices involved should be low cost and effective, in terms of measuring and tracking as many health conditions as possible but also in terms of ease of use.
The right balance among the latter factors will guarantee a decent amount of potential active participants.
Complex, inefficient devices might lead users to reduce their level of participation or even completely abandon them.
As authorised third parties will have access to user’s personal data – especially in case of emergency, the user should have a fully-detailed, complete picture of who and when accessed his/her data.
Therefore, an effective audit management mechanism should be introduced.
As users’ personal health data will be shared to authorized third parties, trust is a major concern.
Therefore, contracts defining ownership of data, usage of data and privacy should be introduced.
Miscommunication is a major issue to be addressed in all domains.
However, it is of even higher importance for Health domain where any missing information could prove crucial.
Therefore, a common interoperability “language” should be defined in order to overcome linguistic barriers that could lead to possible misconceptions.
The latter could be achieved by following equivalent integration profiles such as IHE, HL7, DICOM and Continua.
As one of the deployment scenarios, specifically assisted living at home, might possibly involve emergency incidents triggered by the participants, especially the elderly, a standard process of specific steps should be identified to serve the case.
Ideally, the latter should be an override mechanism that follows “break the glass” principles.
The suggested framework should be accepted by all sides involved and strictly followed by authorised third parties.
Access to health status and in-house conditions could be provided to caretakers for identifying abnormal behaviour with the supervised person.
Affordable devices used for condition assessment using could be made available to elderly citizens and caretakers.
Caretaker should have access to smart drug dispense usage statistics to be able to detect abnormal drug usages.
Simple wearable devices should be supported for elderly people when their ability to operate smart phones or other technical equipment are limited.
Household appliances usage data could be offered to healthcare providers in order to identify abnormal behaviour in elderly citizens’ households.
Weather data from scientific equipment to municipalities could be shared by providers, hence municipalities may recommend maximum time of sun exposure to citizens.
Information about the perceived temperature due to the influence of wind speed and air humidity could be offered to citizens.
This section includes business requirements identified for cross-domain applicability.
It builds upon the findings presented in the domain specific requirements.
Younger population expect to have same user experience regardless of different type of mobile devices and wearables used, however different type of devices might have different set of features.
Elderly people expect use simple and affordable wearable devices while their ability to operate smart phones are limited.
Building owners, managers and tenants expect that VICINITY enabled system should be operated by youth segment that needs special care, however are able to handle tasks.
This would potentially create inclusion into the job market of the fastest growing special care group.
Business domain owners expect cross-availability of IoT data that removes vendor locks from IoT eco systems.
The ontologies should be extensible to provide semantic interoperability for new devices.
Caretaker and elderly citizens expect to use affordable VCINITY enabled devices.
Affects use case: UC 1.1.2.5, UC 1.1.2.6, UC 2.4, UC 2.5, UC 2.6, UC 2.7, environment to allow for third parties services building for demonstration purposes.
Concept of the solutions will be considered in D1.5 VICINITY Technical specification requirements and D1.6 VICINITY Architecture design.
European Commission introduced the EU Data Protection Reform and adopted the Regulation processing and storing of the personal and sensitive data.
Moreover, the regulation defines concepts of how the data protection should be maintained in the organisation processing private data.
Requirement consent 11as described in the Data Protection directive no.
Security functional requirements need to be defined for each potential threat.
Selection of the functional requirements need to balance the cost of the security measure and cost of potential security breach.
The actor should have authorization to access the assets managed or facilitated by VICINITY.
Authorization should be managed by the organization connecting to VICINITY.
Action performed in VICINITY should be traceable to actor who performed the action including evidence that it has been done so.
The unsolicited access or operations in high-volumes can cause unavailability of the part or whole system.
Each of these security threats needs to be addressed by one or several security measures to minimize the security risk to acceptable level.
After Mile Stone 1 – “VICINITY requirements, barriers, pilot surveys and audits available”, the requirements will be implemented into a tool (SysML) for easy definition and maintenance.
IoT ecosystems generate data that can be harvested to provide novel commercial services, or public services for the benefit of society.
This review was designed to identify benefits and issues related to data sharing both within a single domain and across domains.
Achieving interoperability across domains is a key objective of VICINITY.
In the context of VICINITY, interoperability is the ability of a system or a product to work with other systems or products without special effort on the part of the customer.
In order to collect and analyse business drivers and barriers in the context of IoT interoperability, a survey was conducted using a questionnaire to interact with stakeholders and visit to the pilot sites demonstration VICINITY solution.
The scope of the survey included several separate vertical domains (buildings, energy, transport and health) and several horizontal cross-domains (legal & ethics, security & privacy and the technical domain).
Barriers which are similar across domains were identified, along with some more domain- specific ones.
The potential for cross-domain synergies was identified which could maximise the use of clean energy and/or optimise the management of resources.
According to several stakeholder studies, security and privacy are some of the most common barriers to the interoperability of IoT.
Other barriers identified are lack of standards and low level of product maturity from customer standpoint.
There are many IoT ecosystems with actors that operate within cross-domain areas.
These ecosystems generates big data that could be harvested to provide novel commercial services, or services for the benefit of society.
In many case the owners of these data sources are unwilling to share access to their data, especially if they do not have a business case, which enables them to benefit directly from the new services derived from their data.
At this stage of project development VICINITY system´s perceived strengths by the stakeholders in general are as follows: “Interoperability and integration of various standards and protocols which would allow for broad use of the product, allowing for rapid innovation”.
The system is expected to provide efficient, time saving performance and value-added services from a business perspective, while minimising environmental impact and yielding cost savings.
The overall goal of VICINITY is to deliver improved quality of life and open for innovative services that can be built on top of the architecture being offered by solution.
Among weaknesses, resistance to change can be expected from strong market players with existing proprietary products.
On consumer side, potential loss of privacy and security, compatibility, complexity and legislation are voiced as potential weaknesses.
These strengths and weaknesses identified from stakeholders’ interactions also known as drivers and barriers are analysed in vertical domains energy (section 4.2), health (section 4.4), transport (section and building (section 4.1) and horizontal domains (section 4.5) legal & ethics, security & privacy domain.
Stakeholders perceive VICINITY as a project, which has the potential to integrate various disparate standards and protocols.
This report constitutes one of the components of the VICINITY project.
The approach and philosophy employed by VICINITY takes into consideration stakeholders’ opinions in order to build solutions focused on meeting their requirements whilst tackling issues they have identified.
The deliverable uses information derived from the representative list of stakeholders and pilot site operators.
Further deliverables in Work Package 1, such as D1.5 and D1.6, will identify functional and technical requirements, resulting in a system architecture, thus completing the knowledge base upon which the VICINITY solution will be developed, tested, deployed and demonstrated.
This annex will be used in development of concrete business plans that will look to maximize the exploitation potential of identified assets results as well as address stakeholders’ needs and expectations.
Exploitation Strategy and Business Plan Development (WP9 Task 9.5) will monitor and compile the partners’ exploitation activities and seek maximum synergies from them.
One major part of the exploitation planning will be the identification of potential target groups and business cases based on the outcome of the project.
Moreover, market requirements and future application areas will be captured.
Each exploitable result’s related target group will be identified to make sure each target group is informed about the relevant information / activities within the project.
Definition of both individual & consortium exploitation/business/financial plans for each exploitable result.
An annual report will detail exploitation status and perspectives, at partner’s and Consortium’s levels.
The results will be presented in D9.5 in two issues of the document during the second half of the project (M24, M36 and M48).
This annex defines the normative content of the business requirements specification (BRS) document for VICINITY.
The project shall produce the following information items in accordance with the project’s policies with respect to the business requirements specification document.
Organisation of the information items in the document such as the order and section structure may be selected in accordance with the project’s documentation policies.
Describe at the organisation level the reason and background for which the organisation is pursuing new business or changing the current business in order to fit a new management environment.
In this context it should describe how the proposed system will contribute to meeting business objectives.
Defining the range of business activities included in the business domain concerned.
The scope can be defined in terms of divisions in the organisation and external entities that relate directly to the business activities, or functions to be performed by the business activities.
It is helpful to show environmental entities which are outside of the scope.
Describing the scope of the system being developed or changed.
Describe major internal divisions and external entities of the business domain concerned and how they are interrelated.
List the major stakeholders or the classes of stakeholders and describe how they will influence the organisation and business, or will be related to the development and operation of the system.
Define external and internal environmental factors that should be taken into consideration in understanding the new or existing business and eliciting the stakeholder requirements for the system to be developed or changed.
The environmental factors should include possible influences to the business and consequently the system from external conditions like market trends, laws and regulations, social responsibilities, and technology base.
Describe the business results to be obtained through or by the proposed system.
Describe methods by which the business mission is expected to be achieved.
The description should be concentrated on the methods supported by the system to be developed or changed with the items such as product and services, geographies and locales, distribution channels, business alliance and partnership, and finance and revenue model.
Business Motivation Model (BMM) Specification by OMG.
Describe the overall strategy for the organization level decisions on common bases for multiple project portfolio – when multiple system projects are running or planned to pursue the same business goal, the priority, relative positioning, and possible constraints come from the portfolio management strategy.
Provide description of the procedures of business activities and possible system interfaces within the processes.
The purpose of this information item is to represent how and in which context the system supports the business activities.
In general, business processes make a hierarchical structure with decomposition and classification.
Each business process should be uniquely named and numbered in the hierarchy.
The description of the individual business process should be represented as a diagram representing a sequence of activities.
Describe logical propositions applied in conducting the business processes.
The propositions will be conditions to start, branch and terminate the sequence of the business activities in the business processes; criteria for judgment in the business processes; or formula to evaluate a quantity, which will likely be addressed in functional requirements in the SyRS and SRS.
The policies and rules shall be uniquely named and numbered, and shall be referenced in the description of the business processes.
Describe conditions to be imposed in conducting the business process.
The conditions may be on a performance constraint (e.g., the process shall be finished within a day after the triggering event occurs), or may be from a management requisite such as 'every occurrence of the process shall be monitored and recorded'.
Describe methods to conduct the business operation in an unsteady state, for example, a state when business operations might be extremely busy due to some intensive occurrence of events.
An unsteady state of business operation includes a manual operation mode when the proposed system is not available due to some unexpected situation like an accident or natural disaster.
Define the level of quality required for the business operation.
For example, a business process may address required urgency with higher priority than the reliability of the business process.
Identify and describe the structures in the business relevant to the system, such as organizational structure (divisions and departments), role and responsibility structures, geographic structures, and resource sharing structures.
There may be a need to alight the system functions to these structures, and to support future structural changes.
Describe the proposed system in a high-level manner, indicating the operational features that are to be provided without specifying design details.
Describe examples of how users/operators/maintainers will interact with the system (context of use).
The preliminary (upper-level) scenarios are described for an activity or a series of activities of business processes supported by the system.
The scenarios should be uniquely named and numbered, and should be referenced in the description of the business processes in 6.1.9.
Describe how the system of interest if to be acquired, deployed, supported and retired.
Describe constraints to performing the project within cost and schedule.
The core of the smart parking case is to offer interoperability of various suppliers to demonstrate how to provide the same service, with a specific focus on real time occupancy and transaction data in the standard data format DATEX II.
Smart parking is a concept that can be assigned for different kind of areas.
Examples like sites near apartments, parking sites near work places or public buildings, parking sites on industrial areas or for commuters, sites for disabled people, indoor sites, outdoor sites, street based parking sites, road side parking, parking sites for cars or trucks, electrical vehicles (EVs) can be envisioned.
Parking space is a resource that is on-demand, and where is usually is a shortage.
Different uses have different needs, either because of the vehicles, or because of disabilities or other concerns with the passengers/drivers/owners.
Cost efficiency is an issue, and introducing parking space sharing, as it makes it possible to ensure that neither private nor public parking sites are left with unused space.
At the same time it makes it possible to exploit areas that otherwise would have been assigned for a specific usage, as real time information about vacancy opens the door for temporarily assigning the parking space for other purposes.
The situation today is that city managers and agencies, municipalities and counties, governments, site owners and other of the key actors referred to in “Table 4: Key actors in smart parking ecosystem” are experiencing pressure from commuters, residents and their own departments to reduce congestion and improve the conditions of those who are dependent on cars or are living in cities.
The mobility sector affects all city users; commuters, business owners, blue light agencies and other departments that are triggered in emergency situations.
Smart mobility solutions, of which smart parking is a part, is considered being a way of solving some of these challenges.
By allowing people to receive relevant data, the user is enabled to make more choices that are informed.
This provides them the option of deciding how to ride, when and where to ride.
It opens for integration from different domains, and facilitates efficient movement of goods and people, thus ensuring logistics supporting the city.
These are often owners of the parking sites and are responsible for assigning budgets and managing the criteria that governs cities and infrastructure.
The departments that will benefit the most are located in these governmental bodies.
They spend many resources on transporting people and goods, and health care personnel and blue light agencies represent a large part of their budget.
Solutions that will assist in reducing the pressure on the operational agencies will have a large impact and therefore gain a lot of interest.
Personnel responsible for managing mobility solutions within cities.
These are the key buyers in the smart mobility domain.
City Managements are coming congestion, improve the quality of life of those living and working in cities.
These are companies involved in data collection and interpretation to provide traffic management solutions to City Management.
These are the key sellers and potentially the innovators and integrators in the smart mobility domain.
The products produced by companies operating in this sector aim to optimise parking, improve safety and reduce congestion and emissions.
Companies who manufacture technology related to traffic and parking management solutions.
These are the innovators within the smart mobility domain.
Their products are primarily aimed at companies involved with Transport Information Integration and with City Management.
Provide tailored mobility solutions to city inhabitants.
These are a small, but growing player in the industry offering services to commuters in need of parking space.
Their smart mobility products are aimed at making transport more sustainable, greener and safer.
They design and manage the procurement processes and install the infrastructure network.
These companies advise commuters, city management and transport information integration companies.
The goals of these companies are to design and build mobility solutions that are optimally designed, incorporate best practises to reduce congestion, improve quality of life, lower emission and optimise use of urban areas and mass transport.
Companies involved in this sector provide services such as beds, food, fuel, transportation to commuters.
Commuters are informed of available services before, during or after their commute, through the use of smart technology which recognises their route or current location via GPS.
Commuters are targeted by tailored advertising techniques depending on their mode of transport, location or smart phones setup of companies or services which could strike a chord with commuters in that location.
Revenues provided by advertising companies could greatly reduce the cost of development and installation of smart transport solutions for city management.
Companies whose technology will be used to relay data to transport information from advertising companies.
Additional means of communication (data to and from measuring devices, transport information integrators and advertising companies) will be required.
Sensors that can detect occupancy of a parking space needs to send real time data.
A gateway will have to be able to transmit the information further into the network.
Information must be accessible to interested parties through relevant platforms.
Additionally other sources of feedback, most notably smart light and monitors displaying information will improve usability factors and reduce risks for occupying the wrong parking space, or drive in the wrong direction.
Real time transactions and security and privacy issues related to initial authorisation, authentication and access will have to be integrated in the smart parking system.
This opens for other challenges that needs to be addressed; Who are responsible for handling authorisation and access, who will be liable for parking/damages or theft when occupying parking space.
Who will handle payment or other kind of transactions that function as transferring value.
How to handle long term and short- term ownership/rental of parking space – and customisation that may take place without consent from other owners.
How to handle booking and verification, rating and other activities that directly may influence the worth of a given parking space.
This may be considered even more important for underground parking facilities, where the entire facility may be affected by choices that are made in regards to just one parking space.
Finally – the core part of smart parking is based on optimizing the usage of areas in and around the parking facility.
Not just based on available space, but also on specifications that ranges from disabilities to time slot, from location of entrances to nearby services.
Smart parking will offer cross-domain support through interoperability supported through the VICINITY platform.
An extensible core information model, i.e. core ontology, should be used for all information elements to be interpreted, being agnostic of their specific contexts and communication standards.
Domain-specific information elements have to be interpreted using specific model extensions of the core model.
All information elements should have enough associated metadata to become properly annotated and understood using the corresponding information models.
All APIs should provide semantic descriptors of all information elements they expose by leveraging their own metadata.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The requirements document is created in the first steps of the project for getting a wide vision about what kind of data, users and functionalities are going to be presented in WYRED the project (García- Peñalvo, 2016; García-Peñalvo & Kearney, 2016).
This document also represents a guide for the developing process, because it informs about the priority of the tasks, the dependencies among the tasks and the information they require, including the background, objectives, and targets too.
In the process of the requirements definition, we can use different approaches depending on the software methodology.
In this case, we chose the Durán and Bernárdez (2002) methodology because it offers a simple, but powerful, process to elicit and document the requirements.
This methodology is based on tables, where we fill the information about a specific requirement.
The project requirements represent properties that our project is going to have.
However, there are properties about different things (functionalities, design, information, etc.).
In our methodology, the authors defined three types of requirements.
This type of requirement groups all requirements about the information that the system is going to manage.
There is very important to set these requirements in order to develop the system that is going to manage the data.
In WYRED one of the most important thing is to maximize the users’ privacy, due to this we are going to split the user’s information between public and private in the platform.
This information was selected with ours partners to keep the minimum information required to develop a correct social dialogue.
These fields are the nickname (Table 1: NicknameTable 1), the language (Table 2), the avatar image (Table 3), the area (Table 4) and the topics of interest (Table 5).
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The platform also has to manage more information about its users due to this can help us to extract patterns and get conclusions.
However, this information will be private and nobody will see the private information of a specific user.
Therefore, it will be treated anonymously in order to respect the users’ privacy.
The information that we keep private is: the name (Table 6), the surname (Table 7), the country (Table , the city (Table 9), the sex (Table 10), the age (birth date) (Table 11), the education level (Table 12) and the email (Table 13).
The main problem to manage documents is that there are many formats that we have to support and they use many resources.
For this reason, we decided keeping them in a dedicate server to improve the platform performance.
The statistics save data about the number of publications for each user, her time in the platform, how many pages she has visited, in how many projects she is involved, etc.
The both requirements are described in Table 14 and Table 15 respectively.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
This type of requirements groups all requirements about what kind of technologies we are going to use, and regarding the platform design and architecture too.
When we planned how to design a platform for youth people, we checked that they use more their mobiles than other classic devices such as laptops or desktops.
For this reason, we decided to use a responsive design that can work well in all kinds of devices.
However, creating a platform where our users are going to use mainly mobiles has other problem, the performance.
Usually mobiles have wireless connections and depending on the country, they can be a bit slow, so we add a requirement about the platform performance.
The both requirements are described in Table 16 and Table 17 respectively.
We also would like to develop a mobile application for the more common mobile systems (Android and iOS), in order to get a better response from the user.
However, this requirement is more a wish than a urgent necessity and for this reason, it will have a minor priority.
One of the objectives of WYRED is to develop a platform without spend money in non-relevant functions.
Due to this, we have planned to use Linux servers and free technologies.
Linux servers are cheaper than Windows servers and offer a solid base to develop a complete web platform, in the same way, there are free technologies/software such as PHP or MySQL that are widely used in web development and we can use them without a specific cost.
These requirements are described in Table 19 and Table 20 respectively.
One of the most important objectives is to keep all content in the platform hidden for public users, because we are working, in some cases, with underage people.
So, the platform content does not have to be indexed.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
This kind of requirements is related to the functions that the system has to support.
One of the aspect related to the privacy is how to keep the users’ information private and avoid arguments in the platform.
The solution that we have planned is to use an automatic system that checks all messages looking for a list of alert words.
Although the intelligent system can find messages with alert words, these should be checked manually, for this reason we are going to develop a group of moderation tools for editing, deleting, moving, etc.
Moreover, we will put an option where the users can inform a moderator that there is a message that should be analyze.
These requirements are in Table 22, Table 23, and Table 24.
The objective of WYRED is that the youth people can interact themselves and speak things that are important for them, in some cases, they also can offer solutions for their own problems.
For this reason, we have to allow that a user can register in WYRED.
However, WYRED is not a chat but a platform where there should exist a structured social dialogue, so we are going to develop something like forum topics.
This system helps the users answer questions and create discussion threads with their comments.
With the objective of having a good response, first of all, the users will fill a form where they can say in which things they are interested in.
Therefore, a researcher or a user will create a project, this will be hidden for all that are not invited to participate; for achieving this, the creator will select what kind of users are going to be invited using the public user’s information.
This process is described with the following requirements from Table 25 to Table 30.
One of the key elements to attracting young people is working on functionalities that improve the users’ engagement.
This helps to improve a lot of metrics such as page views, used time and social sharing.
The first requirement, which we have defined with this aim, was a translation system due to there are users from countries that speak different languages.
We have also thought that it would be interesting to use a gamification system to increase the users’ engagement, due to it is one of the most common technique used right now.
For improving the usability, we have planned to design custom styles for each age group, because of teenagers do not want to use a childish platform.
We have also kept in mind that if a user likes our platform, she would like to share it with her friends, for this reason, we will develop an easy tool for social sharing.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
These requirements are described in tables: Table 31, Table 32, Table 33 and Table 34.
In the social dialogue process, the researchers need to work with many kind of documents, such as text, audio, images and video.
These documents are a relevant part in the WYRED project, so we have planned to develop tools for showing and sharing them.
The researchers also suggest that they need an online editor and an easy way for adding annotation in audio and video files.
These requirements are described in tables: Table 35, Table 36 and Table 37.
The researchers are also an important part of the WYRED project, for this reason, they are going to have a private zone.
In this zone, they will be able to monitor their own social dialogues, share information between the project researchers and analyze their data.
In order to improve the data analysis process, we are going to develop visualization tools and a pattern matching system.
The visualization tools will help the researchers to discover knowledge using charts, trees and other representations.
The pattern matching system will use techniques such as datamining or natural language processing, in order to give to the researchers some patterns about the stored data.
These requirements are described in tables: Table 38, Table 39 and Table 40.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall store the information corresponding to the user's nickname.
The system shall store the information corresponding to the languages that each user can use in the platform.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall store the information corresponding to the user's avatar (a public image that represents him).
We will offer a catalogue of avatars and the users will choose one of them.
If we can, it would be good has a system to personalize the avatar.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall store the information corresponding to the user's localization.
The system shall store the information corresponding to the main topics of interest for each user.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall store the information corresponding to the user's name.
The system shall store the information corresponding to the user's surname.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall store the information corresponding to the user's country.
The system shall store the information corresponding to the city where the user lives.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall store the information corresponding to the user's gender.
The system shall store the information corresponding to the user's age (based on the birth date).
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The best approach to manage age is to create group of ages, for example teens, kids, etc.
The system shall store the information corresponding to the user's education level.
The system shall store the information corresponding to the user's mail.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall store the information corresponding to platform usage statistics.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall have a multi-platform native approach, for this reason we should create an application for the most important mobile operating system (Android and iOS).
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall keep all content hidden for public users, browsers, etc.
The system shall have moderation tools to allow moderators edit, move and delete messages.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall have an intelligent system to check a list of "alerts" words.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
We have to define what fields are needed in the register process.
The topics system must support that a user cite other user/users answer.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall support selecting what kind of visibility we want for each part of the system.
The allowed options are public, restricted by user or private.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall have an option where the researcher can select what kind of users he needs in his research, using the public profile fields.
The system shall have the possibility that a user can create a project and invite other users to join him.
The project has to have a zone where the users can speak and interact.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall use an automatic translation system such as Google translator in order to allow the communication between people who speaks different languages.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall have an option to insert videos, audios, documents and other stuff.
The system shall have an online editor for documents.
The system shall have a system that allows annotation in video and audio files.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall have monitoring tools in the researcher zone in order to see the social dialogue evolution.
These tools should use the number of participants, their interactions, the number of comments, the number of uploaded documents, etc.
The system shall have a powerful visualization system, that displays the statistics in graphs, charts, trees and other representations.
The aim of this system is helping the researcher to understand the data.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
The system shall have a system that can analyse the data and find patterns automatically.
This system can use multiple approach like NLP (Natural Language Processing), datamining, etc.
The WYRED Project: A Technological Platform for a Generative Research and Dialogue about Youth Perspectives and Interests in Digital Society.
Journal of Information Technology Research, 9(4), vi-x.
Networked youth research for empowerment in digital society.
Copyright This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant Unless officially marked PUBLIC, this document and its contents remain the property of the beneficiaries of the WYRED Consortium and may not be distributed or reproduced without the express approval of the Project Coordinator.
This section describes the services offered for High Value Payments (HVP).
The RTGS for High Value Payments processes payment orders on the RTGS account holders’ Dedicated Cash Accounts (DCA).
This includes the entry disposition, the settlement and the queue management.
As a general rule, it is intended to keep most features almost unchanged or enhanced compared to TARGET2.
Nevertheless, the introduction of a Central Liquidity Management component in order to centralise the liquidity management for RTGS, T2S and TIPS, and to settle all Central Bank Operations, including credit line updates, on CLM as well as the migration to ISO20022, will lead to some changes to the current settlement processes for high value payments in TARGET2.
As a consequence, this URD gives the full picture of all requirements for RTGS.
More details will be provided during the realisation phase within the UDFS for RTGS.
The description of the processes in this document does not differentiate whether the orders are submitted to the component in U2A or A2A mode.
This business process describes the processing of a payment order.
An RTGS account holder will initiate the process by sending the respective message containing a payment order to RTGS, which will process the payment order.
If the message content is either invalid or would fail the reference data checks, it will be rejected and a rejection notification with the appropriate error code(s) will be sent to the sender of the message.
If the message content is valid and reference data checks have been passed, RTGS will perform a series of operations according to the content of the message.
These core settlement operations of a payment order include various checks on timing, e.g. has the predefined latest execution time been reached.
As a result of these checks, the core settlement operation may not be successful and a settlement failure notification is sent to the sender.
Furthermore, there will be checks on blocked accounts/Parties.
If these checks are not passed (i.e., one of the accounts/Parties involved is blocked), the cash transfer order will be earmarked and its processing suspended (until possible approval/rejection by the CB or continuation after unblocking).
Additionally, the core settlement operation also includes provision checks on available liquidity on the reservations/segregation are possibly breached as well as specific offsetting checks.
If, on the one hand, these provision checks fail and all the aforementioned checks succeeded, the payment order will be queued for a re-attempt for settlement.
The queue will then be dissolved through offsetting with new incoming liquidity and optimisation algorithms, payment order amendment (e.g. change the order of payments in the queue) or through payment order revocation or through time-induced rejection (e.g. start of End of Day process, Reject Time reached).
If, on the other hand, these provision checks succeed, the core settlement operation will result in a success and RTGS will finally and irrevocably book the payment order on the debit and credit accounts involved.
In that case, RTGS can optionally send a settlement success notification to the sender of the order.
All in all, the sender will receive - as long as no additional instructions are sent affecting the settlement of the original payment order - at maximum one notification related to the payment order from RTGS through push-mode: either a rejection (negative validation), or a failure (no settlement, e.g. Reject Time reached), or a revocation, or a success notification.
The payment order settlement process described in this section is as generic as possible, i.e. the description aims at capturing the essential user requirements imposed by the different RTGS functionalities: High Value Payments (HVP) and Ancillary Systems (AS).
While main features of the settlement process are described in this section, the discrepancies with and specifics for settlement of Ancillary System transfers can be found in section 2 of this User Requirements Document.
Where the messages are sent packaged in a file, RTGS shall check the validity of the file and split it into single messages.
Each message should keep track of the original file reference, notably for monitoring purposes.
Furthermore apart from instructions to RTGS no other types of requests are allowed to be sent in a file (e.g. queries).
Validation errors after file splitting only cause rejection on a single message level, i.e. not the entire file is rejected.
Other successfully validated instructions included in the same file are further processed.
RTGS shall parse the message and perform a field level validation - e.g. on correct data type, size.
RTGS shall check whether all mandatory fields are populated.
If the validation fails, a rejection notification with appropriate error code(s) must be sent to the sender of the message (depending on the submission channel, a notification in A2A mode or an error message is displayed on the screen in U2A mode).
The component interface shall ensure that the same message has not already been received on the same business day.
A Central Bank acting on behalf of a credit institution.
The check has to be performed as soon as the message has passed the technical validation.
Business Validation - Check on value date for non-warehoused payment Excluding warehoused payment orders, RTGS shall only accept a payment order that specifies a value date as of current business date, except when the CB has activated the back-valued payments for one RTGS account holder.
In such a case, the value date check is de-activated.
RTGS will send non-warehoused payment orders having passed all the checks described above, immediately to the business validation step described below.
Business Validation - Check on value date for warehoused payment orders RTGS shall only accept a warehoused payment order that specifies a value date that is not later than ten calendar days from the business day on which RTGS received the payment order.
Nonetheless, RTGS shall perform the authorisation checks described above as soon as the message has passed the technical validation, in particular, before the value date.
Once the value date is reached and RTGS opens for payments (see section 3.4 on Availability of services in the User Requirements Document for Common Components), RTGS will send the warehoused payment order automatically and immediately to the business validation step described below.
RTGS will perform the checks described below in one step in order to capture all the possible breaches; the checks therefore must not stop after the first breach occurring, as there could be further breaches in the subsequent checks.
If the validation failed overall, RTGS must send rejection notifications with appropriate error codes for all breaches which occurred, to the sender.
RTGS shall check consistency versus a to-be-defined set of rules which depend on the message type.
Customer payment orders will have to pass specific checks, whereas interbank payment orders will have to pass different checks.
RTGS shall check whether a Direct Debit Mandate exists between the account to be debited and the payee Party, and that the maximum amounts granted in the Mandate are not exceeded.
In addition, RTGS shall check that the maximum amount for direct debit order allowed to be debited for the account based on direct debit orders per business day is not exceeded.
Backup payment orders are accepted only where the CB has activated the feature for its RTGS account holder.
The mandated payment order is sent by a Central Bank on behalf of one of its RTGS account holders in case of contingency situations.
The system should identify the accounts to be debited and to be credited from the BIC11 indicated in the message.
In CRDM, each BIC11 is mapped to only one RTGS DCA, may it be for the direct RTGS account holder itself (including multi-addressee) or its indirect participants.
The RTGS account holders have the possibility to determine the execution time of their payments, through From Time and either Till Time or Reject Time.
RTGS shall ensure that a payment order can only be submitted to settlement if its From Time, if indicated, has been reached.
The payment order may specify an earliest time at which RTGS shall submit the payment order for settlement.
RTGS shall ensure that a payment order can only be submitted to settlement if its Reject Time, if indicated, has not yet been reached.
As soon as the Reject Time is reached and if the payment order has not been settled, the payment order will be rejected and a settlement failure notification will be sent out.
If Till Time has been specified instead, the payment order shall not be rejected when this time is reached and the payment order has not been settled, and RTGS shall allow it to be submitted for settlement beyond this time.
At 15 minutes before the indicated Reject Time / Till Time and if the payment order has not been settled, RTGS shall send out a warning notification to the holder of the RTGS account to be debited in U2A and, if the RTGS account holder or RTGS CB account holder has subscribed to A2A notification messages, in A2A.
The payment order may specify a latest time by which RTGS has to submit the payment order for settlement.
When RTGS checks the eligibility of a payment order for settlement, then it shall verify whether the current date and time is less than or equal to the latest time for settlement specified in the payment order.
RTGS shall ensure that a new payment order can only be submitted to settlement if the relevant cut-off time is not yet reached.
This has not been decided yet, and will be further discussed during the realisation phase.
See section 3.4 on Availability of services in the User Requirements Document for Common Components).
RTGS shall ensure that a queued payment order can only be settled until the relevant cut-off time is reached, and the last optimisation algorithm has run (see SHRD.UR.BD.OPER.000.030 on Cut-off in section 3.4 on Availability of services in the User Requirements Document for Common Components).
Through this activity, RTGS will check whether the payment order settlement can be attempted (notably including offsetting).
RTGS shall process payment orders according to their priority classification.
If no priority class is selected, RTGS shall handle payment orders as normal payments.
Conditions for settlement attempt of urgent and high payment orders RTGS shall ensure that an urgent or high payment order can, apart from the exception described below, be submitted to settlement only if no payment order with a higher or the same priority is queued on the same account to be debited.
RTGS shall use the FIFO principle based on submission timestamp to sequence.
Conditions for settlement attempt of normal payment orders - so called "FIFO RTGS shall ensure that a normal payment order can, apart from the exception described below, be submitted to settlement only if no payment order with a higher priority is queued on the same account to be debited.
This means that the submission time for normal payment order is meaningless.
Exception for settlement attempt – offsetting with liquidity increase Even if the conditions described above are not fulfilled, RTGS shall nevertheless attempt settlement for the payment order if bilateral offsetting between the debited and credited accounts brings additional liquidity to the debited account.
In the event that this optimisation feature does not improve the debited RTGS account holder’s liquidity, RTGS shall queue the payment order.
When RTGS has submitted a payment order to settlement, offsetting is required in order to reduce the liquidity needed for its settlement, in any case.
The RTGS account holder whose account is subject to the credit is not blocked.
The RTGS account holder whose account is subject to the debit is not blocked.
The bilateral or multilateral Limits are not breached for normal payment orders.
For a EURO-CB, this check is not relevant since a EURO-CB Account can be negative.
For a non-CB Party, the credit line is managed within CLM, so the balance on the debit account cannot be negative.
The condition for drawing liquidity depends on the priority of the payment order.
As described hereafter, a payment order can draw liquidity from its own reservation and from lower level reservations.
RTGS shall check whether the credited account is eligible (i.e. not blocked) for being credited and the debited account is eligible for debiting.
If the check fails, RTGS shall earmark the payment order and shall, for the time being, take it out of the processing.
The payment order can be re-released or rejected through authorisation by the Central Bank of the blocked account.
RTGS shall check whether the credited Party is eligible (i.e. not blocked) for being credited and the debited Party is eligible for being debiting.
If the check fails, RTGS shall earmark the payment order and shall, for the time being, take it out of the processing.
The payment order can be re-released or rejected through authorisation by the Central Bank of the blocked Party.
RTGS shall perform a check toward bilateral and multilateral Limits, only for normal payment orders.
First, RTGS shall check whether a bilateral Limit exists between the debited account and the credited account.
Where the amount of the normal payment order is less than the free bilateral limit position, the check is positive.
Where no bilateral Limit is defined, RTGS shall check the multilateral Limit.
Where the amount of the normal payment order is less than the free multilateral limit position, the check is positive.
If this is still not enough, then additionally from the H reservation Where not enough liquidity is available, RTGS shall queue the payment order and then check whether the user has configured a rule-based liquidity transfer order for the event where there is a of lack of cash for U payment orders, to draw liquidity from the MCA linked to its RTGS DCA (through the associated liquidity transfer account link).
Where not enough liquidity is available, RTGS shall queue the payment order and then check whether the user has configured a rule-based liquidity transfer order for the event where there is a lack of cash for H payment orders, to draw liquidity from the MCA linked to its RTGS DCA (through the associated liquidity transfer account link).
RTGS shall ensure that a normal payment order will, if any, draw liquidity from the non-reserved liquidity (balance of the account minus the U and H Where not enough liquidity is available, RTGS shall queue the payment order.
RTGS shall ensure that the payment orders are ordered, by default, according to the submission time, i.e. FIFO.
This default order may be changed through amendment/revocation of queued payment orders (see section 1.3 on Queue Management/Payment Order Amendment and section1.4 on Queue Management/Payment Order Revocation).
Optimisation has the objective to dissolve as soon as possible the queues.
It can be either event- based, i.e. triggered when any event that can help settling a payment order occurs, such as new liquidity on an account or settlement of a payment order higher in a queue, or time-based, i.e. started regularly, to take into account all the events that occurred since the last optimisation.
Optimisation is aiming at resolving the reasons for non-settlement, i.e. either lack of liquidity through offsetting, or breach of a Limit which can be bilateral or multilateral.
It is described in terms of objective (to increase the number of settled payments) and constraints (balances and limits, order in the queues).
Optimisation is designed in a way to provide liquidity-saving features.
RTGS shall reduce the stock of unsettled payment orders and minimise the needed liquidity through optimisation.
The constraints described before in the entry disposition (order in the queues, FIFO by-pass principle for normal payment orders, offsetting) need to be applied strictly.
RTGS shall post each and every payment order on a gross basis.
This is without prejudice to the use of offsetting effects in the provision check when RTGS submits several payment orders together for settlement and they settle simultaneously on a gross basis within one legal and logical second.
Where the amount in the U reservation is not enough, and the non- reserved liquidity for normal payment orders is not enough either, the remaining amount is deducted from the H reservation.
For each debiting high payment order, RTGS shall update the H reservation according to the available amount within the H reservation.
Where there is a pending reservation, RTGS shall reduce the Pending Value in the case of a credited payment bringing liquidity to the RTGS DCA, first the pending U reservation and then the pending H reservation, by the same amount.
RTGS shall, for each payment order crediting an account (whatever its priority), increase the free bilateral or multilateral Limit by the same amount.
At the Start of Day, limits are set according to the standing orders (so called Defined Limit), and are updated throughout the business day after each relevant credit and debit (so called Free Limit Position).
RTGS shall, for each direct debit, increase the used amount related to the maximum amounts defined for the Direct Debit Mandate as well as the maximum amount of direct debit orders allowed to be debited from the account per business day.
RTGS shall perform all of the specified updates above in one transaction on an all-or-none basis.
RTGS shall ensure that no credit or debit can take place on the RTGS DCA without being processed by the settlement process.
This requirement will prevent concurrency of different settlement processes for the same units of liquidity.
RTGS shall ensure that no update specified above can take place on the RTGS DCA without being processed by the settlement process.
RTGS shall ensure that, once booked on the cash accounts, cash debits and credits must be final, i.e. irrevocable and unconditional.
Once the payment is final, RTGS shall check whether the account balance is below the floor balance that the RTGS account holder defined for the account or is above the ceiling balance that the RTGS account holder defined for the account.
This check is performed only where the RTGS account holder has defined a floor and/or a ceiling for the account.
The check is done both on the debited and credited accounts.
Creation of a rule-based liquidity transfer order for submission to Central Liquidity Management to adjust the liquidity on the accounts involved so that the balance of the affected account reaches the specified target amount.
The outcome of this final check does not affect the finality of the settlement of the payment.
The process will be initiated by an RTGS account holder via sending of the respective message to RTGS.
If the message content is either invalid or would result in reference data checks to fail, it will be rejected and a rejection notification with appropriate error code(s) will be sent to the sender of the amendment.
If the message content is valid and reference data checks have been passed successfully, RTGS will perform an amendment attempt of the original payment order the amendment message is referring to.
If the amendment operation fails, an amendment rejection notification with appropriate error code(s) is sent to the sender of the amendment.
Where the amendment operation succeeds, RTGS will amend the original payment order accordingly and will send an amendment success notification to the sender of the amendment.
The re- ordering of the queued payment orders triggers their settlement attempt.
Where several payment orders were selected they will be put on top of the queue according to their previous order.
The re- ordering of the queued payment orders possibly triggers the settlement of another payment order.
Where several payment orders were selected they will be put at the bottom of the queue according to their previous order.
The processing has to be executed within the opening hours of HVP (see section 3.4 on Availability of services in the User Requirements Document for Common Components), i.e. from the opening of RTGS until the End of Day process starts, and outside the maintenance window.
Same as RTGS.TR.HVP.PAYT.010 (Perform Technical Validation).
If the validation failed, a rejection notification with appropriate error code(s) shall be sent to the sender of the payment order amendment instruction.
For direct debits, the debtor (=receiver) can initiate a reprioritisation and a reordering within the queue.
Additionally, RTGS.UR.HVP.PAYT.020.050 (Business Validation - field and reference data checks) and RTGS.UR.HVP.PAYT.020.005 (Check for duplicate payment order) apply.
RTGS shall check the validity of amendment instructions.
The re-ordering of the queued payment orders triggers their settlement attempt.
Where several payment orders were selected they will be put on top of the queue according to their previous order.
The default order is determined by the submission timestamp.
Move one or more payment orders to the bottom of the queue in which they are held.
The re-ordering of the queued payment orders possibly triggers the settlement of another payment order.
Where several payment orders were selected they will be put at the bottom of the queue according to their previous order.
The default order is determined by the submission timestamp.
If the validation failed, RTGS shall send a rejection notification with appropriate error code(s) to the sender of the payment order amendment instruction.
The original payment order to be amended with the respective payment order amendment instruction has to be in an intermediate (i.e. not end) state (excluding blocked payment orders) to be eligible for amendment (e.g. queued and not considered in an ongoing optimisation simulation process, an order for which the From Time was not reached yet or a warehoused payment order).
Thus, amendment of payment orders is not feasible if they are already in an end state (settled, rejected or revoked).
The availability can be also dependent not only on the state, but also on the attribute to be changed itself.
E.g., one can change the Till Time or Reject Time as long it has not passed, and only to a time which is in the future.
RTGS shall stop processing the original payment order from the general processing of payment orders before and while the requested amendment takes place.
This means that RTGS shall remove a currently queued payment orders from its queue, if it is not considered in an ongoing optimisation simulation process.
An original payment order for which the From Time is not reached yet or a warehoused payment order will be directly amended according to the valid payment order amendment instruction.
Depending on the most recent state of the original payment order and the attribute or the order in the queue which was amended, RTGS shall process the amended payment order through the core settlement operations chain.
If the queue order was changed, RTGS shall place the amended payment order at the respective position and the usual queue dissolution processes will capture it.
If, on the other hand, the priority has changed, RTGS shall place the amended payment order in the queue according to the new priority and the original submission time of the original payment order (i.e., the amendment does not result in an update of that relevant timestamp; the position in the new queue is determined as if the original payment order has already been placed to that queue originally).
This business process describes the revoke or recall of a payment order.
The process will be initiated by an RTGS account holder via sending of the respective message to RTGS.
If the message content is either invalid or would result in reference data checks to fail, it will be rejected and a rejection notification with the appropriate error code(s) will be sent to the sender of the revocation/recall.
If the message content is valid and reference data checks have been passed successfully, RTGS will perform a revocation attempt of the original payment order the revoke&recall request message is referring to.
If the original payment order has not yet reached a final status, the original payment is revoked and a revoke&recall execution notification is sent to the sender of the revoke&recall request and a payment revocation notification is sent to the initial sender or the original payment order.
If the original payment order has a negative final status (rejected or revoked) a denial notification with appropriate error code is sent to the sender of the revoke & recall request.
If the original payment order has a positive final status (settled) or the original payment order cannot be found in the list of payments of the current business day, the revoke&recall request is forwarded to the payment receiver quoted in the request (except it the original payment order is a direct debit or payment return ).
A notification is sent to the sender of the revoke&recall request informing that the request has been forwarded.
The receiver of the forwarded revoke&recall request is expected to respond either with a payment return or with a revoke&recall notification.
An incoming payment return is processed in the same way as any other payment order.
An incoming revoke&recall notification is forwarded to the receiver quoted in the notification.
The processing has to be executed within the opening hours of HVP (see section 3.4 on Availability of services in the User Requirements Document for Common Components), i.e. from the opening of RTGS until the End of Day process starts, and outside the maintenance window.
The revocation via U2A is possible on the payment orders in the queue only.
Same as RTGS.TR.HVP.PAYT.010 (Perform Technical Validation).
RTGS shall ensure that the Revoke&recall request can be sent by the RTGS account holder, the respective Central Bank acting on behalf of its credit institutions/customers or by any other authorised system user.
If the validation failed, RTGS shall send a rejection notification with appropriate error code(s) to the sender of the Revoke&recall request.
For direct debits, the creditor (=sender) can initiate the revocation.
Additionally, RTGS.UR.HVP.PAYT.020.050 (Business Validation - field and reference data checks) and RTGS.UR.HVP.PAYT.020.005 (Check for duplicate payment order) apply.
RTGS must reject the Revoke&recall request, which refers to a payment order RTGS has already rejected or revoked.
A denial notification with the appropriate reason code is sent to the sender of the revoke&recall request.
A payment order eligible for revocation can either be a queued payment order, an order for which the From Time is not reached yet or a warehoused payment order.
Payment orders which are captured in an optimisation cycle must also be treated as "potentially settled" and are therefore not available to an immediate revocation.
The check for status should also wait for a short period of time until a currently ongoing optimisation cycle is over, so that the payment orders not settled within this settlement attempt reached again an intermediate state.
Payment orders which are already settled cannot be revoked anymore.
In this case or when the payment order cannot be found in the system, the revoke&recall request received via A2A is forwarded to the receiver quoted in the request.
RTGS shall revoke the original payment order according to the valid revoke&recall request.
RTGS shall forward the revoke&recall request to the receiver quoted in the request in case the original payment has been settled or if the original payment cannot be found.
A notification is sent to the sender of the revoke&recall request informing that the request has been forwarded.
RTGS shall forward the Revoke&recall notification to the receiver quoted in the received notification.
RTGS DCA to another RTGS DCA within the same Liquidity Transfer Group.
The process will be initiated by either the RTGS account holder itself or by the AS on behalf of its settlement bank or by the CB on behalf of the RTGS account holder via sending the respective liquidity transfer order to RTGS.
If the liquidity transfer order content is either invalid or would result in reference data checks to fail, it will be rejected and a rejection notification will be sent to the sender (depending on the channel, a proper notification with the error code(s) in A2A mode or an error message on the screen in U2A mode).
If the liquidity transfer order content is valid and certain reference data checks have been passed, RTGS will attempt to transfer (part of) the liquidity amount requested to the account referred to.
Where the intra-RTGS liquidity transfer order (partly) succeeds, RTGS will transfer (part of) the amount requested and RTGS will send a (partly) transfer success notification to the Parties involved (where the Party opted for it).
The processing has to be executed within the opening hours of HVP (see section 3.4 on Availability of services in the User Requirements Document for Common Components), i.e. from the opening of RTGS until the End of Day process starts, and outside the maintenance window.
Same as RTGS.TR.HVP.PAYT.010 (Perform Technical Validation).
The checks described below will be performed in one step in order to capture all the possible breaches; the checks therefore must not stop after the first breach occurring, if there could be further breaches in the subsequent checks.
If the validation failed overall, a rejection notification with the appropriate error codes for all breaches which occurred must be sent to the sender.
RTGS shall carry out a duplicate submission control for incoming liquidity transfer orders.
This control shall include the following fields: Amount.
RTGS shall perform service specific authorisation checks.
A liquidity transfer order from the RTGS DCA of the account holder to the RTGS DCA of the same account holder dedicated to AS can be sent by the RTGS account holder, the AS, the AS on behalf of its settlement bank, the respective CB acting on behalf its RTGS account holder /AS or by any other authorised system user.
The liquidity transfer order can also be triggered by the scheduler in the case of a standing order.
The request for a liquidity retransfer from the RTGS DCA of the account holder dedicated to AS to the RTGS DCA of the same account holder can be sent by the RTGS account holder, AS or the respective CB acting on behalf of its AS or triggered by a standing order liquidity transfer order set up by the RTGS account holder.
This check is not performed if the debitor or the creditor is a CB Accounts.
Additionally, RTGS.UR.HVP.PAYT.020.050 (Business Validation - field and reference data checks) applies.
RTGS shall check whether the credited account is eligible (i.e. not blocked) for being credited and the debited account is eligible for debiting.
If the check fails, RTGS shall earmark the liquidity transfer order and shall, for the time being, take it out of the processing.
The liquidity transfer order can be re- released or rejected through authorisation by the Central Bank of the blocked account.
RTGS shall check whether the credited Party is eligible (i.e. not blocked) for being credited, the debited Party is eligible for being debited and, if relevant, whether the related Ancillary System is not blocked.
If the check fails, RTGS shall earmark the liquidity transfer order and shall, for the time being, take it out of the processing.
The liquidity transfer order can be re-released or rejected through authorisation by the Central Bank of the blocked Party.
RTGS shall check whether enough liquidity is available.
Where there is a lack of liquidity and partial execution is not allowed, the liquidity transfer order shall be rejected.
If the liquidity transfer order is initiated either by an AS on behalf of its settlement bank or by an automatic trigger from the scheduler, RTGS shall settle the liquidity transfer order partially.
For several standing orders, where the sum of all standing orders for intra-RTGS liquidity transfers of the RTGS account holder to be settled at the same event, is larger than the available liquidity; RTGS shall reduce all respective standing orders in a pro-rata mode.
RTGS shall book the liquidity transfer order finally and irrevocably on the two RTGS accounts.
RTGS shall send a (partly) success notification to the sending Party and to the holder of the debited RTGS account.
This process is the second part of the CLM “Process inter-service liquidity transfer order from MCA to DCA" (see section 1.2 in the User Requirements Document for Central Liquidity Management, business process CLM.BP.CLM.LTSEN), the part within RTGS.
It is similar to the process described within CLM "Process inter-service liquidity transfer order from DCA to MCA" (see section 1.3 in the User Requirements Document for Central Liquidity Management, business process CLM.BP.CLM.LTRCV).
For the sake of simplicity, only the specific rules are described here.
This process is the first part of the CLM “Process inter-service liquidity transfer order from DCA to MCA" (see section 1.3 in the User Requirements Document for Central Liquidity Management, business process CLM.BP.CLM.LTRCV), the part within RTGS.
It is similar to the process described within CLM "Process inter-service liquidity transfer order from MCA to DCA" (see section 1.2 in the User Requirements Document for Central Liquidity Management, business process CLM.BP.CLM.LTSEN).
For the sake of simplicity, only the specific rules are described here.
The only specific rule is for liquidity transfer orders triggered by a lack of cash in CLM.
The rule for Partial Settlement is different from the standard one defined in RTGS.UR.HVP.LIQT.040 (Partial order).
Automated liquidity transfer orders triggered by a lack of cash in CLM can be partially settled.
RTGS shall confirm to CLM the settled amount and create a new automated inter-service liquidity transfer order for the remaining part.
The new inter-service liquidity transfer order should be queued.
In such a case, the processing will be similar to payment orders as described in the process "Payment Order Processing" (see section 1.2 in this User Requirements Document), considering that those automated liquidity transfer orders are with urgent priority.
The respective liquidity transfer order shall be placed on top of the queue of all pending payment and liquidity transfer orders.
In case CLM sends a new inter-service liquidity transfer order to RTGS that is triggered by a lack of cash on MCA, while there is already a pending liquidity transfer order in RTGS that is triggered by a lack of cash on the same MCA, then RTGS shall revoke the pending inter-service liquidity transfer order and queue the new inter-service liquidity transfer order with the same conditions.
A standing order liquidity transfer order will not generate any liquidity transfer order if the account to be debited is blocked for debits.
This process is the RTGS part of the CLM process "Process liquidity transfer order between two DCAs in different settlement services" (see section 1.5 in the User Requirements Document for Central Liquidity Management).
No specific rule has been identified for this process.
The initiation of this process takes place through the execution of a standing order to reserve liquidity scheduled for at the Start of Day or through the receipt of a liquidity reservation order from the RTGS account holder or another entity that the RTGS account holder has authorised to act on its behalf.
RTGS shall send a rejection notification with the appropriate error code(s) if either the technical validation or the business validation fails.
If RTGS completes both the technical validation and the business validations without identifying any errors, then RTGS will attempt to reserve the requested amount on the account referred.
The amount that cannot be reserved is called the Pending Value and is queued.
Following any incoming credit, the Pending Value is updated if possible and the Defined Value (i.e. the reserved amount minus the related debits) of the related reservation is increased.
The processing has to be executed within the opening hours of HVP (see section 3.4 on Availability of services in the User Requirements Document for Common Components), i.e. from the opening of RTGS until the End of Day process starts, and outside the maintenance window.
Same as RTGS.TR.HVP.PAYT.010 (Perform Technical Validation).
RTGS shall ensure that the reservation request can be sent by the RTGS account holder, the respective CB acting on behalf its credit institutions/customers, or by any other authorised system user.
The request can also come from the scheduler in the case of a standing order.
If the validation failed, a rejection notification with appropriate error code(s) shall be sent to the sender.
Additionally, RTGS.UR.HVP.PAYT.020.050 (Business Validation - field and reference data checks) and RTGS.UR.HVP.PAYT.020.005 (Check for duplicate payment order) apply.
RTGS shall check if the liquidity available covers the requested reservation amount.
According to the check, RTGS shall create a partial reservation request with the amount which can be immediately covered.
RTGS shall reserve this covered amount for the purpose indicated immediately.
The amount which is surpassing the available liquidity coverage is called Pending Value.
RTGS will queue the remaining (reduced) pending part and will process it in an event-oriented way.
Whenever there is an increase in the available liquidity an asynchronous resolving process attempts to process the pending reservation order.
Even if the increase of available liquidity is not sufficient for the complete processing, RTGS shall process the pending reservation partly (RTGS shall decrease the pending reservation and increase the defined value).
RTGS shall allow for interventions on pending reservation requests.
New reservation requests related to the RTGS DCA of the account holder will either increase the pending amount, or decrease it.
When receiving a new reservation request, RTGS shall stop processing the pending reservation request for the respective reservation type and take into account only the latest request.
Due to the asynchronous processing incoming liquidity might be blocked and used by a parallel booking process before the attempt to increase the reservation has been performed.
Upon receipt of the End of Day notification or a new reservation order, RTGS shall stop to process of the original reservation order.
This section describes the RTGS services for Ancillary Systems (AS).
RTGS processes AS transfer orders on the technical accounts of AS and the accounts of the AS settlement banks.
Besides DCAs for securities and instant payments settlement, it has an RTGS DCA for High Value Payments (with reserved amounts for urgent AS related transfers) and two accounts for AS transfers: one account (for AS procedure "Settlement on dedicated Liquidity Accounts (interfaced)") as a sub account of the RTGS DCA for High Value Payments, the second account (for other AS) as an RTGS DCA dedicated to one or several AS.
Interest on Guarantee Funds Accounts and Technical Accounts will be computed based on an ACT/360 rate convention.
The settlement banks will be able to grant to the AS the right to debit the account of the bank for the AS settlement.
U priority will be used for AS transfers, and From Time / Information period / Settlement period can be defined (see section 1.2 on Payment Order Processing).
AS transfers can be sent in an AS batch message (see section 1 on Eurosystem Single Market Infrastructure Gateway in the User Requirement Document for Common Components).
For procedure 6 RT, one account being the so-called technical account for procedure 6 RT which will have an End of Day balance will be used per AS.
For procedure 6 Interfaced, the accounts to be used by the settlement banks will be sub accounts and an AS can use a technical account.
The additional specific features for procedures 6 RT and Interfaced are described below.
AS transfers will be sent by dedicated AS batch messages (ASTransferInitiation).
Information and settlement periods will be provided as they were formerly in TARGET2.
RTGS will manage the links as formerly in TARGET2 ("Debits first" or "all or nothing"), according the parameters set for the AS in CRDM (which procedure is used, see section 9 on Business Data Definition in the User Requirements Document for Common Components).
Regular liquidity transfer orders (e.g. from RTGS DCA to sub account) at these business events can be set up through standing orders.
Whenever the AS using this interfaced procedure starts a cycle, the liquidity on the sub account involved will be controlled by the AS.
During a running cycle, a liquidity transfer which increases the balance on the sub-account settles immediately.
The control is given back to the settlement bank through the end of cycle.
Always possible, either through an immediate liquidity transfer order or a payment order4.
Can be realised as immediate liquidity transfer orders between two different AS technical accounts owned by ACHs.
The Ancillary System Transfer Processing is similar to the High Value Payments processing, meaning that the processing of AS transfer orders has many similarities with the processing of HVP payment orders, except the specificities described below.
The common monitoring of different AS transfer orders sent in one "batch".
The processing has to be executed within the opening hours of AS functionality (see section 3.4 on Availability of services in the User Requirements Document for Common Components), i.e. from the opening of AS functionality until the End of Day process starts, and outside the maintenance window.
Transfer messages from the AS can be sent in "batch" mode, i.e. through files or batch messages, - for multilateral settlement procedures 4 and 5, the settlement should not break the links ("Debits - for monitoring purposes, for all procedures 3 to 6, it should be possible to have a complete view on the status of all the payment orders in the file/batch message.
Same as RTGS.TR.HVP.PAYT.010 (Perform Technical Validation).
RTGS shall check if the Ancillary System is, indeed, authorised to debit/credit the settlement bank according to a list of settlement banks per Ancillary System.
If the validation failed, rejection notifications with appropriate error code(s) must be sent to the Ancillary System.
The "settlement period" is a time period set by the sender.
An AS transfer order can only be submitted to settlement if its "settlement period", if indicated, has not yet elapsed.
The "information period" is a time period set by the sender.
An AS transfer order can only be submitted to settlement if its "information period", if indicated, has already elapsed.
If no "information period" is indicated, no restriction applies in that respect.
At the start of the information period, the system will be informing the settlement banks about the upcoming settlement via U2A broadcast as well as A2A, if the settlement bank has subscribed to A2A notification messages.
Similar to RTGS.TR.HVP.PAYT.040 (Perform entry disposition).
The main difference stems from the fact that single AS transfer order will be of urgent priority by default.
That means that the entry disposition follows the same pattern for each single AS transfer order.
Either they are settled immediately or they are allocated to the U queue.
For batch messages of AS transfer orders, the links have to be respected in the entry disposition.
As for reservations, there will be a special reservation for AS transfer orders /U payment orders in place.
Limit check: as all AS transfer orders are of urgent priority, there is no check against bilateral or multilateral Limits.
RTGS shall check whether the credited account is eligible (i.e. not blocked) for being credited and the debited account is eligible for debiting.
If the check fails, RTGS shall earmark the AS transfer order and shall, for the time being, take it out of the processing.
The AS transfer order can be re-released or rejected through authorisation by the Central Bank of the blocked account.
RTGS shall check whether the credited Party is eligible (i.e. not blocked) for being credited and the debited Party is eligible for being debiting.
If the check fails, RTGS shall earmark the AS transfer order and shall, for the time being, take it out of the processing.
The AS transfer order can be re-released or rejected through authorisation by the Central Bank of the blocked Party.
For AS Transfers, RTGS shall check whether the Ancillary System is eligible (i.e. not blocked).
If the check fails, RTGS shall earmark the AS transfer order and shall, for the time being, take it out of the processing.
The AS transfer order can be re-released or rejected through authorisation by the Central Bank of the blocked Ancillary System.
Blocking for "Settlement on dedicated Liquidity Accounts (interfaced)" RTGS shall respect that during the settlement process of settlement procedure "Settlement on dedicated Liquidity Accounts (interfaced)" the sub account balance is exclusively reserved for the AS settlement in the case of a running cycle.
RTGS shall consider linkage constraints due to multilateral settlement.
For linked AS transfer orders, the check has to be successful for all linked AS transfer orders involved (possibly at different points in time for the standard multilateral settlement).
If balance check fails for AS transfer orders, and no guarantee funds mechanism has been envisaged, RTGS shall queue order(s) until the end of the settlement period or End of Day, respectively.
RTGS shall consider usage of guarantee funds with respect to settlement: If the first balance check fails, where a guarantee mechanism has been envisaged for linked AS transfer orders, a guarantee fund usage request is sent out to the Party controlling the guarantee account when the intended settlement period has elapsed.
The request can either be accepted or rejected by the AS.
If it was accepted, the guarantee funds will be considered in a second step upon.
That means, the accounts to be debited which lacked liquidity in the first step, will be replaced by the guarantee account.
If then still one of the various linked AS transfer orders cannot be settled, the process for revoking the batch message and unwinding of linked AS transfer orders (see RTGS.UR.AS.AST.070.010) should be started.
The system will inform the settlement banks via U2A broadcast as well as A2A, if the settlement bank has subscribed to A2A notification messages.
Similar to RTGS.TR.HVP.PAYT.060 (Queue payment order and optimise queued payment orders).
The main difference is the optimisation for linked AS transfer orders described below.
RTGS shall consider linkage constraints within optimisation and due to multilateral settlement.
For linked AS transfer orders, the optimisation has to ensure that all linked AS transfer orders are processed such that the links are not broken.
Similar to RTGS.TR.HVP.PAYT.070 (Update cash balances and limit) with two additional requirements.
Unwinding for linked AS transfer orders - standard and simultaneous RTGS shall consider linkage constraints due to multilateral settlement in the case of unsuccessful settlement attempts.
For the standard and simultaneous multilateral settlement, if one of the debits fails, the debits already executed need to be unwound when the batch message is rejected.
The batch message shall be rejected when, after the settlement period, not all debits have been settled or if the AS or Central Bank on behalf revokes the batch message or at the End of Day, if a settlement period has not been defined.
The system will inform the settlement banks via U2A broadcast as well as A2A, if the settlement bank has subscribed to A2A notification messages.
Update cash balances - "Settlement on dedicated Liquidity Accounts (real In case of a successfully settled liquidity transfer or cross AS settlement, the notifications sent to the Ancillary System shall additionally contain information on the resulting balance of the technical AS account.
Same as RTGS.TR.HVP.PAYT.080 (Check balance floor and ceiling).
Availability, calculated on a quarterly basis, shall be at least 99.7%.
RTGS may be subject to incidents or failures, which may cause a temporary and unforeseen interruption of the availability of the component.
Regardless of the total number of such unplanned interruptions, the overall availability calculated on a quarterly basis shall be at least 99.7%.
Payment orders not settled in the “entry disposition” are excluded.
Payment orders stemming from batch procedures of AS are excluded.
Start of the measurement period will be adapted to neutralise the “morning queuing effect”.
On component opening days a maintenance window of at max two hours is foreseen for any kind of technical or functional maintenance.
RTGS shall ensure a recovery point objective value of zero minutes in the event of site failures.
Where there is a loss of a complete region the recovery point objective (RPO) shall not exceed two minutes.
The RPO is a point of consistency to which a user wants to recover or restart the service.
It is measured as the amount of time between the moment when the point of consistency was created and the moment when the failure occurred.
RTGS ensures synchronous point of consistency creations and, as a consequence, no data loss in the event of failures, unless the component cannot be restarted in the same region and a failover to the backup region has to be conducted.
In this case a data loss of two minutes will be tolerated.
RTGS shall ensure a recovery time objective value of one hour in the event of site failures.
Where there is a loss of a complete region the recovery time objective (RTO) shall not exceed two hours.
The RTO is the maximum amount of time required for recovery or restart of the service to a specified point of consistency.
In the event of a site failure, RTGS shall ensure maximum time of unavailability of one hour starting from the time when the decision to restart the component is made up to the time the component is restored.
In the event of a major failure or a regional disaster, RTGS shall ensure maximum time of unavailability of two hours starting from the time when the decision to restart the component is made up to the time the component is restored.
RTGS shall process 95% of the transactions within 2 minutes and 100% of the transactions within 5 minutes.
RTGS shall be able to process 50 transactions per second, enduring the peak load for at least one hour.
In the course of the component’s lifecycle the number of transactions to be handled might change due to market changes or adapted business behaviour.
To be able to cope with this, RTGS shall be able to handle higher throughputs.
RTGS shall be compliant with the Information Security Requirements and Controls.
For details see the Market Infrastructure Security Requirements and Controls document.
All requirements must be fulfilled in a central integrated way.
RTGS shall be compliant with Cyber Resilience Requirements.
For details see Market Infrastructure Cyber Resilience Requirements document.
All requirements must be fulfilled in a central integrated way.
The objective of this section is to provide the user requirements related to user interactions covering the usage of U2A or A2A mode.
A Graphical User Interface (GUI) shall be provided for components, offering functionality to access information in U2A mode.
The GUIs shall be harmonised to the best possible extent.
These requirements do not imply any particular consideration with regard to design and the implementation of the actual screens.
All components shall provide the functionality to query through U2A interface the modified data at the attribute level, the user performing the change and the timestamp of the change made.
It should be visible which attributes were changed, together with the new values.
The query shall return relevant business attributes of the Audit Trail.
All components shall provide the functionality to query system time to align the time of a connected application through an A2A interface.
All components shall provide the functionality to amend or revoke task(s) through the U2A interfaces.
The TARGET Service Desk, to act on behalf of any Party.
All components shall ensure that a user can only access functionality and data that is allowed by the access rights granted to the user through the Roles associated with the user.
All components shall provide the functionality to use the four-eyes approval process through U2A interface, allowing the authoriser to confirm, withdraw or amend.
The User Interaction section covers intraday queries.
For intraday queries, the Value Date would by default be the current business day.
For U2A queries, the Party BIC and the account number would be deduced from the data scope of the user.
The data scope is described in section 4.1 on User Roles and Access / Overview in the User Requirements Document for Common Components.
The extended list of the selection criteria and the output of the queries shall be defined in the UDFS.
All described queries in this section shall be provided in U2A and A2A mode unless otherwise stated.
RTGS shall provide the functionality to query the status and details of all cash transfers on any account.
The user can query within his data scope, which is determined by the Party BIC and the DCA number (Party BICs and DCA numbers in case of a Central Bank as a user).
In addition the query shall allow the user to specify any combination of the following optional selection criteria.
The query shall return all business attributes of the cash transfers including the processing status.
When a file is queried, the status of the file would be also provided with the remaining business attributes.
RTGS shall provide the functionality to query any message in XML format.
The user can query within his data scope, which is determined by the Party BIC and the DCA number (Party BICs and DCA numbers in case of a Central Bank as a user).
In addition the query shall allow the user to specify any combination of the following optional selection criteria.
The query shall return the message in XML format, including the processing status.
RTGS shall provide the functionality to query the balance on any account.
The user can query within his data scope, which is determined by the Party BIC and the DCA number (Party BICs and DCA numbers in case of a Central Bank as a user).
In addition the query shall allow the user to specify any combination of the following optional selection criteria.
This query is also relevant to query liquidity on AS level.
The query shall return the current and projected account balance and all business attributes of the account(s).
RTGS shall provide the functionality to query all reservations on any account.
In addition the query shall allow the user to specify any combination of the following optional selection criteria.
RTGS shall provide the functionality to query all Limits (multilateral and bilateral Limit) on any account.
In addition the query shall allow the user to specify any combination of the following optional selection criteria.
The query shall return all business attributes of the Limits.
Normal information provided in pull mode should be distinguished from alert broadcasts information provided in push mode.
RTGS shall provide the functionality to query on the account statement.
In addition the query shall allow the user to specify any combination of the following selection criteria.
The query shall return all business attributes of the account statement.
More information about producing, sending and downloading a report can be found in section 5 on Information and Reporting in the User Requirements Document for Common Components.
RTGS shall provide the functionality to change the order of payment orders (including warehoused payment orders) currently pending for settlement through U2A and A2A interface.
The change should only be possible for payment orders not having reached a final status yet.
RTGS shall provide the functionality to create a payment order through U2A interface.
RTGS will generate a UETR based on UUID Version 4 standard for the payment.
The ability to enter payment orders would be subject to necessary rights, allowing an organisation to control the use of this feature.
RTGS shall provide the functionality to amend the priority and/or the execution time of a payment order (including warehoused payment orders) currently available in the system through U2A and A2A interface.
The change should only be possible for payment orders not having reached a final status yet.
RTGS shall provide the functionality to revoke a payment order (including warehoused payment orders) currently available in the system through U2A and A2A interface.
Via U2A the revocation is only possible for payment orders not having reached a final status yet.
Via A2A a revoke&recall request is forwarded to the payment receiver in case the payment order has already been settled.
RTGS shall provide the functionality to revoke an AS batch message which has not reached a final status yet through U2A interface.
RTGS shall provide a functionality to create a liquidity transfer order through U2A and A2A interface.
RTGS shall provide a functionality to create a back-up payment order through U2A interface.
RTGS will generate a UETR based on UUID Version 4 standard for the payment.
This action has to be activated by the CB on RTGS account holder level.
RTGS shall provide the functionality to create a reservation order through the U2A interface and the A2A interface.
RTGS shall provide the functionality to amend the defined Limit value, if previously set, with immediate effect through the U2A interface and the A2A interface.
The change will be valid for the current business day only.
The table below shows a summary of the above described queries and actions in U2A and A2A mode.
The purpose of this document is to provide information to the FDP Executive Committee sufficient to allow their endorsement of the development, maintenance, hosting, and use of an on-line FDP Expanded Clearinghouse system [see http://sites.nationalacademies.org/PGA/fdp/PGA_171520].
This document explains the high-level technical and functional requirements, and provides information about the roles and responsibilities needed to support such a system, including the obligations of FDP and the obligations of other parties.
The document also includes a cost estimate for developing and maintaining this type of system for FDP members.
It does not include details about expanding access to the system to non- FDP members, though the system will be designed in such a way to permit such an expansion.
This Functional and Technical Requirements Document outlines the functional, performance, security and other system requirements identified by the FDP Expanded Clearinghouse System Development Working Group (EC-SDWG) as the proposed information system solution for the Expanded Clearinghouse.
The content of on-line profiles is expected to mirror the content currently found in the Expanded Clearinghouse pilot profiles found at: http://sites.nationalacademies.org/PGA/fdp/PGA_171219.
The scope of this work includes the initial development of the web based system, based on information and feedback gathered during the Phase 1 Pilot.
References to future development considerations are included in this proposal for information purposes only.
Points of Contact relevant to this project are listed on the first page of this proposal.
Once the project has received Executive Committee approval, this document will serve as a formal MOU detailing the agreed upon responsibilities and requirements.
A representative from each organization will be asked to sign the document documenting their organization’s acceptance of its roles and responsibilities.
This working group will be responsible for the administrative oversight and operations of the Clearinghouse system as detailed in the next section.
UW will serve as part of the system development group and provide back-up development and technical support should it be needed.
The FDP will serve as the ultimate oversight, in the form of the FDP Executive Committee to ensure appropriate review, support and approval is provided throughout Phase 2.
All other information must be reviewed at least annually.
No subscriber will be suspended without having first had an opportunity to cure.
There is currently no single, on-line electronic database containing all information needed for pass-through entities to perform risk assessments and to do ongoing monitoring of static or annualized data related to subrecipient monitoring.
Select data are instead housed in certain federal government systems, such as the System for Award Management (SAM) or the Federal Audit Clearinghouse (FAC) with the remainder retained by the individual entities themselves.
Certain data expected under the Uniform Guidance to be used for this purpose are as yet unavailable nationally to pass-through entities, including copies of A- creating data collection documents used with each other at time of subaward issuance or updating.
The plethora of forms coupled with most institutions collecting data on a per-subaward basis rather than on a per-entity basis has led to significant administrative burden without commensurate benefit from a risk management perspective.
The FDP Expanded Clearinghouse Phase 1 Pilot created a process whereby each pilot entity could provide a standard set of data and answers to questions in the form of an Entity Profile.
These Entity Profiles are currently maintained in excel and converted to pdf for to a centralized web site repository.
Data relative to administrative burden relief is being captured and will be reported quarterly beginning Fall 2016; early data suggest the relief will be meaningful.
The profiles themselves generally require updating at least twice per year, resulting in significant ongoing burden.
In addition, download of data for use in local systems is not available.
These results, along with the remarkable success of the original FDP Financial Conflict of Interest Clearinghouse that was eventually opened to non-FDP members and now includes more than 1000 entities, signified a need to change to a more automated and electronically robust process for the longer term.
This proposal reflects the FDP’s efforts to build that electronically robust system.
At designated intervals (at least monthly), the system sends an email reminding entities to update their System will also be able to send reminders on certain data elements when the data has become out of date (SAM expiration date, Audit date, etc) A list of these data elements will be developed by the ECWG and provided to the ECSDWG.
System provides designated FDP administrators with additional rights.
FDP Administrators create new entities and approve new profiles FDP Administrators can create, edit, and disable user accounts.
The FDP Expanded Clearinghouse Web-based system will require an initial team of developers and testers.
These roles will be filled by the EC-SDWG on a volunteer basis.
The project will require administrative, project management, and training duties that will be performed by the ECWG.
Any financial management needs of this project will be coordinated through David Wright, Mark Sweet and Pamela Webb with the FDP Executive Committee.
To support the ongoing needs of this system the FDP will require an oversight committee to oversee and manage the system and the community’s data to ensure ongoing reliability and integrity of the system (ECWG).
It is expected that once built, this system will exist on an ongoing basis with at least an annual review process build in to determine effectiveness, needs for updating or changes, potential need for termination, or other circumstances.
Entities will have access to their data and reporting, but publication of community-wide data will be authorized by the FDP Executive Committee.
In anticipation that the web-based FDP Expanded Clearinghouse will eventually replace the various data collection components currently maintained by or through FDP we expect the long-term overall impact to FDP to streamline the type of data currently being housed on the FDP webpage.
Users will interact with the system in real-time via the web.
FDP members will be expected to acquire and maintain a secure and reliable internet connection adequate to facilitate data entry by their staff.
Questions on the system in general, data entry, reporting and use will be facilitated by the FDP Expanded Clearinghouse Working Group.
FDP pilot institutions are required to alter their current subrecipient entity forms and internal processes to accommodate the expected data collection and data entry, including timeliness.
All user information, guidance and FAQ’s will be developed by the ECWG and EC-SDWG and maintained on the FDP webpage.
All Pilot entities will have access to instructions and training materials to access to the system, including FAQ’s regarding the system.
The FDP Expanded Clearinghouse will be maintained in the current Excel spreadsheet / PDF repository fashion until such time as a web-based system has been developed, testing and fully adopted.
Pilot entities will be required to assist in the transition between methods/systems.
Basic data logic warnings (e.g., Gender: Male with Pregnancy status: Y) Manual review and validation of new draft entity profiles by a designated FDP administrator, prior to profiles being added to the system.
The system is intended to be available online 24 hours per day, 365 days per year with the exception of scheduled and pre- notified system maintenance downtimes, if needed.
Data will become immediately available for use, except for new profiles, which will be pending in queue for validation by an FDP administrator.
The ECSDWG will ensure that system resources are adequate for timely response times and overall software functionality.
The ECSDWG will review ISP/hosting provider options and once the initial development is complete will move the clearinghouse over to this for hosting the system.
The system will be built and tested on Vanderbilt hardware and software and then transferred over based on agreement of ECSDWG.
The cost an ISP/hosting provider is estimated to be approximately $1,200 annually.
Temporary inaccessibility, even up to several days, will not create a substantial burden on any user.
The host site for the system will be chosen so as to include data backup capabilities and protocols.
VUMC will maintain a copy of the code on Vanderbilt’s network, which has daily backup protocols.
Additionally, should the ECSDWG believe it to be prudent, a copy could be kept at University of Washington or another backup site.
It is expected that with the use of an IPS/hosting provider that downtime will be minimal or non-existent.
The proposed FDP Expanded Clearinghouse system will consist of a web-based, centralized database Entity Profile and reporting to be utilized in the support of ongoing subrecipient entity monitoring activities and responsibilities by the Entities.
Generally, all users will provide direct input into the system and outputs (reports) will also be generated directly from the system.
However, to ensure growth ability, flexibility is also required for both input and output modes.
Participating Pilot Entities will provide input (i.e., entity level data) and the ECSDWG, as an agent of the FDP, will provide system administration and support for report generation.
The system is planned to be originally developed principally by staff at Vanderbilt University Medical Center, in close consult with the ECSDWG.
As feasible and agreed upon, David Wright and Jason Myers might also assist in components of the development.
An additional desired functionality of the system is to integrate with other external systems.
The FDP Expanded Clearinghouse system will be desired to have the capacity to import and export data without ongoing support by the ECSDWG.
To this end, the system will expose a RESTful API via HTTP to provide data in JSON format for external consumer access.
The clearinghouse system will also include the functionality to interface with the federal System for Award Management (SAM) web services to access certain data elements as defined in the functional requirements.
In addition exploration of utilizing the bulk download capabilities currently available in the Federal Audit Clearinghouse will be explored to allow for the possibility of utilizing that data for uploading entity profile data elements as well.
The desirability for the ECSDWG to continually update and improve the system is a given.
However, the FDP will also require that the system be flexible and customizable to suit their needs.
The complexity of the system will limit the customizations available via the administrative interface at the FDP level.
However, the code should be structured to make customizations a reasonably accessible task for a PHP programmer, and the FDP will have access to the code repository to make such changes as desired.
Any tools that will be utilized, outside of those discussed in this proposal will be discussed an agreed upon among the ECSDWG prior to use.
No closed source or proprietary tools will to be used.
The system will be developed under the leadership of Vanderbilt University Medical Center using industry standard web development tools and practices.
VUMC commits to develop the initial application as described in this document, and to provide additional support and development services up to 5 hours per month on an ongoing basis, without charge to FDP.
Either party may pursue a transfer of maintenance responsibilities at any time.
The ECSDWG will perform at least an annual review of how the system is working and whether responsibilities need to be shifted or changed in any way.
VUMC may, at its discretion, or as contracted by FDP in exchange for appropriate remuneration, provide additional support or development services beyond this commitment.
The FDP and ECSDWG will have access to the source code for the software and may work with other parties to extend, enhance, or edit the system in collaboration with VUMC provided the changes and enhancements are committed to the GitHub repository.
Source code will be stored on GitHub or in another mutually agreed repository.
There is no anticipated need for an end-user guide as system will include an intuitive user interface.
VUMC will provide such documentation as necessary or as requested by the ECSDWG or FDP Executive Committee for technical requirements, including but not necessarily limited to documentation of the system-to-system API.
Any end-user help documentation will be developed by the ECWG.
VUMC, as the primary developer, shall retain all right and ownership in the software product including but not limited to source code, including right to license the product (but not the data) to any third party.
VUMC will grant to FDP a perpetual, worldwide, royalty-free, non-exclusive, non-transferable license to the software product and derivative works, without the right to sub-license, for FDP and its agent(s) to use the software product for its own purposes.
This shall include no more than one production instance at any time, with unlimited backup, development and test copies permitted to maintain, improve and test the software as necessary.
FDP shall retain all right and ownership in its data.
VUMC shall receive a perpetual, royalty-free license to the data strictly for the purposes of maintaining, improving and supporting FDP’s installation of the software product.
The ECSDWG will work together to determine where the system may need to be configurable and ensure that all parties are in agreement and parameters are appropriately documented.
Under the leadership of VUMC, the initial system will be developed in iterations.
As the developers complete portions of the application, they will make the updates available to ECSDWG for review.
These reviews are intended to keep the application development on course, addressing any miscommunications early and providing the ECSDWG with a clear understanding of how the work is progressing.
The ECSDWG will review the submitted product and provide a notice of acceptance or s on changes that need to be made in a timely manner.
This iterative process will repeat until ECSDWG is satisfied with the software product.
Once the parties agree that the software product is in a “Beta”, or near-final state, ECWG members be enlisted for private live testing, including entering new entity profiles, to flesh out remaining bugs and process issues to be fixed before go-live.
Once the initial system is complete, the ECSDWG will submit the software product to ECWG for final review and approval.
Additional timelines and documentation related to the go-live process will be developed, as needed including potential review and approval process by the FDP Executive Committee.
The ECSDWG will be responsible for utilizing currently existing equipment either at VUMC or the FDP, or for contracting with a web hosting service for server space.
No additional equipment is anticipated at this time.
The clearinghouse application will be built with PHP version 7, an open-source web scripting language.
Data will be stored in a MySQL database, also open source.
The user interface will be developed in HTML5, CSS3, and JavaScript.
VUMC will be employing components from standard and commonly accepted libraries such as PHP Symfony and jQuery for Javascript.
PHP dependencies will be managed via the Composer package manager.
The application will be able to run on any web server that supports PHP 7 and has a MySQL database.
The ECSDWG will be responsible for securing the necessary server space.
The application code will use Git version control, and all commits will be archived in a designated repository which can be made available to other ECSDWG members.
Source code will be stored on a mutually agreed platform.
If FDP so desires, Vanderbilt will provide server space for hosting the application.
If FDP decides to have the application hosted on a third-party’s servers, Vanderbilt will work with the hosting provide to assure that it meets the needs of the application developers.
This domain will be configured to point to the web servers used for this project.
We anticipate this will be a minimal time commitment (less than 1 hour per week).
Updated structure and initial input from SINTEF and ONTO.
Initial draft ready for 1st round of internal peer review.
Updated draft ready for 2nd round of internal peer review.
The main goal of the euBusinessGraph project is to create the foundations of a European cross-border and cross-lingual business graph through aggregating, linking, and provisioning (open and non-open) high-quality company-related data, thereby demonstrating innovation across sectors where company- related data value chains are relevant.
This is achieved by leveraging the power of the emerging technologies such as Data-as-a-Service and Linked Data.
Serve as an entry point for company-related data discovery, exploration and analytics; and Grow an ecosystem of 3rd party applications and company-related data services.
This report presents Deliverable D3.1 "Requirements Analysis, Architecture and API Specification for the euBusinessGraph Marketplace – v1" of the euBusinessGraph project.
This deliverable is developed as part of Work Package 3 (WP3) "euBusinessGraph Provisioning".
The objective of WP3 is to develop, integrate, deploy and maintain the technical infrastructure, methods, tools and services for reliable provisioning of the business graph by applying the Data-as-a-Service (DaaS) paradigm.
WP3 will cover aspects related to simplifying the onboarding and population of the business graph (including data preparation and transformation mechanisms and semi-automated interlinking of data), cost-effective hosting of business graph data, support for cross-cutting business cases analytics tasks, and creation and maintenance of a data marketplace for the business graph.
WP3 aims to adapt tools and approaches for tasks such as data transformation, cleaning and integration, enrichment and interlinking, storage and scalable querying, access control, methodologies for data publishing, etc., in order to simplify the business graph data publication and consumption process, and analytics task on top of the business graph.
Work in the WP will include customisation of existing approaches and tools, where necessary extensions of components and methods needed to offer an integrated data infrastructure that the marketplace can be built upon.
WP3 will reuse and build upon results from the FP7 DaPaaS and Horizon 2020 proDataMarket projects for data preparation, hosting, and marketplace services.
To provide a preliminary design and architecture of the platform in terms of the core components, and a set of APIs that will guide the development of the platform in the next phase.
The development in WP3 follows an iterative approach resulting in two versions of the euBusinessGraph Marketplace platform released in month 12 and month 24 of the project as illustrated in Figure 1 below.
Deliverable D3.1 serves as the specification for the first release (Deliverable D3.2 in month 12).
An updated document (Deliverable D3.3 in month 15) will serve as the specification for the second release (Deliverable D3.4 in month 24).
The iterative approach allows us to develop the first prototype based on initial results in the project and adjust the development mid-way based on updated technical results and intermediate feedback and evaluation.
The requirements analysis for the first specification (Deliverable D3.1) is based on the business cases requirements and analysis from WP4 (Deliverable D4.1), initial data gathering results in WP1 and initial system of identifiers and vocabularies concepts developed in WP2.
Updated results from WP1 (Deliverable D1.1) and WP2 (Deliverable D2.1), and intermediate business cases feedback and evaluation, will be taken as input for the updated specification (Deliverable D3.3).
Section 3 presents a set of requirements for the euBusinessGraph Marketplace platform collected through analysis of dataset templates, business case descriptions and requirements input from the business cases.
Section 6 provides a brief summary and concludes this report.
This section gives an overview of the euBusinessGraph Marketplace platform and the relevant roles An overview of the euBusinessGraph approach is provided in Figure 2 below, depicting the connection between data consumers and providers of the business graph data, through the enhanced environment developed in euBusinessGraph.
The core element of the approach is the business graph component (second from the left in Figure 2).
WP2 addresses the design of the business graph, which includes a system of identifiers for company- related data, shared data models and multilingual aspects.
WP3 address the provisioning of the business graph, which includes support for data transformations, onboarding, hosting, access, analytics, data marketplace, and operations.
Data value chain #1 – from silo-ed data sources to customer segments (at the bottom of Figure : the value chain spans from the silo-ed data sources to various customer segments.
The value is created through the set of capabilities provided by the business graph provisioning infrastructure, which are in turn leveraged to establish a set of innovative business products and services.
Data value chain #2 – data value feedback chain (at the top of Figure 2): consists of the data insights generated by the proposed products and services being fed back into the business graph, therefore enhancing the value and scope of the data in the business graph.
This is euBusinessGraph’s mechanism to ensure that the business graph can host highly valuable and high-quality information, while at the same time increasing the chances of a self-sustainable business graph.
A stakeholder in this context represents a group or organization that has interests or concerns in the euBusinessGraph Marketplace Platform.
Data providers that own and/or manage company and company-related data.
As such, they can offer (i.e. provide, market and/or sell) their data via the business graph, or enrich and link their data using other relevant data offered via the business graph.
The data providers in euBusinessGraph are OCORP, CERVED, SDATI, DW, BRC and JSI.
Gazettes: Public records of company-related legal notices.
Public Administration Tenders: data about companies participating in public tenders.
The data consumers in euBusinessGraph aim to develop business products and services (the second from the right in Figure 2) that will access and use the data made available via the business graph for creation of data-driven products and services.
Below we only list and provide a brief description of the business products and services to be developed in the business cases.
OCORP Corporate Events Data Access Service (CED) is a new product to provide cross- jurisdictional data and alerts about changes in companies, deriving these from official primary sources (primarily company registers and government gazettes), and making them available in a standardised form.
CERVED Tender Discovery Service (TDS) is a new set of algorithms and services easing and facilitating discovery and participation of business companies in public administration tenders.
SDATI Atoka+ will extend the Atoka service, which currently only provides company-related data in Italy, to cover new jurisdictions, specifically company-related data in the United Kingdom and Norway.
DW Data Journalism Product (DJP) is envisaged to be a web-based application that supports journalists in dealing with complex and large volumes of company related data across the three journalistic workflows: search, monitoring and content production.
The customer segments (the first from the right in Figure 2) represent customers of the business products and services.
The focus here is on the value created to the end users of the business products and services.
This stakeholder is considered out of scope for the requirements analysis in WP3, as any requirements from such a stakeholder should be covered indirectly in the requirements from the business cases.
The marketplace technology providers in euBusinessGraph are SINTEF, OCORP, SDATI, ONTO, JSI and UNIMIB.
They are the providers of the marketplace services such as system of identifiers, shared data models, multilingual support, data transformations, data onboarding, data hosting, data access, data analytics, data marketplace, and operations.
In addition to the requirements from the data consumers' and data providers' point of views, there may be technical requirements and constraints from the technology providers' point of view that must be taken into account for the development of the marketplace services.
In particular, any design requirements and constraints from WP2 should be taken into consideration.
As can be seen in Figure requirements and constraints on the development of the euBusinessGraph provisioning services.
For this first version of the euBusinessGraph Marketplace platform specification, we have taken preliminary working results from WP2 into account.
This section provides details on the requirements analysis and the resulting requirements for the euBusinessGraph Marketplace platform.
Analysis of the business cases descriptions in Deliverable D4.1.
A set of initial dataset templates as part of Work Package 1 (WP1).
These inputs were analysed with respect to technical requirements for the euBusinessGraph Marketplace platform.
The technical focus for this analysis was on the data provider role that aims to share its data to the business graph.
The results of this analysis has been summarised into the requirements table below.
Data cleaning and transformation activities (RDF-ization).
Specifying access through different channels (e.g. reporting service).
APIs for accessing and modifying (updating) datasets.
Analysis of the business cases descriptions in Deliverable D4.1.
It should be d that amongst the business cases there are partners that represent both the data provider and data consumer roles (i.e., CERVED, SDATI and DW) and thus see things from both perspectives.
The result of this analysis has been summarised into the requirements table below.
Ability to download large volumes of data as data dumps.
Single access point to information about company data.
API keys for specifying access policies for different users.
As written in Section 2.2.3, we have also considered technology providers requirements for the data analytics services and preliminary working results related to the development of the system of identifiers and the common data model in WP2.
The data analytics of the euBusinessGraph Marketplace platform are supported by the Cross-Cutting Business Cases Analytics Services that are specified in sections 4.4 and 5.4.
For sharing of data in the business graph we have discusses three different options.
All data is shared: Data providers joining the business graph share all their data.
Advantages: Single point of access to all data, but only a subset is stored in the euBusinessGraph infrastructure.
Overcomes some big data scalability challenges when compared to option 1 (where all data is shared).
Data can be indexable, which allows for efficient data discovery, faceted search, ranking, etc.
Disadvantages: Simple analytics services can run on the data shared in the business graph infrastructure, while more advanced analytics services will require additional (local) data and must be run locally.
No data is shared (common data model + pointers to data): Data providers joining the business graph share descriptions of their data (metadata) with pointers to the actual data (search and access APIs).
All data that is offered via the business graph must be described according to a common vocabulary, i.e. the common data model.
Advantages: Does not require data providers to share any data.
Can be viewed as a virtual knowledge graph whose storage is distributed in different physical databases – where different parts are accessed under different licenses.
Disadvantages: Performance issues with federated search and data retrieval.
Makes it problematic to support faceted search, merging result lists, ranking, linking, or reliable pagination.
Will require data providers to implement a common search and data access API so that results can be ranked similarly amongst all data providers.
Based on the initial discussions and requirements input collected from the business cases, there were different views on the sharing of data.
While some data providers were willing to share data and making all their data indexable (e.g. to support faceted search), others were more concerned and wanted to use the euBusinessGraph Marketplace platform more as a means of enriching, promoting and marketing their data instead of sharing their data fully.
One particular requirement raised by the business cases was to make it simple for data providers to join the business graph.
Thus rather than requiring data providers to share all their data, a better strategy would be to give data providers a choice of how to join the business graph.
Data providers can choose whether they want to share all data, a common subset of data or only provide pointers to their data.
Data providers that primarily want to market their data, but not share it directly through the euBusinessGraph infrastructure, would be required to describe their data according a common vocabulary and implement a common search API that allows the euBusinessGraph Marketplace to query data that are being offered.
Data consumption in this case will be through the specific APIs of the data providers.
Data providers that are willing to share their data can use the data hosting facilities of the euBusinessGraph Marketplace platform, make use of cleaning and transformation services to map the data to RDF linked data, and make use of data interlinking services to enriched and linked their data with other data sources.
Some data providers may want to fully host and share their data in the euBusinessGraph infrastructure, while for other data providers it make more sense to only provide and share a minimum common subset.
This choice may depend on the size of the dataset, data governance, business policies, etc.
For the first release of the euBusinessGraph platform we are exploring support for option 3 and 2.
Initial work in WP2 is now developing a common vocabulary for the common data model.
This data model can be used to both describe the data being offered and used to share a common data subset.
The model is a foundation for the business graph, whose purpose is to distribute company-related data from various data providers in a uniform way and facilitate implementation of business products and services on top of this data.
The first users of the business graph are the business cases of the project that intend to consume data from the graph.
Work Package 2 (WP2) of euBusinessGraph will create and maintain a system of shared identifiers for companies in Europe, together with a mechanism for mapping to existing proprietary identifier systems, while at the same time relying on common conceptual models (e.g. shared vocabularies and ontologies) to address semantic heterogeneity and cross-lingual problems in company-related data.
An initial survey in WP2 has identified a set of external identifiers used in the relevant datasets for the business cases in the project.
See Appendix C for the list of external identifiers.
The common data model must be able to represent the identifiers used by the dataset, and the euBusinessGraph Marketplace platform must provide services that allows to create mappings between these identifiers.
This section describes platform architecture covering the data preparation, data interlinking, data hosting, cross-cutting business cases analytics, and marketplace and operational services.
For each of these service categories we are planning on developing a set of features as shown in Figure 4 below.
The Data Preparation Services will include reuse and customisation (including extensions where needed), of the Grafterizer1 data cleaning and transformation approach (service deployed as part of the DataGraft platform2).
This is meant to simplify data cleaning and transformation processes, and ensure the availability of services to help with the generation of the business graph data.
The focus of these services will be on intelligent support in data cleaning and transformations using approaches such as predictive interactions in data transformation pipelines, support for data profiling and automated data quality issues resolutions.
DataGraft comprises services and GUI tools that facilitate the importing of datasets into the platform.
OpenRDF API3 and a Linked Data Platform (LDP) API4.
The Grafterizer component of the DataGraft platform was initially developed to support general-purpose data cleaning and transformation operations so that it could be applied in many different settings.
Data cleaning and transformation in DataGraft platform is performed with the help of a “pipeline” concept.
In DataGraft you are able to see the partial preview of the transformation on each step.
Last option makes it possible to see how the transformed data looks like for every stage of transformation.
The Grafterizer component also supports creation of RDF mappings.
After having defined the pipeline for the data cleaning you can start creating RDF mappings.
In euBusinessGraph we will extend the Grafterizer component with predictive interactions in data transformation pipelines, support for data profiling and automated data quality issues resolutions.
An initial prototype demonstrating the visual data profiling extensions has been developed at SINTEF.
It extends the functionality of the current data cleaning and transformation framework of Grafterizer.
The prototype features capabilities for integrated, interactive data cleaning and transformation, and visual data profiling as shown in Figure 5.
Visual data profiling is the statistical assessment of datasets to identify and visualize data quality issues such as outliers or missing data values.
The approach has the potential to help data scientists and data workers make an informed decision on how to deal with data quality issues, and reduces effort spent on cleaning and transforming data.
The framework has been validated in terms of usefulness and ease of use, and will be publicly available in future versions of Grafterizer.
Predictive, intelligent data cleaning and transformation that recommends domain-specific and relevant next steps in the data preparation process.
The data cleaning and transformation steps are incrementally applied in a pipeline approach.
Visual data profiling that analyses and determines data quality based on statistical properties, semantics and structure of data.
The data quality assessment is presented to the user by means of statistical and scientific charts and visualizations.
The table view can be manipulated interactively by the user.
Reaching the main goal of euBusinessGraph project, i.e. the creation of a cross-lingual business graph through aggregating, linking, and provisioning (open and non-open) high-quality company-related data, requires to integrate and link several data sources.
Data interlinking is the task of establish semantic links between pieces of information represented in one or more than one independent sources.
Without loss of generality, when establishing links between two data sets, we distinguish between a source piece of information (the data that have to be linked) and a target piece of information (the data to which links are stablished).
Different problems of data interlinking can be devised depending on the types of data that are considered.
In euBusinessGraph interlinking is considered only against a reference Knowledge Graph (KG), i.e., the target data are represented as a KG.
Link discovery get KGs as input and produces a set of mappings.
In the following sections, we will discuss deeply about the three main problems that the Data Interlinking Services address.
Given a text, the Named Entity Linking (NEL) task consists in finding all named entities (sequences of words in the text that are names of things, such as person and company names, or gene and protein names) mentioned inside the text that are also represented in a KG.
Named Entity Disambiguation: to link the above mentions to entities in a KG (usually, instances of some class in the KG).
This linking task must consider the meaning of the mentions: since a named entity mention can refer to multiple entities (named candidate entities), this task has to resolve the appropriate meaning (disambiguation phase) in the considered context.
A class such as ‘person’ or ‘organization’ could be assigned to the identified entity at this point (several state-of-the-art existing tools accomplish this task, e.g. GATE, OpenNLP, Stanford Named Entity Recognizer, etc.).
However, since we aim to perform full linking to an established KG, which contains this information about the linked entities, this step can be skipped without any loss.
The output of this step will be refined in the next step.
The candidate named entities are used as input for the Named Entity Disambiguation task (Step 2 in This task includes deciding which candidate entity, if any, has to be linked.
The result of the disambiguation task may be a refusal to add a link for the candidate entity mention, if disambiguation is too uncertain to be trusted.
For example, if the text mentions the Louvre, Notre Dame and Avenue des Champs-Élysées than the string ‘Paris’ in this text is much more likely to refer to the French capital than miss Paris Hilton.
Some established systems perform only the Named Entity Recognition task, while most of systems that perform the full NEL pipeline do not share information about the output of subtasks performed by their algorithms, e.g., of the Named Entity Recognition task.
In euBusinessGraph we need to link texts to entities described in KGs, thus we need to perform the full NEL pipeline.
Several services performing NEL are available (e.g., DBpedia Spotlight, Babelfy, AIDA), none of which focus on company names, one of the key target class of entities in euBusinessGraph.
Wikifier5 (JSI): it links entity mentions to their Wikipedia concepts, i.e. the URLs of their related Wikipedia pages.
For a specific language it is built using the corpus of Wikipedia pages in that language.
It supports languages of the top 100 largest Wikipedias (i.e. largest corpus size in the sense of the number of pages).
Entity Extraction in Dandelion API6 (SpazioDati): it finds mentions of places, people, brands and events in documents and social media using SpazioDati reference KG.
It support fetching of additional data about the entities.
The NEL services described above fulfil also these additional requirements.
The main goal of semantic labelling approaches (sometimes called also Table Interpretation or Table Annotation approaches) is to map different (weakly) structured data sources to a common KG.
Since we can transform all of these sources and obtain a CSV, in the following we consider only CSV as sources (using also “tabular data” as interchangeable name).
The instances reconciliation is an important step because allows the data enrichment phase, whose exploits the links between entities to find new knowledge.
Some of existing Semantic Labelling tools focus on the schema alignment task (e.g. Karma, STAN, Datalift and Juma); some other tools focus on the instances reconciliation (e.g. OpenRefine).
There exist also tools that perform both (e.g. TableMiner+ and Linda).
These tools can operate at different level of automatization.
For the euBusinessGraph platform purposes, UNIMIB will provide a semi-automatic Semantic Labelling Service (SLS) based on the STAN (Semantic Table ANnotation) tool, able to accomplish the Semantic Labelling task either at schema and instances levels.
The SLP is a semi-automatic process (see Figure 8) that includes the user in the suggest/refine annotations loop to reach high-quality annotations with a low effort.
The tool takes advantage from the summarization capabilities of ABSTAT to support user in decision making.
The information carried by the annotation must be those that are needed to generate mapping useful for the RDF-ization task.
Link Discovery refers to linking tasks where source and target data are represented as KGs, when a commitment to a specific relation represented by the discovered links is not specified.
Classification Matching is a particular case where the KGs considered are classifications (e.g.
In general, the Link Discovery task must perform using two generic KGs, one as input and one as target.
The blu arcs indicate properties declared in the target KG ontology.
KG1, KG2 and KG3 use properties of the target ontology at different level of usage (total, partial and absent, respectively), showing that the Link Discovery Service must accept as input KGs based on different ontologies.
There exist several state-of-the-art tools that accomplish this task; the most used tools are LIMES and Silk.
LIMES is a link discovery framework for the Linked Data, that uses also time-efficient and dimension-scalable approaches.
Silk is an open source framework for integrating heterogeneous data sources, focused also on generating links between related data items within different Linked Data sources.
Both require to set configuration parameters, such as the properties to be discovered, the similarity metrics to be used, and so on.
In euBusinessGraph UNIMIB will provide a Link Discovery service that gets a generic KGs as input and produces a set of mappings between entities represented also in the reference KG.
The result of this task is a set of mappings between entities represented in both the KGs considered.
The Data Hosting Services will provide a reliable hosting service for the business graph.
It will be based on the Ontotext S4 service suite7, which provides a semantic graph (triplestore) database-as-a-service that will be into and deployed as part of the DataGraft platform.
The hosting task covers improvements of the scalability, performance and reliability of a large number of semantic graph databases (triplestores) running in the Cloud, so that large volumes of data can be managed and simultaneous queries and data access requests can be supported.
The hosted datasets are accessible to third-party applications via various standard data access mechanisms: SPARQL query and Linked Data endpoints, as well as various RESTful APIs.
Data may also be uploaded into the platform via standard read/write APIs for managing RDF data, such as the RDF4J API8.
An RDF store that contains any RDF-ised datasets and provides SPARQL query access to the data.
A data exporting service, to access the dataset in original format.
The Cross-Cutting Business Cases Analytics Services provide a set of analytics services on top of the business graph data.
These services are meant to be generic and customisable, and so will be reused in more than one business case.
These services aim to simplify analytics tasks in the business cases, and will include deployment of machine learning techniques, statistical analysis and pattern detection on graph data, as well as information extraction and natural language processing techniques on unstructured data (news content).
The Cross-Cutting Business Cases Analytics Services will be based on JSI's Event Registry10, JSI's Wikifier11, which will both be extended with new functionality during this project, and newly developed service for relation extraction.
When the components of the business graph will be more precisely defined, an additional service for analytics on the graph will be added.
All data needed to run these services will reside locally, especially for the graph, because of specific requirements of used algorithms.
All these services will be loosely coupled and available via API.
When we will generate some new data (like new nodes in the graph), we could push the new data back to the main graph, which would need to have an API for that.
The Marketplace and Operational Services will ensure the availability of a data brokerage system in the form of a data marketplace where data that are part of the business graph can be provisioned and accessed.
The focus here will be on the implementation of a mechanism for controlled access to business graph data, together with services for user management and data access mechanisms.
In addition, the operational services needed for the marketplace will be addressed.
Components for platform monitoring, availability, administration quota enforcement, branding and billing will be created.
The underlying infrastructure of the marketplace will be based on DataGraft.
As stated in Section 3.3.2 we are exploring support for different models of sharing data.
Current we are implementing a minimum viable product (MVP) supporting discovery and search of data made available via the euBusinessGraph Marketplace platform.
The current version of the MVP provides an initial feature for a federated search and a feature to view and compare company data provided by different data providers.
The license models sub-system is responsible for managing and enforcing data access based on license models and agreed agreements.
Payment models restricting access to data based on payments; and Dual license models, allowing payment plus share-alike for public-benefit use.
According to the requirements from the data providers, license models for full datasets are not sufficient, as there is a need to have different licenses models at the property level in the datasets.
The security and access control sub-system is responsible for ensuring the proper authorization and authentication for accessing all public service on the platform.
As an additional security measure, all access to the platform services will utilize transport encryption (SSL/TLS).
Authentication and authorisation – all access to the platform marketplace portal will be authenticated via a user name and password, while all access to the REST services exposed on the platform will be authenticated via private API key/secret pairs.
Furthermore, datasets will offer different access levels: public, private, restricted.
Appropriate authorisation measures will be taken to ensure that users have access only to appropriate data.
Billing – this component is responsible for aggregating all the usage of a particular account on a monthly basis.
In that way, based on predefined billing metrics such as volume of data accessed, number of queries, or number of datasets accessed, a usage cost can be calculated for the account and packaged as a monthly bill.
This section describes the API specifications for the data preparation, data interlinking, data hosting, cross-cutting business cases analytics, and marketplace and operational services.
The APIs, names and parameters are provisional and will evolve throughout the development of the euBusinessGraph platform.
Inputs and outputs will be further refined and concretised as the platform API evolves.
The euBusinessGraph Marketplace platform will be implemented with the capability of generating, publishing and invoking executable transformation services.
The API specification for the data workflows and publishing defines methods for transformation resources.
Transformation services themselves will be capable of periodically triggering remote downloads, thus satisfying the requirement for continuous updates of the dataset.
GET retrieves a particular transformation description.
Performs a transformation to RDF and loads the result in certain repository.
The API specification for the data interlinking services defines methods for table, KG and document resources.
We suppose that the table to be annotated is already available and accessible through a known identifier (ID) (if not, we can also provide an extra method to upload the table that returns as output the ID).
The API specification for the data hosting platform services defines methods for dataset and repository resources.
We focus on the DCAT vocabulary APIs and the way that SPARQL endpoints are managed.
Input: A dataset identifier taken from the catalogue.
Input: 'dataset-id' – URI of dataset to be removed.
A distribution may contain the raw dataset uploaded to the platform or a transformed dataset available on the platform.
The API is format-independent, meaning that it resolves the underlying file/database format based on the distribution data/metadata.
Uses a multipart HTTP request with form parameters for the input.
Uses a multipart HTTP request with form parameters for the input.
Some of these APIs will require input and all will provide a response with results.
JSI's semantic multilingual annotation service will be based on JSI Wikifier.
JSI Wikifier is a web service which takes a text document as input and annotates it with links to relevant Wikipedia concepts.
Each request to the API must be accompanied with an API (user) key which you can obtain for free by registering on the Web site.
Parameters specified would typically include the text of the document that you want to annotate and language of the document.
There exists a list of all the languages currently supported by the JSI Wikifier.
One could also use the language parameter auto to let the system auto detect the language.
Other parameters, if specified, would influence the amount and type of information returned by the JSI Wikifier.
The spaces and words arrays show how the input document has been split into words.
Event Registry has a Python package which can be used to easily access the data available in the Event Registry through the provided API.
Each request to the API must be accompanied with an API key which you can obtain for free by registering on the Web site (http://eventregistry.org/register).
For searching for events that match various search criteria, such as relevant concepts, keywords, date, location or others.
For searching for articles based on the publisher's URL, article date, mentioned concepts or others.
For finding concepts which are currently trending the most in the news.
For finding the list of articles that have been shared the most on Facebook and Twitter on a particular date, or the most relevant event based on shares on social media.
For finding how often a particular concept or category was mentioned in the news in the previous years, or the sentiment expressed on social media about some person or event.
The API described below is based on Python package, which is easy to use.
Python package itself is using a raw HTTP request protocol via JSON input formatted data, which is varying regarding the requested functionality and needed input parameters.
For searching for events two python classes are available: QueryEvents and QueryEventsIter.
QueryEvents class returns a list of events with complete information about the matching events in various forms, possibly including a timeline distribution of the matching events over time, distribution of matching events into predefined categories, list of top concepts in the matching events, etc.
QueryEventsIter class offers an iterator, with which you can easily iterate over all events that match the specified conditions.
The information returned is similar to the QueryEvents class.
More details about these two classes are available on Searching for events.
The returned information about events is a JSON formatted data with elements from the Event data model, which is explained in detail on Event data model.
For search for articles two python classes are available: QueryArticles and QueryArticlesIter.
QueryArticles class returns a list of articles with complete information about the matching articles in various forms, possibly including a time distribution when articles were published, distribution of top news sources that wrote the matching articles, top concepts mentioned in the articles, etc.
QueryArticlesIter class offers an iterator, with which you can easily iterate over all articles that match the specified conditions.
The information returned is similar to the QueryArticles class.
More details about these two classes are available on Searching for articles.
The returned information about articles is a JSON formatted data with elements from the Article data model, which is explained in detail on Article data model.
A concept in Event Registry's terminology is an annotation which can be assigned to an article or event.
Concepts can represent entities (people, locations, companies) or non-entities/keywords (things like phone, computer, cars, …).
A concept is associated with the article if it's mentioned in it or with event if it appears in the containing articles.
The concepts' trends in Event Registry are computed by comparing how many times a particular concept appears in the articles in the last two days compared to the last two weeks.
For getting a list of currently top trending concepts a python class GetTrendingConcepts is available.
The returned information about top concepts is a JSON formatted data with elements from the Concept data model, which is explained in detail on Concept data model.
For every article or event stored in Event Registry a lot of additional information (metadata) is stored.
Among them is also a number how many times an individual article or event is being shared on social networks, such as Facebook or Twitter.
For getting this information you would use python class QueryArticles, setting the parameter socialScore in ArticleInfoFlags to True.
The list of the resulting articles will then contain information about the number of shares of the article.
More details about this are available on Social shares.
Based on the shares of the articles in social media Event Registry also computes a social score for events.
A social score of an event is computed by selecting all articles assigned to the event and sorting them by decreasing social score.
Maximum top 30 articles are then selected and an average social score of them is computed and used as the event's social score.
For getting the information how often a particular concept is mentioned in the news or social media on a particular date you can use the python class GetCounts.
GetCounts class returns a JSON formatted data with a list of dates and number of mentions of matching concept on that date.
More details about this are available on Number of mentions in news or social media.
For any meaningful data analytics on the constructed graph the basic agreed data from the data providers should be made available to store offline.
The input to the service would be tag words describing tender.
The service would then find all companies, who are dealing with the similar products/services as the specified tag words and return a list of such companies along with the basic data and pointers to the providers of the data.
The input to the service would be the description of the product/service.
The service would return all companies dealing with or related to the specified product/service along with their basic data.
The service would return all companies with similar products/services grouped in clusters along with their basic data.
The input would be the person's name and the output would be the relations of this person to the companies (i.e. "Person A is a CEO of company B").
The input would be the company's name and the output would be a list of companies connected in any way to a specified company along with their basic data.
The exact list of implemented functionality will be finalized as soon as all business case requirements are known and defined.
The details of the API like the REST url, the input and the output format will be finalized in the next version of this document.
The relation extraction service will operate on local data separately of other components and will find out the relations between entities (companies, people and products/brands).
It will generate additional information for the graph which could be pushed back to the main graph, which would need to have an API for that.
This additional information will then serve as input in some of the graph based analytics.
The API will be defined in the next version of this document.
The API specification for security and access control defines methods for account and API key resources.
The API for usage reporting provides a method that provides usage reports of the platform resources euBusinessGraph platform.
This document provides an overview of the euBusinessGraph Marketplace platform, introducing the relevant stakeholders of the platform, and outlining a set of requirements for the platform from the perspectives of the stakeholders.
Furthermore, the document provides an initial architecture design for the platform, together with a preliminary set of APIs for the components of the architecture.
The architecture, components, and APIs will evolve during the course of the project, as the business cases will become more mature.
The requirements, architecture, and APIs outlined in the document will serve as input for the implementation of the platform in the next phase.
It should be d that the technical results in the project should not be to solve the individual business cases, but focus on making the business graph and its services as smart and useful as possible to the business cases.
The different business cases are different in nature, but should represent a diverse set of needs and requirements that can be used as input for the technical solutions to be developed in the project.
In euBusinessGraph, we are developing a Minimum Viable Product (MVP) to provide end users, via a graphical user interface, core features supported by the euBusinessGraph Marketplace platform outlined in this deliverable.
The MVP is a web-based application developed in Ruby on Rails14.
The current version of the MVP provides an initial feature for a federated search and a feature to view and compare company data provided by different data providers.
In the following, we first provide an overview of the current version of the MVP, and then we explain how we plan to improve the current version in order to facilitate the envisioned features of the euBusinessGraph Marketplace platform.
However, the latter is explained at a high level of abstraction and does not cover detailed technical aspects.
This is because the technical details of the MVP are evolving during the course of the project, in line with the architecture, components, and the APIs of the euBusinessGraph Marketplace platform.
As mentioned above, the current MVP version supports two main features: federated search and view/compare information about specific companies.
The search feature also has filtering capabilities, but is currently filtering only on selected countries (Norway, Italy, Great Britain).
In the example in Figure 10, we have restricted the search to Great Britain.
These input values are passed to a class (Search) which assembles an API search query based on the Company name and the Country filter, and then executes the search by passing the search query to the data providers.
Notice that, currently, the MVP executes queries only on the Open Corporates API and the Atoka API.
Having executed the search queries, the data providers respond with a JSON object containing the search result.
The Search class captures and merges the responses into a single search result.
For example, for search result "C. LIPTON LTD.", we see that both Open Corporates and Atoka may provide information about the company.
In order to retrieve information about a specific company, the user has to click on a company name in the search result.
Assume we click on the company "C. LIPTON LTD." in Figure 10.
It is beyond this deliverable to go into the details of the company information given in Figure represented in Figure 12, and then we explain the corresponding conceptual model for retrieving company info (Figure 13).
The first column in Figure 12 lists the properties of a company.
A property is a type of information about a company.
For example, the legal name of the company, the public identifier of the company, etc.
This is a preliminary list of properties (at the time of writing) and will be updated as soon as the euBusinessGraph ontology model is defined.
The second column shows available data for each property.
The idea is that the information shown in the second column is retrieved from a "base data provider" (which may be user defined), and used to compare against information retrieved from all other data providers about the same company.
In this example, the "base data provider" is Open Corporates.
That is, the name "C. LIPTON LTD." in the second column is retrieved from Open Corporates.
This is compared to information retrieved from "all other" data providers, which in this case is only Spazio Dati (Atoka).
The row Legal Name is highlighted green because the legal name for company "C. LIPTON LTD." is identical in all data providers (Open Corporates and Spazio Dati).
Whenever a row is highlighted yellow, the corresponding information is either missing, or it is different from the information retrieved from other data providers.
For example, we see that the row Startup is highlighted yellow including the text "Information not available".
This means that Open Corporates do not have this information.
The user may then click on the "drop-down" button on the third column to see whether other data providers provide this information.
We see from Figure 12 that Atoka provides this information and the information is "false" meaning that "C. LIPTON LTD." is not a startup company.
As mentioned initially, a user has to click the name of a company to retrieve information about that company.
Each hyperlink in Figure 10 contains the unique company ID as defined in the databases of the data providers (Open Corporates and Spazio Dati).
These unique IDs are then passed to the Search class that assembles and executed an API query.
The data providers respond with a JSON object containing information about the specific company.
The Search class retrieves the JSON object from each data provider and stores the JSON object retrieve from Open Corporates as the "base" information.
Then, for each property in the base information, the Search class checks for whether the information exists, is equal to, or is different from the corresponding information retrieved from all other data providers.
Finally, the information is displayed to the user as represented in Figure 12.
Notice that the choice of using Open Corporates is not intentional, but only used as an initial "base" as part of the development of the MVP.
As pointed out above, the technical details of the MVP are evolving during the course of the project, in line with the architecture, components, and the APIs of the euBusinessGraph Marketplace platform.
It is therefore not possible to foresee all possible improvements of the MVP at the time of writing.
However, in the following, we list the improvements we plan to implement.
With respect to the federated search, the current version of the MVP is executing search on different APIs provided by different data providers.
As mentioned in Section 3.3.2, for the first release of the euBusinessGraph platform, we are therefore implementing a common data model that all data providers can use to share a common data subset.
For data that data providers are unwilling to share, we may use pointers to their respective source.
With respect to viewing company information, the current version of the MVP is limited to show whether the underlying information about a company exists, is equal to, or is different from the information provided by other data providers.
We plan to extend this feature to support other useful data analytics.
SO 1: Study the formation and evolution of compact binary stars in the Milky Way Galaxy.
SI 1.1: Elucidate the formation and evolution of Galactic Binaries by measuring their .
SI 1.2: Enable joint gravitational and electromagnetic observations of GBs to study the interplay between gravitational radiation and tidal dissipation in interacting stellar systems.
SI 2.4: Test the existence of Intermediate Mass Black Hole Binaries (IMBHBs) .
SI 2.1: Search for seed black holes at cosmic dawn .
SI 2.2: Study the growth mechanism of MBHs before the epoch of reionization .
SI 2.3: Observation of EM counterparts to unveil the astrophysical environment around .
SI 4.1: Study the close environment of SOBHs by enabling multi-band and multi- .
SI 3.1: Study the immediate environment of Milky Way like MBHs at low redshift .
SO 3: Probe the dynamics of dense nuclear clusters using EMRIs .
SO 4: Understand the astrophysics of stellar origin black holes .
SI 4.2: Disentangle SOBH binary formation channels .
SO 5: Explore the fundamental nature of gravity and black holes .
SI 5.1: Use ring-down characteristics observed in MBHB coalescences to test whether .
SI 5.2: Use EMRIs to explore the multipolar structure of MBHs .
SI 5.3: Testing for the presence of beyond-GR emission channels .
SI 5.5: Test the presence of massive fields around massive black holes with masses .
SO 6: Probe the rate of expansion of the Universe .
SI 6.1: Measure the dimensionless Hubble parameter by means of GW observations only 39 SI 6.2: Constrain cosmological parameters through joint GW and EM observations .
SO 7: Understand stochastic GW backgrounds and their implications for the early Universe .
SI 7.1: Characterise the astrophysical stochastic GW background .
SI 7.2: Measure, or set upper limits on, the spectral shape of the cosmological stochastic .
SI 8.1: Search for cusps and kinks of cosmic strings .
SO 8: Search for GW bursts and unforeseen sources .
Mission constraints on the sky, inclination and polarisation-averaged strain sensitivity .
A plot of accumulated SNR versus frequency for the reference source.
Instantaneous frequency of the reference source as a function of time to merger.
A plot of accumulated SNR versus time to merger for the reference source.
Instantaneous frequency of the reference source as a function of time to merger.
Instantaneous frequency of the reference source as a function of time to merger.
Instantaneous frequency of the reference source as a function of time to merger.
Instantaneous frequency of the reference source as a function of time to merger.
Instantaneous frequency of the reference source as a function of time to merger.
Frequency domain waveform of for the reference source.
Instantaneous frequency of the reference source as a function of time to merger.
Instantaneous frequency of the reference source as a function of time to merger.
Overview of science objectives and their respective science investigations .
Einstein’s theory of spacetime and gravity, General Relativity, predicts that suitably accelerated masses produce propagating vibrations that travel through spacetime at the speed of light.
These gravitational waves are produced abundantly in the universe and permeate all of space.
Measuring them will add an altogether new way to do astronomy, conveying rich new information about the behaviour, structure, and history of the physical universe, and about physics itself.
LISA is a space mission designed to measure gravitational radiation over a broad band at low frequencies, from about 0.1 mHz to 0.1 Hz, a band where the universe is richly populated by strong sources of gravitational waves (Danzmann and LISA Consortium, 2013).
It will measure signals from a wide range of different sources that are of strong interest to the astrophysics of black hole and galaxy formation, to tests of general relativity and to cosmology: massive black holes mergers at all redshifts; extreme mass ratio inspirals; the inspiral of stellar-origin black hole binaries; known binary compact stars and stellar remnants; and probably other sources, possibly including relics of the extremely early Universe, which are as yet unknown.
A major objective of the mission is to determine how and when the massive black holes, present in most galactic nuclei today, have formed and grown over cosmic time.
It will explore almost all the mass-redshift parameter space relevant for reconstructing their evolution.
The gravitational wave signal from coalescing black holes reveals their spin and redshifted mass, and the distribution of masses and spins will be studied to differentiate between different formation scenarios.
The mission will also study in detail the signals from thousands of stellar-mass close binaries in the Galaxy and give information on the extreme endpoints of stellar evolution.
It will provide distances and detailed orbital and mass parameters for hundreds of the most compact binaries, a rich trove of information for detailed mapping and reconstruction of the history of stars in our Galaxy, and a source of information about tidal and non-gravitational influences on orbits associated with the internal physics of the compact remnants themselves.
By observing highly relativistic black hole-black hole coalescences, LISA will provide exceptionally strong tests of the predictions of General Relativity.
The signal of merging binary black holes, where maximally warped vacuum spacetimes travel at near the speed of light interacting strongly with each other, allow the study of the full nonlinear dynamics of the theory of gravity.
By observing the signal of stellar black holes skimming the horizon of a large massive black hole at the centre of a galaxy, LISA will measure the mass, spin and quadrupole moment of the central object testing its level of Kerrness; thus testing for the first time the black hole hypothesis, and the no-hair conjecture.
Finally, a space-based gravitational wave detector will probe new physics and cosmology, and will search for unforeseen sources of gravitational waves.
The LISA frequency band in the relativistic early Universe corresponds to horizon scales where phase transitions of new forces of nature or extra dimensions of space may have caused catastrophic, explosive bubble growth and gravitational wave production.
The Gravitational Universe is addressed here in terms of science objectives (SOs) that are listed in table 1 and detailed in appendix A.
Each SO contains a number of science investigations (SIs) that need to be conducted to fulfill the respective SO.
Each SI comes with operational requirements (ORs) that set the required ability of the observatory, part of which can be expressed in MRs that can be stated in form of strain sensitivities.
The stated strain sensitivities always assume no further knowledge on the sky position and polarization of the respective source and must therefore be considered as a sky-average and polarization average.
The envelope of the various MRs sets a strain sensitivity requirement for the observatory and is depicted in figure 1 as the dashed line and given quantitatively in equation (3).
The observatory strain sensitivity together with other requirements derived from the ORs are summarised in the mission performance requirements (MPRs).
The GW strain signalℎ(𝑡), also called the waveform, together with its frequency domain representation ̃ℎ(𝑓), encodes information about intrinsic parameters of the source in the source frame (e. g., the red-shifted mass(1+𝑧)𝑀 and spin𝜒 of the interacting bodies) and extrinsic parameters, such as source inclination𝚤, luminosity distance𝐷𝐿 and sky location(𝜃,𝜙).
The power spectral density𝑆ℎ(𝑓) of the signalℎ(𝑡) is estimated by the square of measurement bandwidth,𝑓MBW.
Signals are computed according to General Relativity, with redshifts estimated using the cosmological model and parameters inferred from the Planck satellite results (Planck Collaboration et al., 2016).
For each class of sources, synthetic models driven by current astrophysical knowledge are used in order to describe their demography.
Foregrounds from astrophysical sources as well as backgrounds of cosmological origin are also considered.
The performance of the observatory is defined by the MR associated with each OR (see Appendix sA).
However, performance goals are set which significantly enhance the science return of the mission.
These goals shall not drive the mission design, nor are required to be verified during ground testing.
Nonetheless, it is required to assess the impact (through analysis) of design and implementation choices as to not knowingly render the observatory incapable of reaching the science goals.
The assessment of the operational requirements requires a calculation of the expected accuracy of the estimation of the parameters of the GW and its respective source.
Though the Signal-to-Noise Ratio (SNR) is not the only determining factor, it can be used as a good proxy for the expected parameter estimation accuracy.
The minimum SNR above which the required parameter accuracy for a given source can be reached translates into the respective measurement requirements for the observatory.
Requiring the capability to measure key parameters to some minimum accuracy sets measurement requirements that are generally more stringent than those for just detection.
The detailed derivation of the measurement requirements from the operational requirements is given in Appendix A.
The solid red line constitutes the resulting requirement on the strain sensitivity of the observatory over the measurement bandwidth, taking into account all the MRs defined in the sections above.
Whereas the dashed green line indicates the strain sensitivity goal (2.1.3).
The green dotted lines above 0.1 Hz and below 0.1 mHz indicate mission goals.
The grey dashed line indicates the envelope of the sensitivity at high frequency due to nulls in the observatory response arising from the relationship between armlength and gravitational wavelngth.
The requirements above 25 mHz can only be exceeded to accommodate the partial cancellation of the effect of the GW on the instrument at higher frequencies arising from the length of the interferometer arms, and the subsequent nulls in the observatory response.
This also assumes no particular source, in other words, it includes the effects of sky, inclination and polarisation averaging.
The observatory shall to be able to measure both polarisations of a GW simultaneously, i.e., without relying on observatory motion with respect to the source to break the degeneracy.
Observing both polarisations allows some degeneracies in the waveform parameters to be broken leading to significant improvement in parameter estimation and allows an additional degree of freedom of the source to be measured.
The observatory shall provide at least two quasi-independent data streams containing the science observables.
This allows instrumental effects (such as force glitches on a single test mass) to be distinguished from unmodelled astrophysical sources.
The mission shall provide a null stream to aid in assessing instrumental noise and artefacts.
The null stream should be ‘perfect’ for a given source location, but may be frequency dependent for the general case.
The mission lifetime shall accomodate a 4 year science measurement phase.
The goal is to make an extension to 10 years posssible.
Out of the 4 years of science measurement, the duty cycle of usable science data at full (nominal) performance shall be greater than 75%.
All science data streams discussed below shall be properly acquired employing suitable anti-alias filtering and appropriate resolution of acquisition to ensure the measurements are not limited by (TBD) quantisation noise or aliasing effects in the measurement band.
The data streams needed to produce the science observables at the required sensitivity shall be made available for all times when in science observing mode, with the required data rate and at the required resolution.
The data rate shall be sufficient to allow for the sensitivity to be achieved over the measure- ment band-width of 100 mHz with a goal of 1 Hz.
An additional set of data streams needed to monitor physical environment of the observatory to allow for characterisation of the science observables shall be made available at the appropriate (TBD) data rates and resolutions.
We refer to such data streams as ‘auxilliary science data streams’.
Monitoring of critical instrument parameters shall be done via the appropriate set of housekeeping data streams.
Science data shall be telemetered to ground daily to allow for prompt analysis and monitoring of the observatory performance.
Data shall be available from the ESA LISA data archive within TBD hours of reception on ground.
The quality of the data may be reduced due to the various reasons detailed below.
This will impact the detectability of sources as well as the parameter estimation.
The data still can be analyzed in a fully coherent way, but since matched filtering is usually done in the frequency domain, it requires windowing of the data on both sides of every gap.
The windowing should be taken into account when computing the duty cycle.
The length of the windows is of order 1 day (TBD), independent of the gap length.
The long duration signals are mainly affected through the reduction of SNR (duty cycle), while short signals (< half a year), e.g., Massive Black Hole Binaries (MBHBs), are very sensitive to when gaps happen with respect to the merger.
The short duration signal will be affected not only due to reduction of SNR but also in degradation of the parameter estimation.
The biggest effect of the gaps is on the early warnings for the coalescing MBHBs.
Long gaps (duration of order a week) will significantly affect the parameter estimation within the last month before the merger of MBBHs (TBC).
Data gaps are anticipated either due to planned interruptions or anomalies.
Unplanned gaps obviously depend upon probability, reliability of subsystems, etc, but the effect of gaps has a significant impact on science output, and therefore such gaps should be minimised through system reliability, and also by optimising anomaly recovery time.
Two types of non-stationarity of the overall observatory performance needs to be considered: fluctua- tions in the PSD of observatory sensitivity arising from, e.g., orbital dynamics, temperature variations, and fluctuating magnetic fields; and periods of excess noise due to maintenance or calibration activi- ties.
For fluctuations in the PSD, the system shall be designed to avoid any deliberate sources of disturbance that can lead to non-stationarity in the observatory performance at a level of 10% in power (TBD) of the averaged PSD estimated over month time-scales.
The system shall be designed to minimise periods of excess noise arising from deliberate activities, for example, antenna repointing.
Glitches refer to transient, sporadic events in the system which can lead to transient signals in the science observables.
Therefore, the observatory shall be designed to minimise the possibility of such transients.
Instrumental sources of quasi-monochromatic signals in the measurement band should be avoided as these are potentially indistinguishable from sources of monochromatic gravitational waves, especially if they have any measurable frequency derivative.
Constellation science data shall be properly and consistently recorded and telemetered to ground to ensure that science analysis is not affected by missing samples.
In addition, proper time-stamping of all science data shall be made at the level of 10 ns (TBD) to allow for accurate, coherent combining of data streams on ground.
This implies, for example, that a single clock should be used on each satellite to ensure uniform sampling across different units, and that a system is in place to allow for comparison of clocks on each satellite at the required level.
It shall be possible to reschedule any planned interruptions of science data taking to allow for a specified protected period of up to 14 days starting no sooner than 2 days following the request.
For such protected periods, all science data shall be available for analysis at the DPC not later than 24 hours after the measurement occured.
The science case of LISA is addressed here in terms of Science Objectives (SOs) and Science Investiga- tions (SIs), and the Observational Requirements (ORs) necessary to reach those objectives.
The ORs are in turn related to Mission Requirements (MRs) for the noise performance, mission duration, etc.
The assessment of ORs requires a calculation of the Signal-to-Noise-Ratio (SNR) and the parameter measurement accuracy.
The MR requirement has been translated into a gain,𝛼, and spectral index,𝛽 as defined in Section B.
The mass ratio between the two bodies of the binary system,𝑞=𝑚1/𝑚2 interval of widthΔ𝑓 =𝑓, centered at frequency𝑓.
SO 1: Study the formation and evolution of compact binary stars in the Milky Way Galaxy.
Numerous compact binaries in the Milky Way galaxy emit continuous and nearly monochromatic GW signals in the source frame (Nelemans et al., 2001).
These GBs comprise primarily white dwarfs but also neutron stars and stellar-origin black holes in various combinations.
Higher frequency systems are typically louder and better characterized than low frequency systems.
At low frequencies, GBs are thought to be so numerous that individual detections are limited by confusion with other binaries yielding a stochastic foreground or confusion signal.
Several verification binaries are currently known for which joint gravitational and EM observations can be done and many more will be discovered in the coming years, e.g. by Gaia and the Large Synoptic Survey Telescope (LSST).
SI 1.1: Elucidate the formation and evolution of Galactic Binaries by measuring their period, spatial and mass distributions.
To detect the low frequency galactic confusion noise in the frequency band from 0.5 mHz to 3 mHz.
In duration, and after subtraction of individual sources.
The ORs pose requirements in the band from about 0.5 mHz to 30 mHz.
The identification of the low frequency galactic confusion signal requires the ability to subtract all the identified, known sources with a certain precision that is limited by the other unknown sources as well as the detector sensitivity.
In order for the detector sensitivity not to limit this significantly, we require the detector noise level below 2 mHz to be at, or below, the combined signal from galactic binaries.
SI 1.2: Enable joint gravitational and electromagnetic observations of GBs to study the interplay between gravitational radiation and tidal dissipation in interacting stellar systems.
To enable identification of possible electromagnetic counterparts, determine the sky location of about To study the interplay between gravitational damping, tidal heating, and to perform tests of GR, localise about 100 systems within one square degree and determine their first period derivative to a fractional accuracy of 10 % or better.
ORs 1.1.a, 1.1.b, and 1.2.b and 1.2.c set requirements on the science observation time in order to achieve the desired measurement precision.
These requirements may not be fully met for a science observation time less than four years.
Current studies predict masses for their seeds in the interval between about 103M⊙, and a few 105M⊙ and formation redshifts10≲𝑧≲15 (Volonteri, 2010).
Mergers and accretion influence their spins in different ways thus informing us about their way of growing.
The GW signal is transient, lasting from months to days down to hours.
Being sources at cosmological redshifts, masses in the observer frame are(1+𝑧) heavier than in the source frame, and source redshifts are inferred from the luminosity distance𝐷𝑙, extracted 𝑧 from an identified electromagnetic counterpart).
The definition of the threshold systems (which are shown as red stars in figure 2) for each OR leads to one or more MR, shown in figure 1.
Have the capability to detect the inspiral of MBHBs in the interval between a few 103M⊙ and a few 105M⊙ in the source frame, and formation redshifts between𝑧 = 10 and𝑧 = 15.
Such a “threshold” system would have a mass of 4000M⊙, mass ratio𝑞=0.2, and be located at a redshift of𝑧 = 15.
The waveform for this “threshold” system is shown in figure 3.
This system enters the band of the observatory (i.e. starts accumulating significant SNR) at around accumulated SNR as a function of frequency for this system is shown in figure 4.
Under the assumed characteristic strain, most of the SNR is accumulated from about 2 mHz up to 10 mHz; as the total SNR is relatively low, accumulation starts as soon as the signal enters the band.
The requirement on the PSD of the observatory strain sensitivity is set in a band from 3 mHz to 10 mHz.
The resulting MR is shown in Figures 7 and 8 expressed in strain sensitivity and characteristic strain, respectively.
Have the capability to detect the signal for coalescing MBHBs with mass104M⊙<𝑀 <106M⊙ in the source frame at𝑧≲ 9.
For sources at𝑧<3 and105M⊙<𝑀 <106M⊙, enable the measurement of the dimensionless spin accumulated SNR (up to the merger) of at least200.
All systems in OR 2.2.a and 2.2.b with higher mass, mass ratios, spins, or lower redshift will result in higher SNR, and better spin estimation.
The waveform for the threshold system is shown in figure 9.
This system enters the band of the detector at around 0.2 mHz and leaves the band around 50 mHz.
The accumulated SNR as a function of frequency for this system is shown in figure 10.
From that we see that most of the SNR is accumulated between 0.6 mHz and 9 mHz, so we set a requirement on the power spectral density (PSD) of the observatory strain sensitivity from 1 mHz to 8 mHz.
The resulting MR is shown in Figures 13 and 14 expressed in strain sensitivity and characteristic strain, respectively.
This would yield coincident EM/GW observations of the systems involved.
After gravitationally observing the merger of systems discussed in OR 2.3.a, the sky localisation will be significantly improved, allowing follow-up EM observations to take place.
This has the potential to enable the observation of the formation of a quasar following a MBHB merger.
This needs excellent sky localisation (about 1 deg2) to distinguish from other variable EM sources in the field months to years after the merger.
To achieve this operationally, data from the observatory needs to be made available for analysis, around one day after measurement on-board.
Additionally, in order to ensure coincident observations of GW and EM, we need to trigger a “protected period” on-board during which no commissioning activities should take place.
Hence there are three MRs here: a constraint on the strain sensitivity; a constraint on the cadence with which data are downloaded from the satellites; and the ability to trigger “protected periods” where the instrument configuration is maintained.
For all other systems in OR 2.3.a with lower redshift, the SNR will be higher, and the sky-localisation correspondingly better.
In selecting a threshold binary here, we need to consider the time at which we will detect the signal, and if that is enough time to trigger an alert for prompt EM observations.
For the lowest SNR system in OR2.3.a, which corresponds to a mass of 106M⊙ at𝑧 = 2, we will detect the inspiral signal (withSNR=10) about 11.5 days prior to merger (see figure 18).
This is compatible with the data download cadence being considered, and a little margin for processing time given the chosen observatory strain requirement.
The threshold system enters the band of the detector at around 80 μHz and merges in the band.
The accumulated SNR as a function of frequency for this system is shown in figure 16.
The SNR is accumulated over the whole band, so the frequency band for the requirement is chosen form the lower end of the band to approximately the onset of the galactic foreground signal.
The resulting MR is shown in figure 19 and figure 20, expressed in strain sensitivity and characteristic strain, respectively.
SNR of 22 under the strain requirements (equation (3)).
It enters the detection band around 2 mHz and leaves the band long before merger at around 40 mHz.
Systems of OR 2.4.b set constraints on the strain sensitivity of the observatory along the descending branch of the U-shaped curve where the galactic confusion noise-like signal dominates.
This requirement holds as long as the galactic confusion noise-like signal is at the level shown in figure 1.
The threshold system for the ranges above is chosen with a mass ratio of𝑞=103, a total intrinsic mass the observatory to be below 5× 1020 at 0.3 mHz, falling to 1.2× 1021 at 3 mHz.
This corresponds to saying that we require the instrument sensitivity to be below the galactic confusion noise over this frequency band.
The large number of orbital cycles allows ultra precise measurements of the parameters of the binary system as the GW signal encodes information about the spacetime of the central massive object.
Considering the large uncertainty in the astrophysics of EMRIs, fulfillment of the requirements of this section would yield a minimum rate of one observed system per year, according to current most conservative EMRI population models.
Have the ability to detect EMRIs around MBHs with masses of a few times 105M⊙ out to redshift 𝑧=3 (for maximally spinning MBHs, and EMRIs on prograde orbits) with the SNR larger than 20.
This enables an estimate of the redshifted, observer frame masses with the accuracy𝛿𝑀/𝑀 <104 for the MBH and𝛿𝑚/𝑚<103 for the SOBH.
All other systems with either lower redshift, higher component mass, or higher spin will produce a higher SNR.
The plunge time will be known to high accuracy several months ahead.
We require the capability of triggering a protected period of about one week around the plunge time to allow testing the accumulation of SNR against GR.
Based on the inferred rates from the LIGO detections, fulfillment of the requirements of this section would allow LISA to individually resolve a minimum number of about multi-band GW astronomy (Sesana, 2016).
This will allow the triggering of alerts to ground-based detectors and to pre-point EM probes at the SOBH coalescence.
SO 5: Explore the fundamental nature of gravity and black holes MBHBs and EMRIs enable us to perform tests of GR in the strong field regime and dynamical binaries, that is, MBHBs with very high (larger than100) SNR in the post-merger phase or EMRIs sector (Barack and Cutler, 2007; Berti et al., 2006).
Precision tests such as these require “Golden” withSNR>50.
SI 5.1: Use ring-down characteristics observed in MBHB coalescences to test whether the post-merger objects are the black holes predicted by GR.
The range of systems defined in OR 5.1 sets a constraint on the sensitivity curve by requiring the high SNR and the observation of the merger.
The range of systems defined in OR5.1 sets a constraint on the sensitivity curve by requiring the high SNR and the observation of the merger.
SNR of 100 in the baseline observatory configuration, about 70 % of which is in the ringdown (see figure 40).
The SNR accumulated in the ringdown occurs in the band from about 3 mHz to 9 mHz.
The system enters the band about 7 days prior to merger and merges at around 10 mHz (see figure 41).
So we set another threshold system of 107M⊙ at redshift𝑧=4, again with𝑞=0.2.
The MRs are the same as MR3.1, but due to uncertainties in the astrophysical populations, a mission lifetime of several years is essential here to increase the chance of observing a Golden EMRIs.
SI 5.3: Testing for the presence of beyond-GR emission channels Test the presence of beyond-GR emission channels (dipole radiation) to unprecedented accuracy by detecting GW150914-like binaries, which appear in both the LISA and LIGO frequency bands (Barausse et al., 2016).
Test propagation properties of GW signals from EMRIs and from coalescing MBHBs.
The ORs and MRs are the same as those in MR 2.2 and MR 3.1.
Constrain the masses of axion-like particles or other massive fields arising in Dark-Matter models by accurately measuring the masses and spins of MBHs (Arvanitaki and Dubovsky, 2011).
The requirements on the accuracy of the mass and spin measurements are the same as in SI 2.2.
Have the ability to observe SOBH binaries with total mass𝑀 >50M⊙ at𝑧<0.1 with SNR higher than 7 and typical sky location of better than1deg2.
In terms of sensitivity curve, the OR 6.1a-b are automatically met if MR3.1 and MR4.1 are fulfilled.
The need to collect a large enough sample of sources translates into a minimal mission duration requirement.
According to current best population estimates, a 4 year mission is needed to yield a measurement of the Hubble parameter to better than 0.02, which helps resolving the tension among the values of the Hubble parameter determined with local Universe standard candles and with the Cosmic Microwave Background.
In terms of the sensitivity curve, OR 6.2 is automatically met if the MRs related to SO2 are fulfilled.
The need to collect a large enough sample of sources translates into a minimal mission duration requirement.
SO 7: Understand stochastic GW backgrounds and their implications for the One of the LISA goals is the direct detection of a stochastic GW background of cosmological origin (such as the one produced by a first-order phase transition around the TeV scale) and stochastic foregrounds.
Probing a stochastic GW background of cosmological origin provides information on new physics in the early Universe.
The shape of the signal gives an indication of its origin, while an upper limit allows to constrain models of the early Universe and particle physics beyond the standard model.
For these investigations we need to ensure the availability of the data streams needed to form the Sagnac (or null-stream) TDI channel where the GW signal is partially suppressed in order to help separate the GW background from instrument noise.
This requires the frequency ranges0.8mHz<𝑓 <4mHz and4mHz<𝑓 <20mHz.
The SNR over the assumed 3 years of accumulated science observation must be larger than 10 TBD in the two frequency ranges.
SI 7.2: Measure, or set upper limits on, the spectral shape of the cosmological Probe a broken power-law stochastic background from the early Universe as predicted, for example, by first order phase transitions (Caprini, 2016) (other spectral shapes are expected, for example, for cosmic strings (Report from the eLISA Cosmology Working Group) and inflation (Bartolo, 2016)).
Therefore, we need the ability to measureΩ = 2.8×1011(𝑓/104Hz)1 in the frequency ranges ranges2mHz<𝑓 <15mHz and15mHz<𝑓 <0.1Hz.
Ensure an SNR higher than 10 (TBD) over the 3 years of accumulated observation time in the three frequency ranges specified in OR 7.2 .
Probing the gaussianity, the polarisation state, and/or the level of anisotropy of a potential stochastic background will give important information about the origin of the background.
In particular, limiting the number of instrumental glitches will help to assess the gaussianity.
The polarisation state shall be assessed with the three arm configuration.
The measurement of the level of anisotropy depends on the frequency range and the amplitude of the background.
LISA will lead us into uncharted territory, with the potential for many new discoveries.
Distinguishing unforeseen, unmodelled signals from possible instrumental artifacts will be one of the main challenges of the mission, and will be crucial in exploring new astrophysical systems or unexpected cosmological sources.
Searching for GW bursts from cusps and kinks of cosmic strings requires a deep understanding of the instrument noise and non-stationary behavior.
Using the known shape of the bursts in the time and frequency domains will help to distinguish them from the instrumental artifacts and fluctuations in the stationarity of the instrument noise floor.
Having the ability to use the Sagnac (or null-stream) TDI channels (MR 7.2) to veto such instrumental events will play a crucial role in the exploration of this discovery space.
Searching for GW bursts from completely unmodelled and unforeseen sources will also require a deep understanding of the instrument noise and non-stationary behavior.
To distinguish such signals from instrumental effects, it is essential that sources of instrumental non-stationary artifacts be kept as few as possible and that we maintain the ability to form the Sagnac combination (which is insensitive to GWs at low frequencies).
This requires that we maintain 6 laser links for the full duration of the mission (MR 7.2) and that we make available the necessary data streams to allow required computations on ground.
This will help to veto out all non-GW burst-like disturbances.
Given the nature of this Science Investigation, no MR is levied on the instrument.
The piece-wise defined strains are for information only and do not constitute a sensitivity require- ment.
D 83 (4), p. 044026. doi: 10.1103/PhysRevD.83.044026.
Theory-Agnostic Constraints on Black-Hole Dipole Radiation with Multiband Gravitational-Wave Astrophysics”.
D 73.6, 064030, p. 064030. doi: 10.1103/PhysRevD.73.064030.
II: Gravitational waves from cosmological phase transitions”.
D 93.2, 024003, p. 024003. doi: 10.1103/PhysRevD.93.024003.
The gravitational wave signal from the Galactic disk population of binaries containing two compact objects”.
The Runaway Growth of Intermediate-Mass Black Holes in Dense Star Clusters”.
In: Physical Review Letters 116.23, 231102, p. 231102. doi: 10.1103/PhysRevLett.116.231102.
III: Probing the expansion of the Universe using gravitational wave standard sirens”.
Mary Burmeister, Immunization Registry, Arkansas Department of Health Nathan Bunker, Dandelion Software & Research Inc.
Special thanks to the participants listed above, as well as to our partners from the American Immunization Registry Association (AIRA) and the IIS Support Branch of CDC.
Without the contributions of everyone involved, this project would not have attained its level of success.
Finally, we gratefully acknowledge the support of the Health Resources and Services Administration, Maternal and Child Bureau, and our Project Officer, Mary Kay Kenney.
The Public Health Informatics Institute (the Institute) wishes to acknowledge the invaluable sources of collective knowledge contained in the various guidance documents from the Modeling of Immunization Registry Operations Workgroup (MIROW), and from other past and present CDC and AIRA workgroups.
The maturity and sophistication of the products of these collaborative efforts provided critical input into this project.
The Institute welcomes comments on this document and related issues.
Please send all correspondence via email or phone using the contact information listed at the Public Health Informatics Institute web site (www.phii.org).
This document contains the products of three workgroup sessions to collaboratively and rigorously define a full range of important IIS functions, referred to here as business processes.
This detailed documentation for seventeen such business processes is intended to establish best practice for how an IIS should function in an increasingly e‐health world.
The IIS experts who participated in this effort represent local, state and federal level health agencies (see the preceding Acknowledgments page).
Examine and challenge your current workflows and “ways of doing business.” Identify enhancements for your IIS application to make it more functionally robust.
Develop a Request for Proposal to procure a new system.
Business Process Matrix: A table that outlines the components that describe a business process.
Logic Model, it captures elements like the goal, desired outcomes, inputs, outputs, and other factors that comprise a process.
Task Flow: A graphical model that illustrates the activities of a business process and the entity that performs the activities.
The task flow provides a “story” for the diagrammed process.
System Requirements: Statements that describe the functionality needed for an information system to support the business process.
Institute for this and other requirements gathering projects.
Appendix C: Recommended data elements to collect when enrolling a new provider site or organization.
Appendix D: EHR‐HIE‐IIS Data Exchange business process matrix and task flow.
This business process demonstrates what immunization data exchange might look like using a Health Information Exchange (HIE)/Health Information Organization (HIO) as an intermediary; a model in which the HIE/HIO does not transform the message but simply passes it through.
Appendix E: Example Business Process Matrix, Task Flow Diagram, and Requirements Document.
Appendix G: General System Assessment (attachment to the RFP).
Appendix H: Vendor Assessment (attachment to the RFP).
Appendix I: A glossary of key IIS terms used in this document.
Appendix J: Explanation of terminology used in this document.
You are invited to adopt and modify the information in this document as needed to address specific or unique requirements.
For example, you may want to remove business processes and/or requirements that do not address a specific need or refine and customize the templates presented in this document to better serve your particular needs.
Please these requirements describe what is needed from an information system to support a business process; we do not attempt to identify existing software or systems that might meet those needs in this document.
Whether you develop a custom software solution or purchase a commercial, off‐the‐shelf (COTS) product is a determination only you can make based on your own programmatic and organizational needs.
Appendix E contains an example of each business process tool with explanatory text.
The business process matrix is a table that outlines the components that describe the process (objective, business rules, trigger, inputs, outputs, and outcome).
The business process matrix is designed to be used as a quick reference for groups who are analyzing business processes.
It is useful as a reference when developing graphical models, such as the task flow diagrams, to focus everyone on the same objectives.
The task flow diagram is a graphical model that illustrates the activities of a business process, as well as who performs those activities.
The task flows provide a “story” for the process being diagrammed.
Swim Lanes: A functional individual or group; these are entities that perform or are accountable for designated activities in the process.
Requirements are the statements that describe the functionality needed for an information system to support the business process.
The requirements associated with each business process are not intended to suggest any physical implementation strategy for an information system.
This section contains the work products developed by the workgroup.
First, an overall framework for the IIS business processes is provided.
Then, each individual business process is defined with a business process matrix, a task flow diagram, and a requirements document.
VFC eligibility status (5 statuses: uninsured, underinsured, insurance, etc.
Possible ways to provide order status: email, fax, phone, web portal, etc.
Have ability to produce reports in multiple formats i.e., Text delimited file, etc.
Defining requirements is a critical step in developing or acquiring an information system that will effectively support the work of the organization.
If the requirements are not correctly defined, the system will not meet the needs of the users.
Describing requirements for the way in which an information system should function involves first analyzing how the work gets done, by clearly defining the processes involved.
The Institute engaged 11 public health practitioners from 10 local and state health agencies, and applied a facilitated collaborative approach to developing requirements for reportable conditions surveillance information systems.
Through use of our Collaborative Requirements Development Methodology (CRDM), the Institute assisted practitioners in analyzing the tasks and processes performed; defining better ways to perform those processes; and documenting the requirements for the ways in which information systems should support that work.
The information system requirements they defined will enable other public health agencies to communicate with vendors and developers about how to meet their specific software needs, rather than having to buy off‐the‐shelf products that may not meet those needs, or settle for a vendor recommendation based on arbitrary likes or dislikes, as opposed to one based on product appropriateness.
To learn more about the Institute’s CRDM, visit the “How We Work” page at www.PHII.org.
Any deficiency is visible to the clinical user each time an individual’s record is viewed.
The IIS automatically identifies individuals due/past due for immunization(s), to enable the production of reminder/recall notifications from within the IIS itself or from interoperable systems.
This version of the functional standards was sent review and comment to the IIS community in May of 2012 by the Immunization Information Systems Support Branch (IISSB).
Eligibility is tracked at the dose level for all doses administered.
The IIS can provide immunization data access to healthcare providers, public health, and other authorized stakeholders (e.g., schools, public programs, payers) according to law, regulation, or policy.
Provide the necessary reports and/or functionality to facilitate vaccine recalls when necessary, including the identification of recipients by vaccine lot, manufacturer, provider, and/or time frame.
Facilitate reporting and/or investigation of adverse events following immunization.
This appendix contains the recommended data set for an Immunization Information System (IIS) to collect when enrolling a new facility, organization, or private provider to record and access information regarding administered immunizations and other health‐related data by healthcare providers.
Facility Type (Private Hospital; Public Hospital; Private provider; Community Health How will the practice report to the Registry?
The following pages contain the business process matrix and task flow for the EHR‐HIE‐IIS Data Exchange business process.
This business process demonstrates what immunization data exchange might look like using a Health Information Exchange (HIE)/Health Information Organization (HIO) as an intermediary; a model in which the HIE/HIO does not transform the message but simply passes it through.
There are multiple ways to accomplish this task (ex.
The system determines if a patient record is found.
The following pages contain example forms with explanatory language for each of the business process tools.
Preparing a Request for Proposal can be a challenging process.
It is important that you properly prepare and allow sufficient time to fully understand your business requirements and your jurisdiction’s procurement policies, and to thoroughly evaluate potential software vendors.
Typically, it takes from six to twelve months to complete the RFP process, depending on the complexity of the information system.
The RFP begins with understanding your business requirements and categorizing them as “need to have” versus “nice to have”.
There may be additional technical requirements or constraints that must be articulated to the vendors, so it is best to involve your IT department early in this process.
Most jurisdictions also have an RFP/RFQ template which you may be required to use as the basis for your solicitation.
Numerous state/local regulations are likely to govern the sequence, timing, publication and communications requirements of your competitive solicitation.
Be sure to connect with your procurement, legal and other departments early to understand these requirements.
For example, to what extent can you communicate with potential respondents, if at all?
Can you send a notice to vendors that the RFP has been published in the state register or on jurisdiction’s web site?
If a respondent asked you a question about your RFP, how are you to respond?
Are you required to publish the question and response to all other respondents?
Be sure to allow enough time for respondents to prepare thorough responses to the RFP.
Responses will then be evaluated and ranked on the basis of how closely they align with your “need to have” and technical requirements, with consideration given to “nice to have” as a further ranking criteria.
Based on this evaluation, you may select one or more respondents to perform live demonstrations of their systems, incorporating several use cases that you will provide ahead of time.
Depending on the breadth of the functionality, it will be best to allot half‐ or full‐day sessions for these use‐case demonstrations.
The value of your RFP and the resulting demonstrations in making the best vendor selection will depend on the clarity and applicability of the use cases you define, and on determining prior to the demonstration how – based on what criteria – the use‐case demonstrations will be judged.
Take time immediately following each presentation to evaluate the strengths, weaknesses and/or scores for each use case, based on the judgment criteria previously established.
In addition to evaluating the ability of the vendor to address your specific business and technical requirements, several additional factors should be considered.
It will be important to understand the overall cost of owning the software, including start‐up and maintenance costs, and to ascertain the vendor’s customer service capability, both their track record of customer satisfaction, as well as their strategy for maintaining and upgrading their software.
Along these lines, it is equally important to assess the vendor’s financial viability.
It would be unfortunate indeed to purchase software with all of the correct functionality from a vendor on the verge of bankruptcy, which could leave you with no one to turn to for support or maintenance of your software.
Although the RFP and vendor selection process requires significant effort and due diligence, the result will be a well‐organized and documented approach that allows you to select the most appropriate software application and vendor to meet your business needs.
It is also helpful to provide insight to what is defined as out‐of‐scope.
The more clarity provided here, the better the vendor can assess their ability to meet your needs.
This section typically details expectations related to implementing the information system, user testing, system documentation, system maintenance, and technical support.
This is typically laid out in a matrix or table structure, along with a scoring scheme.
More weight is given to those areas of highest importance.
Can a customer install multiple non‐interacting instances of the information system in order to support training and testing?
If so, describe approach most commonly used by customers, and give two supporting customer references.
Wherever possible, established definitions are used.
A successful vaccine event data upload into the registry after confirmation of the data’s conformity to the data guidelines.
A federally‐established advisory committee that provides advice and guidance to the Secretary of HHS, the Assistant Secretary for Health, and the Director of CDC regarding the most appropriate selection of vaccines and related agents for effective control of vaccine‐preventable diseases in the civilian population.
The committee specifically provides advice for the control of diseases for which a vaccine is licensed in the U.S. IIS vaccine validation and forecasting logic is based upon ACIP recommendations.
A membership organization that promotes the development and implementation of immunization information systems (IIS) as an important tool in preventing and controlling vaccine preventable diseases.
The organization provides a forum through which IIS programs and interested organizations, individuals, and communities combine efforts, share knowledge, and promote activities to advance IIS and immunization programs.
A user who is granted access to a database and agrees to the confidentiality and security guidelines set by the owner of the system.
For IIS, authorized users are generally established in statute or policy, and can include both those that submit immunization data and those that have read‐only access.
Two‐way exchange of immunization information that includes an inbound vaccine query/add/update records message from an EHR or other system, and a reciprocal response that includes patient‐specific vaccine history and vaccine forecasts (see below).
The process of establishing consistency among data from different sources within a centralized database.
The process of identifying and eliminating redundant records or data.
As a result of the de‐duplication process, duplicate data is either deleted or merged, leaving only one copy of the record or data to be stored.
The Electronic Health Record (EHR) is a longitudinal electronic record of patient health information generated by one or more encounters in any care delivery setting.
Included in this record is patient demographics, progress s, problems, medications, vital signs, past medical history, immunizations, laboratory data and radiology reports.
The EHR automates and streamlines the clinician's workflow.
The mobilization of healthcare information electronically across organizations within a region or community.
HIE provides the capability to electronically move clinical information between disparate healthcare information systems while maintaining the meaning of the information being exchanged.
The goal of HIE is to facilitate access to and retrieval of clinical data to provide safer, more timely, efficient, An organization that oversees, governs, and facilitates the exchange of health information among organizations according to nationally recognized standards.
Also referred to as a Health Information Organization (HIO).
Confidential, population‐based, computerized databases that record all immunization doses administered by participating providers to persons residing within a given geopolitical area.
MIROW (Modeling of Immunization Registry Operations Workgroup) An initiative of the CDC National Center for Immunization and Respiratory Diseases (NCIRD), in partnership with the American Immunization Registry Association (AIRA), to analyze and improve Immunization Information System (IIS) operations by developing best practice recommendations for IIS operations and processes.
The initiative uses a collaborative approach, leveraging subject matter experts from the IIS community to identify and analyze current practices and in a focused, structured process, reaches consensus on best practices.
A health care provider is an individual or an organization, licensed by a state to provide preventive, curative, promotional or rehabilitative health care services in a systematic way to individuals, families or communities.
Most providers are licensed to administer immunizations but it is most commonly done by family practices and pediatric practices, as well as hospitals and, increasingly, pharmacies.
The designation for an organized group of providers.
The organization may include a number of different provider offices or sites and physician groups, and is often a corporate name.
A governmental agency with public health oversight or management responsibilities over a particular public health jurisdiction and associated population.
Information Technology on Defining Key Health Information Technology Terms, April 2008.
The primary mechanism for retrieving information from a database.
A query consists of questions presented to the database in a predefined format.
A synchronous, reciprocal transaction sent from the provider to the IIS.
An immunization that is due or past due for an individual/patient per a recommended immunization schedule, and based on the patient’s age and immunization history.
The rejection of a vaccine event data upload into the registry when data fails to conform to the data guidelines.
A rejected submission is customarily returned to the sender to be fixed and subsequently re‐ sent.
A set of rules and procedures that guides the Reminder/Recall operations.
An entity (provider; see above) that submits immunization data to the IIS.
The date the patient received one or more doses of one or more vaccines.
Several vaccination events can happen during one vaccination encounter.
A Valid/Invalid flag indicating that a dose of vaccine should not be considered when generating a vaccine forecast (see below).
An Invalid flag indicates that a dose administered to a patient is considered substandard and therefore not a valid dose that can be counted.
Broad categories of vaccines that generally correspond to individual antigens and are related by vaccine type (e.g., Hib‐PRP‐T and Hib‐HbOC have a Vaccine Family/Group Name of Hib).
A Vaccine Family/Group Name may correspond to a group of multiple vaccine types that are typically given in a combination vaccine (e.g., MMR and DTP).
The output of an IIS’s algorithm/business rules that des what vaccines are due, coming due, or are past due for an individual, based upon that person’s age, vaccine history, and the ACIP recommendations.
A vaccine forecast is a form of clinical decision support.
A federally funded entitlement program offering vaccines at no cost for eligible children through VFC‐ enrolled doctors.
IISs have increasingly been engaged to assist with greater VFC vaccine accountability.
A tracking system that supports accurate inventory management of vaccines within an organization, including doses on hand by vaccine lot number (see below), doses wasted or transferred, reports, and other inventory functions.
The manufacturer‐assigned number for a specific batch of vaccine developed and distributed.
This number is the tracking number of the administered vaccine, and can be used by IIS to help recall individuals who received sub‐optimal vaccine from a lot.
The name under which the manufacturer copyrights a vaccine/vaccines.
A trade name is usually assigned by the manufacturer to identify vaccine type.
An internationally recognized numerical code that designates a specific vaccine product.
CPT and CVX codes are the two most recognized standards to designate vaccine type.
An information system that integrates the entire publicly‐funded vaccine supply chain, from purchasing and ordering to distribution of the vaccine.
The following terms are included to clarify the meaning of words used within this document.
Attempting to reduce an existing manual job to a set of computer programs that can replace the existing manual effort with the minimum of human effort or understanding.
A technique or methodology that, through experience and research, has shown to reliably lead to a desired result.
Habitual or customary actions or acts in which an organization engages.
Also used in the plural to describe a set of business operations that are routinely followed.
A set of related work tasks designed to produce a specific desired programmatic (business) result.
The process involves multiple parties internal or external to the organization and frequently cuts across organization boundaries.
The effort to understand an organization and its purpose while identifying the activities, participants and information flows that enable the organization to do its work.
The output of the business process analysis phase is a model of the business processes consisting of a set of diagrams and textual descriptions to be used for design or redesign of business processes.
The effort to improve the performance of an organization's business processes and increase customer satisfaction.
Business process redesign seeks to restructure tasks and workflow to be more effective and more efficient.
A set of statements that define or constrain some aspect of the business process.
Business rules are intended to assert business structure or to control or influence the behavior of the health agency (business).
Entities are represented by circles and transactions are represented by arrows.
A context diagram may involve all the transactions of a single user of a system or of multiple users.
Usually, single‐user diagrams are attempted first (for ease), but multi‐user diagrams are needed to get a good look at an entire process.
An action or set of actions that adds an identifiable value to a given business process objective.
Groups or individuals who have a business relationship with the organization— those who receive and use or are directly affected by the services of the organization.
Customers include direct recipients of treatment and services, internal customers who provide services and resources for final recipients, and other organizations and entities that interact with an LHD to provide treatment and services.
A person, group of people, or organization that performs one or more tasks involved in a process.
Entities are represented by circles in context diagrams.
A defined support structure in which other components can be organized and developed.
A logical structure for classifying and organizing complex information.
A system of rules, ideas or principles that provides a unified view of the needs and functionality of a particular service.
A repeatable task series or operation that is used in more than one instance and can be shared across multiple business processes.
The major health goal that the business process supports.
The goal is the end state to be achieved by the work of the health agency and should be defined in terms of the benefits provided to the community/population or individual/client.
Refers to the interaction between processes and technology, which may occur within or between organizations.
It includes the information technology an organization uses, the ways the organizations interacts with the technology, and the ways technology works with the organization’s business processes.
Information received by the business process from external sources.
Logical design describes textually and graphically how an information system must be structured to support the requirements.
Logical design is the final step in the process prior to physical design, and the products provide guidelines from which the programmer can work.
A concrete statement describing what the business process seeks to achieve.
The objective should be specific to the process such that one can evaluate the process or reengineer the process and understand how the process is performing towards achieving the specific objective.
A well‐worded objective will be SMART (Specific, Measurable, Attainable/Achievable, Realistic and Time‐bound).
The resulting transaction of a business process that indicates the objective has been met.
Producing or delivering the outcome satisfies the stakeholder of the first event that triggered the business process.
Often, measures can be associated with the outcome (e.g., how much, how often, decrease in incidents, etc.).
An outcome can be, but is not necessarily, an output of the process.
The information may have been the resulting transformation of an input, or it may have been information created within the business process.
A task output that may be used in one of three ways: (a) as an input to the next sequential step, (b) as an input to a downstream step within a task series; or (c) as the achievement of an organizational objective.
Define the specific tasks that need to be performed by an information system to complete a task.
Serves to specifically define the functionality to be supported.
Requirements are also specified to ensure that activities within the business process remain within physical and operational boundaries.
A logical, step‐wise approach to think through the tasks that are performed to meet the specific public health objectives (analyze business processes), rethink the tasks to increase effectiveness and efficiency (redesign business processes), and describe what the information system must do to support those tasks (define system requirements).
A person, group, or business unit that has a share or interest in a particular activity or set of activities.
A definable piece of work that can be done at one time; i.e., what happens between the in‐box and the out‐box on someone’s desk.
A business process is made up of a series of work tasks.
Graphic depiction of tasks showing inputs, processes, and results for each step that makes up a task.
A business process may contain more than one task series.
The set of tasks that are carried out in a business process.
May also be the exchange of goods (e.g., a vaccine or payment) or services (e.g., an inspection) between two entities.
Transactions are represented by arrows in context diagrams.
Event, action, or state that initiates the first course of action in a business process.
A trigger may also be an input, but not necessarily so.
Public Health England exists to protect and improve the nation's health and wellbeing, and reduce health inequalities.
It does this through world-class science, knowledge and intelligence, advocacy, partnerships and the delivery of specialist public health services.
PHE is an operationally autonomous executive agency of the Department of Health.
Prepared by: Public Health England, NHS England, Department of Health and the Health and Social Care Information Centre.
Lead authors: Helen Duncan, Mandy Harling, Coleen Milligan and David Low.
You may re-use this information (excluding logos) free of charge in any format or medium, under the terms of the Open Government Licence v3.0.
To view this licence, visit OGL or email psi@nationalarchives.gsi.gov.uk.
Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned.
Any enquiries regarding this publication should be sent to [insert email address].
This document is an update of the Information Requirements Specification (IRS) and the Output Based Specification (OBS) for Child Health Information Systems that was published as two documents in 2012 by the Department of Health.
It is intended to support the commissioning and delivery of services to help meet requirements within the Child Health Information System - NHS Service Specification Section 28.
This edition brings together the IRS and OBS into one document to improve the consistency of information in one source and avoid duplication.
The document has been updated to include developments in the screening, immunisation, health and development requirements of children and young people which are reflected in national policy or statutory requirements that have emerged since its original publication in 2012.
This revision has been led by Public Health England (PHE) with content and advice from across organisations involved in child health programmes including NHS England, Health and Social Care Information Centre (HSCIC), United Kingdom National Screening Committee and experts from the national Child Health Information System (CHIS) Programme Board and working group, clinician and managers of child health services, informatics leads and professional groups representing child health services.
The initial work for the 2012 documents was compiled by the Department of Health’s Child Health Information Systems Transition Steering Group, chaired by the National Clinical Director for Children, Young People and Maternity Services, Dr Sheila Shribman.
This built on earlier work initiated in 2007 by the Department of Health and Connecting for Health Child Health Programme to establish universal information requirements for child health information systems (CHIS).
The content detailed within this document results from the engagement of clinical and informatics colleagues without whom this current document and project would not have been possible.
This document does not set out to repeat the many non-functional or general requirements that appear in existing contractual documentation, covering themes such as information governance, Personal Demographics Service (PDS), and clinical requirements or safety.
In the event of a conflict between this document and the policy documents that are referenced from each chapter, the policy documents should take precedence unless explicitly stated otherwise.
To reflect the transition period this document avoids the use of specific terms to refer to organisations involved in commissioning child health services delivery eg local health provider, Local Authority, Clinical Commissioning Group etc.
Instead, the term “Commissioning Organisation” (CO) is used to refer to those organisations involved in the planning and purchasing of health services for a defined local population.
In some cases, an organisation may be responsible both for commissioning the information service that underpins delivery of healthcare services (the CHIS) and for commissioning the health service (eg immunisation).
However, in some cases responsibility may be split between different organisations.
Clarity about roles and responsibilities for the different parts of the public health and healthcare system in relation to CHIS will be set out in an operating model developed by the NHS England.
It is worth noting that, following the Transforming Community Services agenda, in some cases the current commissioning organisation for CHIS will be separate from the child health service provider.
It is anticipated that this distinction between commissioning and provision will be retained in the future.
All children receive what are known as “Universal” services.
This document is limited in scope to key requirements for Child Health Information and information systems in England-only.
It also recognises the varying degree and maturity of IT infrastructure and systems within Child Health Record Departments.
Use of the term information system in this document does not assume a single monolithic application.
An information system may comprise many separate components.
To ensure that the requirements support the development of the entire information system, it is important to consider all stakeholders in the development and who will be responsible for different components or services underpinned by the CHIS.
Where MUST is outlined, this will indicate that the definition is an absolute requirement of the specification, and services should be able to demonstrate compliance.
Where SHOULD is outlined, this will indicate that there may exist valid reasons in particular circumstances to ignore a particular item, but the full implications must be understood and carefully weighed before choosing a different course.
Where MAY is outlined, this will indicate that the requirement is truly optional.
The Information Requirement Specification within this document details the technical requirements of an information service that is necessary to the Healthy Child Programme.
The primary objective of CHIS is to ensure standardised and accurate data and information to support the commissioning and delivery of child health services.
This means providing a service that delivers a comprehensive local record of a child’s public health (screening, immunisation and other health protection or health improvement interventions) and of their community based healthcare.
Use information to improve health outcomes for children and families.
Produce uniform data and process for child health systems that will provide a basis for consistent information exchange across health and care leading to better outcomes for all children.
Proposing a clinically driven national standard for supporting local procurement and commissioning of child health information service systems.
Allowing for better information exchange across other multiple agencies leading to improved outcomes for the child.
To support the national and local delivery of the national Healthy Child Programme as originally mandated in the Section 7a of the NHS Mandate and subsequently for Local Authorities by further regulation.
In addition to the above, this document outlines the functional and non-functional requirements to meet the needs of the Child Health service which are covered in more detail throughout this document.
The OBS elements of this document are directed primarily at colleagues within NHS England in order to build an outline business case to support the future commissioning of CHISs.
It will be of interest to people who are currently involved in the commissioning and delivery of these information systems and those who will have this responsibility or in the future and those who rely on these systems to fulfill their own responsibilities.
The term ‘commissioning organisations’ (COs) has been used throughout this document to refer to those responsible for contracting for the Child Health Information Service (CHIS), and/or responsible for contracting for screening, immunisation and other child health services.
It is acknowledged that this document reflects the current child health information requirements but that these will evolve and increase over time to reflect clinical and policy changes.
Therefore it is important to a future refresh will be needed to reflect these requirements.
CHISs are child health information service systems currently operated at a local level, they are commissioned locally and the commissioning standards are overseen by NHS England.
A significant proportion of these systems are currently part of the Local Service Provider contracts originally delivered under the National Programme for IT, which expire in 2015/16 and new commissioning arrangements need to be established.
The system supports a variety of child health related activities, including public health services such as immunisation and childhood screening.
They are also important as they provide information about individual children’s health (which can be useful eg during an outbreak of a disease for which some children may need to be immunised), but also provide a population-perspective, enabling effective targeting of services (eg they capture non-GP-registered children enabling a whole population view to be taken about needs to inform local commissioning strategy).
In summary all CHIS systems provide a database of information on a population of children for the purposes of immunisation, screening and delivery of the Health child programme 0-5.
However in some instances they have been developed to support the delivery of care to individual children and have become integrated within an electronic community child health record.
NHS England were given the responsibility for Child Health Information Service Systems in 2013 via Section 7A of the NHS mandate following the identification of a number of concerns about child health information.
These set out the key considerations in the transfer of commissioning responsibilities for the 0-5 year old Healthy Child Programme to ensure a stable transition process which will maintain service continuity and support the continued development of the service, offering best outcomes for children and their families by ensuring clear and robust contractual arrangements are in place between commissioners and service providers through transition.
The transfer of 0-5 years commissioning to local authorities will enable joined up children’s Public Health commissioning and services across health, education and Social Care from 0-19 years old, improving continuity for children and their families.
Personalised Health and Care 2020 highlights one of the greatest opportunities of the transform our society, to meet the challenges of improving health and providing better, safer, sustainable care for all.
To date the health and care system has only begun to exploit the potential of using data and technology at a national or local level.
The ambition is for a health and care system that enables people to make healthier choices, to be more resilient, to deal more effectively with illness and disability when it arises, and to have happier, longer lives in old age; a health and care system where technology can help tackle inequalities and improve access to services for the vulnerable.
Maternity and Children's Dataset (MCDS) is a comprehensive data collection on maternal and child health which is collected from different systems that include maternity, Child Health Information Service Systems and CAMHS systems.
Information on child health is collected on topics ranging from birth details, screening results and immunisation to growth measurements on school entry, breastfeeding indicators and social care information.
The maternity and children’s dataset is a new dataset specifically developed for all commissioned maternity, child health, and child and adolescent mental health services (CAMHS) as a key driver to achieving better outcomes of care for mothers, babies and children.
The maternity and children’s dataset supports the Healthy Child Programme and provides comparative, mother and child-centric data that will be used to improve clinical quality and service efficiency; and to commission services in a way that improves health and reduces inequalities.
The three information standards that comprise the data set have received full approval from the Information Standards Board for Health and Social Care as information standards for England and are therefore mandated data collections for organisations which provide these services.
In line with a core aim of the Healthy Child Programme being the encouragement of partnerships between agencies in service development, the data items required by the child health dataset as a necessity will relate to domains ranging from primary and secondary care to social services and education.
There are no current firm plans to integrate social services or educational systems into a data collection service (although this might be the long-term ideal), and so the national maternity and children’s dataset has been designed to collect information at the same standard and granularity as those systems but must currently be recorded in locally based systems.
Like other clinical datasets this one is designed for secondary use purposes using patient care records, so that clinical information can directly inform the planning and commissioning of services.
Without this solution, there is no efficient means of making these data available to commissioners, service providers and stakeholders.
CHISs will be required to collect some of the data for the maternity and children’s dataset, ensuring that data is collected only once, and used for multiple purposes.
The requirements set out in this document reflect and include the requirements of the new dataset.
Outcome measures for children are an increasing priority area in line with the Public Health Outcomes Framework and the NHS Outcomes Framework.
Such measures are essential for service planning and the effective delivery of care to children.
The Public Health Outcomes Framework includes an indicator 3.31 for population vaccination coverage this indicator will use data from CHIS and cover all vaccination programmes across the life course, as previous evidence shows that highlighting vaccination programmes encourages improvements in uptake levels.
In addition, a population based measure of child development at age 2-2.5 years is currently underway and, when finalised, the required information will need to be captured on CHISs together with the accompanying information flows to other parts of the system.
Similarly the Children and Young People’s Outcome Strategy (announced 26 January 2012) has produced recommendations that are incorporated into this document as part of the future requirements for CHISs.
The Section7A agreement of the Health and Social Care Act 2012 sets out the governance, key deliverables and ambitions for public health functions which are to be commissioned by NHS England.
It is agreed annually through tripartite negotiations between Public Health England, Department of Health and NHS England.
The Section 7A Agreement provides the mandate through which funding will pass from the Department of Health to NHS England to deliver on these specifications.
The content of each year’s Section 7A Agreement is collated and proposed by the various public health programme boards, for prioritisation by the tripartite NHS Public Health Steering Group and onward consideration by the Senior Oversight Group.
The resulting recommendations are given for approval by the NHS England Board and the Secretary of State.
This programme is being delivered in partnership between the Department Health, NHS England, Public Heath England, and Health Education England.
NHS England is responsible under the NHS Mandate Section 7A for commissioning the additional capacity and service transformation for children as part of the National School Nurse Development Programme.
Set out plans for commissioning responsibilities to be devolved to the most local level where possible and appropriate.
Subsequently it has been clarified that groups of health professionals operating within the primary care sector as clinical commissioning groups will take on many of the roles previously exercised by Primary Care Trusts.
The NHS White Paper placed great emphasis on the importance of information, both for people using services and about those people and professionals providing those services.
Stressed the importance of sharing information between appropriate professionals to enable delivery of effective care for children, especially vulnerable and at-risk young people.
Set out that in the future public health system, local authorities will undertake many of the public health duties currently discharged by Primary Care Trusts, focused on improving the health of their population.
It described a life course approach to protecting and promoting the public’s health, which included an emphasis on starting and developing well.
The Public Health White Paper emphasised the importance of information in supporting effective planning, commissioning, delivery and evaluation of services.
This focused on providing a universal service for improving the health and wellbeing of children, through health and development reviews, health promotion, parenting support, screening and immunisation programmes.
Its goals are to identify and treat problems early, help parents to care well for their children, change health behaviours and protect against preventable diseases.
The programme is based on a systematic review of evidence and is expected to prevent problems in child health and development and contribute to a reduction in health inequalities.
The Healthy Child Programme which is a prevention and early intervention public health programme offered to all families is led and delivered by health visitors and their teams.
This work provides the foundation stage for the Healthy Child 5-19 Programme.
Together these changes represent a significant shift in the way in which Services are planned, commissioned and delivered for health and care, including those provided for children.
The changes to the health and care system provide an opportunity to further the reach of these programmes by drawing together, in mutual responsibility, the work of local authorities, clinical commissioning groups and local services.
Commissioning of those elements of the 0-5 Healthy Child Programme which are led by health visitors and family nurses, will move from NHS England to local authorities on 1 October 2015.
In its response to the NHS Future Forum’s second report, the Government published an information strategy for health and social care in England.
This strategy built on the Information Revolution consultation, which discussed a challenging idea in relation to electronic care records – that these could be used both to deliver safer, integrated care, but also progressively become the main source for all patient and professional information, including secondary uses of such data.
Potential secondary uses of care data include population health, clinical improvement, research and commissioning.
CHIS systems are no exception to the expectation that better use should be made of data captured at the point of care and some systems in use around the country are already meeting this requirement, providing both a database of information on a population of children, and supporting the delivery of care to individual children.
The child health record exists for each child additional to the GP’s record for all and hospital records for some children, and contains for each child a health protection and promotion section, as well as illness biography.
The common core content would form the basis of a shared record of child health and care, and what such a shared record should contain.
CHIS are currently operated at a local level by providers and commissioned by NHS England until at least 2020 to provide opportunity for upgrading specifications in local contracts.
They support a variety of local child health related activities, including immunisation, screening and health and development reviews, and provide opportunity for health promotion.
Due to the evolving nature of the NHS since 1974, systems are not always coterminous with either organisational or geographical boundaries, yet they have to provide information on a number of different population bases.
The extent of CHIS solution and use varies across the country and include local paper forms, home-grown IT solutions and those that already provide an electronic patient record.
All CHIS systems should be able to generate lists and schedules for immunisation, surveillance and health promotion services, to enable clinicians to record their key activities and interventions or measurement, screening, assessment and outcomes; incorporating heath protection, health promotion and health data.
The systems are not necessarily well-adapted for specialist services, such as community paediatrics.
A child-focussed public health record must be available to all healthcare professionals with the relevant permissions who have contact with a child.
There will be multi-agency sharing with in an agreed consent framework.
All standards (including messaging and interfaces) should make use of existing NHS standards and processes, and implementations should comply with those published by the NHS Information Standards Board.
Where new standards are required, they should be based on existing NHS standards and protocols, and should be developed with approval by the NHS England Standardisation Committee for Care Information (SCCI) process to make them available to the wider NHS and Government in the UK.
The table below outlines the overarching principles and requirements for all the services and systems involved.
The majority of these principles and requirements will be covered in greater detail later in this document.
These requirements will help to shape the functionality required to meet the needs of the organisations involved and to act as an interoperable information source for all key stakeholders in the development of CHIS Services.
The high level principles supporting the commissioning and procuring of CHIS should be considered the starting place for what the service will deliver.
CHISOP001 Access and use of the CHIS must not increase a clinician’s workload nor disrupt operation of clinical practice and should be as efficient as or more so than the existing processes.
These elements are being mandated from October 2015 as evidence shows that these are key times to ensure that parents are supported to give their child the best start in life.
They will also produce extracts for dataset submission.
CHIS systems should support extensive system interoperability to deliver the critical linkages between child health delivery systems, maternity records, the personal child health record (PCHR), the eRed Book, national screening systems, laboratories and GP practice systems in terms of communication of records and provision of failsafe processes.
CHISOP015 CHIS systems should address consent at a national level and thereby disseminate the legal framework for consent and information and data sharing which is vital for the provision of care.
The information sharing approaches should be consistent with GMC 0-18 requirements.
The proposed subset of this information is contained in the Appendix B.
In development of the system, consideration should be given to support the integration of information arising from social services and educational information systems, and any other commissioned alternative providers.
The following high-level requirements will require further elaboration with the supplier and user base to derive detailed system functionality.
Functionality described here is common to all or most of the services which are covered by a child health information system.
This section must be read prior to reading the service-specific functionality which only describes the information needs of services over and above these general requirements.
Service-specific functionality is outlined in chapters 5-15 of this document and non- functional requirements are addressed in chapter 16.
Each supplier must outline how they will meet each of the following Functional Requirements.
The keywords Must, Should and May should be interpreted as described in section 1.5.
Please note that we do not describe datasets and data items in detail in this document as for secondary uses the requirements have already been set out in the Health and Social Care Information Centre’s (HSCIC) Maternity and Children’s Dataset.
Compliance with these datasets is addressed in chapter 3, Overarching principles.
This includes the searches for and recording of aliases.
The system must use Organisation Data Service codes.
It should be d that the parent can register the child with a different GP.
CHIS system providers are responsible for providing electronic interfaces to allow information to be sent and received and where appropriate accessed by Educational, Local Authority and NHS Care System users for direct care purposes.
Interoperability of systems is vital to ensure that professionals have up to date information about the health of children for whom they have a responsibility.
Information sharing with other agencies is guided by nationally agreed consent and data sharing rules within legal frameworks and guidance4.
A child’s record can be held on more than one CHIS, for example where they live and where they access services, and it is important that both records are maintained with accurate up-to-date information.
Although they may choose to register the child at a different GP practice.
The requirements below describe both this core content of the child health record but also make reference to a summary child health record which needs to be readily viewable and available for exchange with other systems.
The system must be able to collect child health information and support national aggregation of clinical data about child health, - immunisation (including pre and post immunisation testing) This list is not exhaustive and development of the system must requirements.
The child health summary record must be readily accessible for child’s care.
The system must be able to provide the facility to record the parent/guardian’s consent or withholding of consent to release data to any department of the local authority, to schools, or any other named agency.
The system must be able to exclude those withholding consent when producing reports to be shared.
The system must be able to provide the facility to record why date the information was released, by and to whom.
When information is released without consent, the system must enable the selection of standard reasons, which could include: ∙ the law sets aside confidentiality, e.g. Section 251 of the ∙ the law overrides confidentiality, e.g. Section 47 of Children override confidentiality considerations.
The system must be able to record and store the immunisation indicator 3.3: population vaccination coverage.
If this is a review that has taken place with no significant items which justify a detailed record being made this must also be supported, ie it must be possible to record that the review has taken place even when the review has no significant outputs.
The system must be capable of recording the name and role of the healthcare professional treating or screening the child.
Homeless, Refugee, Asylum Seeker or Traveller status.
The following sections contain requirements particular to a domain, for example, ‘Registration’ or to a specific service provided by child health professionals, for example, Immunisation.
General functional requirements which are common to more than one service or domain, for example, ‘Reporting’ are not repeated in this section.
Readers are strongly advised to familiarise themselves with chapter 4 prior to reading this section.
Similarly chapter 16 details the non-functional requirements for CHIS systems such as conformance to information governance, role-based access, security audit etc.
Again these are common requirements across services so are not repeated here.
This chapter addresses the issue of which children a CO should hold details on and what is the status of the CO’s responsibility towards the children in its database.
There is considerable uncertainty among many professionals involved in the delivery of care to children as to where ultimate responsibility lies, in particular where children live near the border with another CO.
The CO should ensure that their database includes all children for whom they have a statutory responsibility.
This includes the child population resident within the local area, the child population registered with GP practices in the local area, and all children who are in schools within the local area but are neither locally registered nor locally resident.
Technology Reference Data Update Distribution Service.
Further work will be required by NHS England to establish the footprints of COs responsible for CHIS in the future system.
Continual work will be required as commissioning arrangements evolve and the possibility of service and CHIS provision is re-commissioned.
Hence CHIS systems need to be sufficiently flexible to allow such changes to be reflected in their databases.
It is necessary to be able to identify whether the CO has the statutory responsibility for the child.
If the child is the responsibility of the CO but the delivery of certain services have been contracted to multiple providers it should be possible to record which organisations have been contracted to provide which services for the child.
This requirement includes the arrangement for some services to be provided to school children by the CO regardless of the registration or address of the individual children.
Changes in a CO’s scope of responsibility can be triggered by a number of events as outlined in the following sections.
In relation to each child, the CO must maintain a list of services that the CO is responsible for fulfilling and, where appropriate, record the commissioning organisation for all aspects of the service.
A process should be put in place to, where possible, obtain as many details from the child’s previous health service.
By “deregistration” all that is meant is that the child’s record status is changed to indicate that they are no longer the responsibility of the CO.
The CO must record a forwarding address for the child, or if not known record as “address not known”.
This is performed on PDS (known as an “exit posting’).
Upon request the CO must be able to send a detailed record to the new health service.
A cause of death must, if known, be recorded using recognised coding.
Procedures should be in place to ensure that health professionals, including the child death review panel, involved in the care of the child are made aware of the child’s death.
In all cases CHIS should update PDS if aware of a death and if the death status has been found on PDS while tracing, should follow the procedures for example, enter date of death, and cancel all appointments.
If the child does not have an allocated health visitor or school nurse, the CO should allocate a health visitor (or health visiting team) or school nurse team to a child and make available all pertinent information relating to the child.
The CO should be able to record details of any notifiable congenital malformations of a child using standard codes.
CO who has the statutory responsibility for the child.
The CO must be able to record the tracking of records, ie record source of records and date received or details of where records have been sent and when.
The system should be able to send and receive demographic details to enable an up to date record.
It should be possible for a CO to maintain linkages between the child and their siblings (including half and step siblings) and parents through recording NHS numbers.
It should be possible to record and maintain patient/family preferences and concerns, such as with language, religion, culture, medication choice, invasive testing, compliance with the Mental Capacity Act 2005, and advance directives.
It should be possible for such information to be incorporated in relevant communications and, in addition, made available to staff who will come into contact with the child and immediate family.
It should be possible for the CO to keep a record of the child’s current and previous school.
This must also include schools that are outside the care community but are attended by pupils who are resident or registered within the care community.
It should be possible for the CO to be able to run a report that identifies all children for whom a school has not been recorded.
The CO must be able, through a controlled method, to merge or link dispersed information for an individual person upon recognizing the identity of that person.
If health information has been mistakenly associated with a person, then the CO must provide the ability to mark the information as erroneous in the record of the person in which it was mistakenly associated and represent that information as erroneous in all outputs containing that information.
If health information has been mistakenly associated with a patient, the CO must provide the ability to associate it with the correct patient.
This section describes any interfaces for which standards have been set.
Personal demographics service (PDS) issues birth notification messages, details of which can be found in the current Message Implementation Manual.
There is a need to be able to record information pertaining to the unborn child and to ensure that information about the family relevant to the child eg hepatitis B status is incorporated seamlessly into the baby’s record at birth.
There is a need to be able to record information pertaining to the unborn child and to ensure that information about the family relevant to the child is incorporated seamlessly into the baby’s record at birth.
And that relevant information contained in the maternity record is transferred electronically to the CHIS system, which could potentially be linked to the registration of the child’s NHS number.
The requirements in this chapter relate specifically to safeguarding aspects of the Child Health Information Service.
Safeguarding should be woven into the delivery of all child health services.
References are made in a number of chapters within this specification to safeguarding issues, including the means of identification and assessment.
This chapter focuses on safeguarding-specific information systems issues, such as coding schemes and integration with national systems that contribute to safeguarding which are over and above the general functional requirements outlined in Chapter 4, which must be read prior to consideration of this chapter.
Send Safeguarding Information e.g. Conference minutes etc.
To conduct safeguarding responsibilities, a CO must maintain a record of relevant information from many different sources, as is demonstrated by the diagram.
This includes visibility of a child’s status such as having a child protection plan or being looked after.
CO systems must be able to receive relevant information from the local authority such as notification of a child protection plan or looked after plan.
Likewise health services safeguarding information should be easily shared with the local authority as the statutory agency for safeguarding children subject to local data sharing agreements.
All children should be subject to checks for safeguarding issues.
The requirements in this section relate specifically to Newborn and Infant Physical Examination aspects of CHIS, which are over and above the general functional requirements outlined in Chapter 4 of this document, which must be read prior to consideration of this chapter.
The newborn and infant physical examination (NIPE) is offered to all babies.
The initial examination should take place within the first 72 hours of birth and again at 6-8 weeks of age.
It includes screening for developmental dysplasia of the hip, congenital heart conditions, congenital cataract and undescended testes in boys; in addition a full physical examination including detection of some congenital abnormalities is undertaken.
The 6-8 week examination is the responsibility of the child’s GP.
Each of these conditions has a pathway of care and is distinct from the general newborn examination, which may detect other congenital abnormalities for which there is not a formal screening programme.
Where a possible abnormality is detected the baby is referred for a specialist assessment.
This chapter describes the situation that will exist when the NIPE Screening Management and Reporting Tools (SMART) IT System is fully adopted.
The SMART IT system is currently being rolled out across England.
The CO must ensure that all the children for whom they are responsible are offered the NIPE.
The report should indicate the process status for each child in the list, eg “no action taken.
Visitors as outlined above in data flows section above.
The primary screening process will be managed by the central NIPE screening management system when fully implemented, but the CO has responsibility for ensuring coverage of all of the children for which it is responsible, and for assuring the quality of the service.
ITK messaging specification for newborn screening: messaging newborn hearing, NIPE and Bloodspot screening results to the CHIS.
Standard criteria for the optimum age for conducting NIPE has been set – initial examination within 72 hours and second examination between six and eight weeks .
Newborn NIPE screening system must be able to inform the CHIS of missed appointments and schedule for Infants up to and including three months of age who miss either optimal newborn examination within 72 hours or infant examination by 8 weeks of age.
They should have the examination undertaken as soon as is possible.
This section does not attempt to describe the end-to-end process, or even all of the processes that take place within primary care.
This section does not attempt to describe the end-to-end process, or even all of the processes that take place within primary care.
Rather, it identifies functional requirements placed on the Child Health Information Service that support the administration of newborn blood spot screening.
The CO must ensure that all the children for whom they are responsible are offered the blood spot screening test up to the age of one year.
For children who are born in the UK, in the great majority of cases the midwifery teams will follow-up births by visiting the baby and taking the blood spot sample.
However, for a variety of reasons (such as immigrant children) it may be necessary for COs to initiate a blood spot screen.
The report should indicate the process status for each child in the list, eg “no action taken” or “sample requested”.
In the event that a CO needs to instigate blood spot screening the CHISs must support the production of an instruction to a healthcare professional (usually a health visitor) to take the sample.
The CHISs must update the blood spot screening status to indicate “sample requested”.
The CO must indicate on a child’s record whether it has been offered a blood spot test.
In addition, if the parents decline a test for any or all of the conditions, this must also be recorded.
The CO will be sent a status code 01 by screening laboratories that have implemented the national standard for electronic reporting.
This status code indicates that the labs have received a blood spot card for a child in the CO’s care.
COs must record receipt of status code 01 against the child’s record.
The CO will be sent the results of the screening test by the laboratory.
The CO must be able to record the results against the child’s record using the nationally agreed status codes as described in the “External interfaces” section below.
It must be d that the results of a test for cystic fibrosis conducted on a sample taken from a child older than eight weeks are not valid.
The CO must not store such results against the child’s record.
In the event that a child has an elevated immunoreactive trypsin reading, the CO will be sent a mutation result from a DNA laboratory.
The mutation result must be stored against the child’s record and be available to support quality assurance of the screening programme.
Any screening may be pre-empted because of family history or clinical need, e.g. meconium ileus.
This information needs to be retained on the child’s record by the CO.
All results and other status codes received for repeat tests should be stored in addition to previously received results and other status information.
Thus a full chronology of activity and outcome should be maintained and be accessible.
Often, when tests need to be repeated, the baby is still in the care of the community midwifery team, and they will take responsibility for taking the repeat sample.
However, if responsibility has been transferred to the health visitor, then the CO must take responsibility for organising a repeat sample to be collected.
COs must record the date and reason that a repeat sample has been requested regardless of whether the repeat sample was requested by the screening laboratory or by CO staff.
In the event that results are received for babies born at less than 32 weeks gestation (less than or equal to 31+6 days) a repeat test for CHT should be offered at 28 days or age or discharge home whichever is sooner.
If blood spot results arrive with an indication that the baby has had a blood transfusion then a repeat test must be scheduled 72 hours after the last transfusion for phenylketonuria, congenital hypothyroidism, cystic fibrosis and MCADD, HCU, MSUD, GA1 and IVA.
DNA testing for SCD at 72 hours, replaces the need for 4 months repeat test after last transfusion (status codes 10 added for screen positive infants identified by DNA).
To support the above processes the CO must be able to produce reports in accordance with Maternity Commissioning Contract.
The screening laboratory has the responsibility for initiating clinical follow-up in the event of adverse investigation results.
However, the CO has the responsibility for informing parents in the event of normal investigation results and for forwarding all results to the child’s allocated health visitor.
The CO should record the date on which the parents are sent the test results, whether normal or abnormal.
If notification is performed by letter (as is usually the case when all results are normal), the date recorded should be the letter’s dispatch date.
In the event that the “sample received” notification has not been received by date of birth (achievable standard), the CO must be able to identify this situation and expedite the process in a timely fashion.
In the event that a repeat sample is not required and a terminal status code (i.e. “Declined”, “Not suspected”, “Carrier”, “Carrier of other haemoglobin”, “Not suspected other disorders follow up”, “Suspected“, “Not screened/screening incomplete”) has not been received for all of the nine conditions by 17 days and up to 1 year from the child’s date of birth (acceptable standard), or 14 days and up to 1 year from the child’s date of birth (achievable standard), the CO must be able to identify this situation and expedite the process in a timely fashion.
These requirements can be met by implementing daily reports as specified in Maternity Commissioning Contract.
This section provides details of the interfaces that the CO must put in place with external systems and organisations as shown in Diagram 4 above.
COs should be able to receive screening results from the screening laboratory that use the nationally agreed status codes as described in reference 8.3.
Suppliers must conform to the latest version of status codes, COs should be able to receive screening results as Spine messages as described in reference 8.1.
COs should plan for an extended period during which results could be received either electronically or on paper depending on which screening laboratory is involved.
COs should use the nationally agreed template for the letter to parents for normal test results.
COs should comply with the normal test results letter production criteria, which is listed in the appendix of reference 8.2.
PHE has developed an ITK Newborn Screening Messaging containing Blood Spot, Hearing and NIPE outcome results to CHIS to send information about newborn blood screening to CHIS and encourages commissioners and providers to include this in contracts and to adopt this into processes.
COs must use the messages defined in reference 8.4 to store blood spot screening results against a child’s record on the shared child health record, and to send all subsequent updates.
Blood spot data is collected quarterly by the National Screening Committee and annually by the Newborn Blood Spot Screening Programme in the form of Excel spreadsheets.
The CHIS should be able to report all data items as outlined in the Blood Spot Data Specification as referenced in section 8.2 above.
In the event of a screen positive result, the child’s registered GP practice should be informed of the results.
For the purposes of this document the health visitor is considered to be external to the CO as they often use systems that are not under the direct control of the CO and this dictates the means of communicating with them.
The CO must forward all results to the child’s allocated health visitor.
COs have a statutory requirement to ensure that all of the children that it is responsible for, up to their first birthday, have had or are offered a newborn blood spot screen.
This includes movers in from other parts of England, the other home countries, and from other countries.
The system must act as a local failsafe for enabling liaison reflecting national standards where applicable.
The system must be able to record results of cystic fibrosis screening.
The four month exception will not apply if the child’s record holds a result obtained via a DNA test for sickle).
The initial implementation of messaging for blood spot results does not include the ability to withdraw or amend results.
This facility is being considered for a later phase.
Such a feature would obviate the need for the manual processes that are currently involved in carrying out these activities.
The requirements in this section relate specifically to Newborn Hearing screening aspects of the Child Health Information Service which are over and above the general functional requirements outlined in Chapter 4 of this document, which must be read prior to consideration of this chapter.
The Newborn Hearing Screening Programme aims to screen all newborn babies in England up to 3 months of age to identify moderate, severe and profound hearing impairment.
Screening is carried out by the NHSP team using the NHSP screening management IT system to manage the screen, carry out failsafe checks and record screening results and outcomes.
Currently there is no automatic messaging of outcomes from the NHSP Information System to CHIS.
In some areas hearing screening results are manually added to CHIS but in others this is not.
At publication of this document, PHE had developed a Newborn Screening ITK Message which includes a minimum data set from the NHSP Information System to CHIS and would encourage commissioners and providers to include this within contracts and in their relevant processes.
The CO must ensure that all the children for whom they are responsible are offered the Newborn Hearing Screening Programme.
Screening should ideally be completed by 4-5 weeks of age in most cases and by 3 months in all cases (with age corrected for prematurity).
If the failsafe report indicates that the process has not been completed arrangements must be in place to complete the process.
The aim is to complete screening by 4-5 weeks of age in most cases and by 3 months in all cases (with an exception for babies that have not reached 44 weeks gestational age).
Screening teams will need to ensure that parents are provided with information about how to seek assessment in the event of future concern.
The health visitor or GP should discuss with parents the implications of not having, or of not completing the screen.
Parents and health visitors or GPs should be made aware that they can request an audiological assessment at any time.
The report should indicate the process status for each child in the list, eg “no action taken”.
The following data is currently made available to child health information systems through electronic interfaces with the existing systems provider or through manual data entry.
PHE has developed a Newborn Screening ITK Message which includes a minimum data set from the NHSP Information System to CHISs and would encourage commissioners and providers to include this within contracts and in their relevant processes.
However, interoperability between the two systems needs to be part of local development plans.
In addition to the generic baby identifier and demographic details the information needs to be held locally with the child’s record and is currently entered onto the PCHR.
All children should have had or been offered a newborn hearing screen by 3 months of age (corrected for prematurity).
This includes movers in from the other home countries, and from other countries.
The primary screening process is managed by the NHSP IT system, but the CO still has responsibility for ensuring coverage of all of the children for which it is responsible, and assuring the quality of the service.
The system must provide a report of those children that do not have results of a hearing screening test recorded against their record.
ITK messaging specification for newborn screening: messaging newborn hearing, NIPE and Bloodspot screening results to the CHIS.
The quality and extent of handover of care from one health professional or organisation to another is a common source of adverse safety issues.
This chapter describes the process for an orderly transfer of responsibility for a baby’s care from the community midwifery team to the CO and Health Visitor team.
The Child Health Information Service system should be able to receive information collected during a woman’s maternity care from the discharge and handover process that is relevant to the child’s health and wellbeing, including the universal antenatal contact by health visitors.
This will help facilitate care for the mother and the child within the community.
This chapter outlines requirements which are over and above the general functional requirements for a CHISs detailed in Chapter 4, which must be read prior to consideration of this chapter.
It is recognised that at the time of publication of this document, that no national dataset for this handover had been agreed, and that the exchange of information between maternity information systems and Child Health Information Service systems is varied, incomplete and dependant on local arrangements.
However it is recommended that the development of these requirements takes place in the future [see CHISMW001].
The CO must receive and store antenatal and neonatal information from the maternity unit.
This is largely screening data, but includes infections, and some immunisations and health promotion and social support interventions.
In some cases, the CO must then be able to act on the information it has received.
For example, hepatitis B and tuberculosis immunisations need to be scheduled based on the antenatal and neonatal information that is shared, and the schedule should enable this to be tracked where required, e.g. for serology testing at one year for hepatitis B.
In addition, it is expected that a number of health promotion activities take place before the CO assumes responsibility for the child.
Immunisation schedule and advice, including noting whether next vaccine is due at 1 month (hepatitis B) or starting from routine schedule at 2 months.
The CO should ensure that the hospital discharge summary is sent to the GP and health visitor.
This information should include the outcomes of the screening programmes listed above, and that it includes the mother’s hepatitis B status.
At the time of this documents publication, a female genital mutilation (FGM) prevalence dataset is currently being collected from Trusts, and will run until March 2015.
At the moment this is not part of CHISs but may be a future requirement and a data item recorded in the future safeguarding dataset.
Further details can be found via http://www.hscic.gov.uk/catalogue/PUB15711 and http://www.isb.nhs.uk/documents/isb-1610/.
From April 2015 an enhanced FGM dataset collection is due to be launched.
This will also cover FGM that has already taken place.
In the future, it is intended that the FGM dataset will be extended to also cover potential risk of FGM, and FGM that is yet to happen.
This requirements in this chapter relate principally to the health promotion and protection activities that are scheduled for delivery to the child from 14 days onwards as currently commissioned by the CO.
These aspects of the Child Health Information Service are over and above the general functional requirements outlined in Chapter 4, which must also be read prior to consideration of this chapter.
This commissioning responsibility will be transferring to local authority commissioners from the October conjunction with parents, and the ‘Red Book’ also known as the personal child health record (PCHR).
Some other topics such as screening may also come under the heading of health promotion, but they are covered in other specific chapters.
For Universal Services for the various age groups between 0– 9 years of age the progressive categories of children and families identified through the programme will have additional health promotion services led by a number of different professional contact groups including health visitor and school nurses to address particular needs and risks.
In addition, the face–to-face contacts within the immunisation programme offer the opportunity for reviews to take place so that all children will have reviews at these points.
All reviews are universally offered but may lead to a common assessment framework (CAF) being conducted, where information gathered is based on CAF domains.
From October 2015 five core stages of the Healthy Child Programme will be mandated for local authority commissioners these are the initial antenatal visit nd new baby reviews, and the subsequent child development assessments which take place at at 6-8 weeks, at one year and then at 2 to 2.5 years of age.
Reference 11.1 provides COs with a standard description of the full range of health promotion services that should be offered to pre-school children.
A distinction is made in the referenced documents to services provided on a universal basis, i.e. all services to all children, and a progressive basis, i.e. some services to some children that have been identified with particular needs.
This section describes how information systems should be used to support the delivery of the universal set of services.
For the progressive categories of children and families identified through the programme there will be additional health promotion services led by health visitor contacts to address particular needs and risks.
The schedule for delivering universal services and healthy child programme deliverables from 0-5 years is shown in table 3.
The ASQ Ages and Stages questionnaire tool can be used for developmental and social-emotional screening for children from one month to 5 ½ years.
In addition to these universal contacts, there needs to be functionality in the CHIS system to capture health promotion activity from all health and care contacts, including for example details of any dental care delivered, and any optometry screening conducted prior to school entry.
References in section 11.2 above provide COs with a standard description of the full range of health promotion services that should be offered to children who have started at school.
A distinction is made in the referenced documents to services provided on a universal basis, i.e. all services to all children, and a progressive basis, ie some services to some children that have been identified with particular needs.
This section describes how information systems should be used to support the delivery of the universal set of services.
For the progressive categories of children and families identified through the programme there will be additional health promotion services led by school nurse contacts to address particular needs and risks.
The schedule for delivering universal services is shown in table 4.
Reference 11.2 provides COs with a standard description of the full range of health promotion services that should be offered to young people aged between 11 and 16.
A distinction is made in the referenced documents to services provided on a universal basis, ie all services to all children, and a progressive basis, ie some services to some children that have been identified with particular needs.
This section describes how information systems should be used to support the delivery of the universal set of services.
For the progressive categories of children and families identified through the programme there will be additional health promotion services led by school nurse contacts to address particular needs and risks.
The schedule for delivering universal services is shown in table 5.
Reference 11.2 provides COs with a standard description of the full range of health promotion services that should be offered to young people aged between 16 and 19.
A distinction is made in the referenced documents to services provided on a universal basis, ie all services to all children, and a progressive basis, i.e. some services to some children that have been identified with particular needs.
This section describes how information systems should be used to support the delivery of the universal set of services.
For the progressive categories of children and families identified through the programme there will be additional health promotion services led by school nurse contacts to address particular needs and risks.
The schedule for delivering universal services is shown in table 6.
From October 2015 five core stages of the Healthy Child Programme will be mandated for local authority commissioners, these include the initial antenatal visit plus the new baby review as well as health promotion opportunities outlined below.
In addition, the face to face contacts within the immunisation programme offer the opportunity for reviews to take place.
At each review the CO must have the facility for records to be made.
All above reviews are universally offered but may lead to a common assessment framework (CAF) being conducted so that information gathered is based on CAF domains.
All contacts information about breastfeeding status should be recorded until age six months.
At each contact weight and at 6-8 weeks head circumference measurements should be made and plotted on centile charts.
The CO needs to be able to record that an intervention has taken place, and record items which will be permanent health biographical records for each child.
Each healthcare professional requires the facility to record relevant core items for each child for the universal programme in whatever service this has taken place and even if this is merely a review that has taken place with no significant items which justify a detailed record being made.
The CO must provide the ability to record s in GP surgeries and community child health clinics, health centres and during hospital interventions, screening, immunisations and vaccinations encounters, and also for ad hoc interventions that may take place at home or in nursery or children’s centres.
For a few children an assessment will be performed under the CAF (see reference number of “domains” as detailed in table 7 below.
The hearing category may also require fields to record audiological measurements for each ear.
The system must be capable of generating the documentation for the particular assessment or review that is due, and relevant records must be available.
This must include the results of previous assessments and any diagnoses held on the child’s record.
The system must provide facilities to record and report the process within health and social care to meet statutory responsibilities in providing information to the local authority detailed in the Education Act 1996.
CO level, incorporating local and nationally defined quality measures where applicable.
The system needs to provide the ability to monitor the quality and coverage of the Healthy Child Programme delivery at CO level and to provide some information needed for secondary uses purposes including whatever key outcome measures are selected.
The requirements in this chapter relate specifically to the Immunisation Programme aspects of the Child Health Information Service, to enable data sharing to a national standard and the transmission of vaccine coverage data information to PHE in (http://www.isb.nhs.uk/docu,ments/isb-0089/amb-8-2014/index_html).
It also offers immunoglobulins for passive immunity and ad hoc vaccinations eg for travel.
This programme will continue to be reviewed by JCVI and may extend beyond 2019.
Further details can be found via: http://www.isb.nhs.uk/documents/isb-0089/amd-8-2014/index_html A new annual adolescent immunisation data collection was also being considered at the time of publication of this document.
The CO must be able to identify which children need to receive which immunisations, based on the guidance in the Public Health England Green Book (reference 12.1).
In addition, the CO must be able to identify children who need to receive specific immunisations because their immunisation status is either unknown or incomplete, or their requirements are distinct and additional from the rest of the population (eg hepatitis B, or a child with asthma see reference 12.5).
This information will be available in the child’s clinical record.
The CO should follow the schedule of ideal ages for the routine immunisation of children in accordance with the Public Health England Green Book (reference 12.1).
The CO must be able to update the personal child health record on the shared child health record as described in the “External interfaces” section below.
The system should be capable of supporting a variation to the immunisation schedules to reflect clinical need and risk factors.
Such a variation may be via national guidance due to public health priorities Alternatively, variant schedules may be chosen because of risk factors, eg a child who is at risk of measles (e.g. due to exposure) might be given an earlier immunisation.
The Child Health Information Service system CHISs needs functionality to schedule and run catch-up programmes, including for example, where children are not available for the initial vaccination date at school.
In the event that a child’s vaccinations or immunisations are missed, late or unknown the CO must take action to regularise the situation as described by Public Health England in reference 12.5.
It is essential that CHISs systems are able to accommodate changes to the national vaccine programme as new vaccines and clinical evidence becomes available.
This may include the introduction of new vaccines, or changes to the schedules for existing vaccinations.
Changes are announced via Tripartite letters with the detail of the clinical guidelines outlined in the Green Book.
The response to this requirement needs to be achieved in a timely manner.
It is important that the design of CHISs allows sufficient flexibility so that changes to the schedule of existing immunisations (see COVER ISN 2014) or the addition of new immunisations for children of any age can be made in a straightforward and timely manner.
The system must be flexible enough to allow an internal schedule to be set according to local priorities and locally defined populations.
For example, the requirement to call children for a measles, mumps and rubella (MMR) catch-up by setting up a schedule which calls the youngest children with no MMR first and older children with only one MMR later.
The system should be capable of inviting children for immunisations, and also able to allocate appointments in the CO clinics and schools or on behalf of practices.
It must be able to produce locally adaptable editable letters for transmission in mailer or short message service form and flexible clinic scheduling.
Vaccine information should be recorded in a consistent manner to ensure continuity of information and safe governance across systems, for example, whether it is the name or the product that is recorded.
This will help ensure continuity across the country as children move between areas and different services record immunisations onto the child’s record.
The system must record vaccines in line with nationally agreed naming and utilisation conventions.
The national data dictionary codes for vaccines should be used when recording on CHIS30.
These codes are nationally agreed, and are compliant with COVER and the Maternal and Children’s Dataset ISNs.
It must also be possible to retrospectively record immunisations given, including those given abroad.
It must be possible to record as much or as little information that is available about immunisations given elsewhere, including an indicator of how certain the information is, e.g. documentary evidence exists, or unvalidated statement by parent.
The CO must record any refusal by a parent/guardian or young person to be immunised, including the reason given.
This may include the entire programme or an individual antigen under which circumstances it may be appropriate to suspend routine scheduling but allow for subsequent follow-up.
The system should allow a deferral; for example holiday, for which rescheduling at later date is required.
The CO must be able to identify all children whose immunisation(s) have been refused.
The system must record did-not-attends and generate associated actions including appropriate prioritisation according to local rules.
The CO must maintain records of vaccine issues regarding quality, potential vaccine failures, and adverse reactions.
In order to fulfil this requirement, COs must record the antigen, batch number and vaccinator for every immunisation dose administered.
The system should permit the user to record the contraindication resulting in the immunisation not being given as scheduled/planned.
The system must allow the user to record the subsequent action to be taken.
Children may receive doses of vaccines that are not scheduled.
These may be extra doses of scheduled vaccines e.g. an “extra dose” of a tetanus containing vaccine given in A&E.
Alternatively the child may be given one or more doses of other vaccines which are currently available in the UK but not included in the routine immunisation schedule, or which are available and given elsewhere in the world but not available in the UK.
Any vaccines given should be recorded in the same format as the scheduled vaccines so that it is possible to work out age specific vaccination status.
Certain children require additional vaccines or doses according to identified risk factors.
The system must allow the recording of specific indicators of need and the flexibility to produce a resultant individual schedule, for example neonatal hepatitis B post exposure prophylaxis, pneumococcal and influenza immunisations.
It is essential that all these additional doses are clearly recorded as that may change future scheduling.
The CO must maintain records of immunisations and vaccinations carried out by GP practices so that claims from GP practices can be confirmed.
The system must be flexible to be able to generate reports for front end users based on any data field combinations and local population configurations.
The CO must be able to submit central returns for the immunisation programme as described in the “External interfaces” section, in line with current ISNs.
The CHIS must reflect the specification of the CSV file layout which is available on the Open Exeter on-screen help page.
The CHISs system must comply with information data standards to enable the submission of immunisation data to the MCDS [HSCIC].
Commissioning organisations should ensure local NHS contracts for children’s services identify within the information specification or the data items required to be collected and reported.
The provider of the CHISs is responsible for ensuring all those data items are recorded and complete.
For example, to ensure that all of the children, up to the age of 19, have had or are offered immunisations in accordance with the Public Health England Green Book (reference 12.1).
Book (ref 12.1 Appendix A) Tri-partite letters and ISNs.
The list should include details of the child's registered GP Community Paediatric service where available.
Guidance for immunisation and vaccination in England is produced by Public Health England and informed by advice and recommendations of the Joint Committee on Vaccination and Immunisation and set out in the Green Book (reference 12.1).
While the list of immunisations is correct at the point of publication, immunisations are added on an unscheduled basis, and so the ultimate reference must be the Green Book.
The latest COVER ISN should also be referred to, to provide details for the data collections required.
While there are already codes for vaccine brands and types, codes should also exist for single antigens (as these are available in other countries), and should be able to distinguish between subtly different vaccination names.
In addition it will be helpful if the notion of different codes for 1st, 2nd, 3rd, “additional’, “booster” and “reinforcing” doses etc.
The vaccinations given should simply be recorded and the system should be intelligent enough to add up the number of doses against each disease that a child has had, know what it should have had by a certain age and appoint or prompt if too few or too many, have or may be being offered taking into account possible additional immunisations required for some individuals e.g. in defined clinical risk groups.
It should be d that some conjugate vaccines are age dependent and so the number of doses required will vary with age.
This information is given in tripartite letters introducing new vaccines and/or revised schedules for existing vaccines and is in the Green Book.
Immunisations are added on an unscheduled basis, and so the ultimate reference must be the Green Book (reference 12.1).
It is therefore important that the design of CHISs allows sufficient flexibility so that changes to the schedule of existing immunisations or the addition of new immunisations can be made in a straightforward and timely manner in line with current ISNs.
A number of vaccines providing protection for the same diseases are available from different manufacturers.
It would be beneficial if the CHIS had the facility to record the product name, batch number and expiry date and manufacturer source for immunisations delivered.
The National Child Measurement Programme (NCMP), and are over and above the general functional requirements outlined in Chapter 4 of this document, which must be read prior to consideration of this chapter.
The NCMP is a mandatory public heath function of all local authorities in England.
The programme involves measuring the height and weight of Reception and Year 6 children at state-maintained schools.
This chapter details requirements which are over and above the general functional requirements outlined in Chapter 4 of this document, which must be read prior to consideration of this chapter.
The system must validate the data at the point of entry.
The requirements in this chapter relate specifically to the information and data exchange between health and care professionals and the Child Health Information Service to support effective delivery of health care for Looked after Children.
The information processes outlined below are based upon the services that children who are of ‘looked after’ status need, including health assessments on entry to statutory care, and then regular reviews.
Looked after children are recognised to have greater health needs than the general population35 and that these needs may also be more difficult to meet, in view of the mobility of some young people and a change of carers.
Their health biography can be more difficult to trace and thus needs to be carefully recorded and shared with those who have responsibility for them and also with the child themselves.
Although the primary responsibility lies with the social care service for reviews of status, the health needs of the child should be met and monitored by health services.
This applies also to any children placed for adoption where there are long-term health needs or background family history to be taken into account.
After placement for adoption healthcare plans are in place in the same way as for other children looked after until the final adoption order is made.
The system should ensure that any healthcare professional involved in any formal reviews of or delivery of care to a looked after child has visibility of such status as well as of the presence of a Child Protection Plan, and any identified special needs.
Such data may be more reliable with the introduction of the Child Protection Information Sharing Project (CPIS).
From those with parental responsibility and when relevant child/young person: agreement about how much to share with carer.
The CO must be able to perform the statutory additional reporting required for looked after children.
It must also be possible to report whether the child or young person is up to date with immunisations, particularly under national requirements relating to children continuously in local authority care between specified dates.
The requirements in this chapter relate specifically to the information and data exchange between health and care professionals and the Child Health Information Service to support effective delivery of health care for disabled children and young people and children with long-term care needs.
This chapter also details the CHIS service related interactions for children with special educational needs requiring support.
The CO’s information systems must support the planning and delivery of services, over and above those designed for the population as a whole, to disabled children and young people and or those with complex healthcare needs or special educational needs and require an Education Health and Care Plan [EHC] .
The CO’s information systems must maintain a full record of the assessment, care plan, treatment or interventions and outcomes.
They must enable the user to comprehensively monitor a child’s progress through the assessment of special educational needs procedures.
The system must offer the facility to audit the process and timescale against assessment of educational needs and standards.
All children that the CO is responsible for and who are either registered as disabled or have complex healthcare needs or who have special educational needs.
The service will contain personal sensitive information.
The purpose of the use of standards is to drive interoperability administration systems.
The architecture for all system components must conform to the principles and outline design described in this module.
Each supplier must describe how its proposed architecture supports the primary objects outlined in section 1.6 which contributes towards child public health services.
Each supplier must provide an outline implementation plan, including any dependencies, showing high level resource requirements, both from the IT supplier and the provider of public health services.
Each supplier must describe any proposals for initial data take on (including data cleansing) for the service.
It must be possible for user administrators to disable access for a user account.
In this case, the user shall no longer be shall no longer be available to other users.
The system must allow for the correction of information that correcting and deleting of information.
The system must be able to provide the facility to record why date the information was released, by and to whom.
Much of the information in the shared child health record will also appear in the PCHR.
Greater detail about the child’s healthcare would be available in local systems for those health professionals who require it and this is defined for each screening programme in the relevant section of the report.
