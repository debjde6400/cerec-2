{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to D:\\Program\n",
      "[nltk_data]     Files\\Anaconda3\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to D:\\Program\n",
      "[nltk_data]     Files\\Anaconda3\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tagged Sentences  3914\n"
     ]
    }
   ],
   "source": [
    "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
    "print(\"Number of Tagged Sentences \", len(tagged_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ix = index\n",
    "def word_to_ix(word, ix):\n",
    "    return torch.tensor(ix[word], dtype = torch.long)\n",
    "\n",
    "def char_to_ix(char, ix):\n",
    "    return torch.tensor(ix[char], dtype = torch.long)\n",
    "\n",
    "def tag_to_ix(tag, ix):\n",
    "    return torch.tensor(ix[word], dtype = torch.long)\n",
    "\n",
    "def sequence_to_idx(sequence, ix):\n",
    "    return torch.tensor([ix[s] for s in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {}\n",
    "tag_to_idx = {}\n",
    "char_to_idx = {}\n",
    "\n",
    "for sentence in tagged_sentence:\n",
    "    for word, pos_tag in sentence:\n",
    "        if word not in word_to_idx.keys():\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "        if pos_tag not in tag_to_idx.keys():\n",
    "            tag_to_idx[pos_tag] = len(tag_to_idx)\n",
    "        for char in word:\n",
    "            if char not in char_to_idx.keys():\n",
    "                char_to_idx[char] = len(char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pierre': 0,\n",
       " 'Vinken': 1,\n",
       " ',': 2,\n",
       " '61': 3,\n",
       " 'years': 4,\n",
       " 'old': 5,\n",
       " 'will': 6,\n",
       " 'join': 7,\n",
       " 'the': 8,\n",
       " 'board': 9,\n",
       " 'as': 10,\n",
       " 'a': 11,\n",
       " 'nonexecutive': 12,\n",
       " 'director': 13,\n",
       " 'Nov.': 14,\n",
       " '29': 15,\n",
       " '.': 16,\n",
       " 'Mr.': 17,\n",
       " 'is': 18,\n",
       " 'chairman': 19,\n",
       " 'of': 20,\n",
       " 'Elsevier': 21,\n",
       " 'N.V.': 22,\n",
       " 'Dutch': 23,\n",
       " 'publishing': 24,\n",
       " 'group': 25,\n",
       " 'Rudolph': 26,\n",
       " 'Agnew': 27,\n",
       " '55': 28,\n",
       " 'and': 29,\n",
       " 'former': 30,\n",
       " 'Consolidated': 31,\n",
       " 'Gold': 32,\n",
       " 'Fields': 33,\n",
       " 'PLC': 34,\n",
       " 'was': 35,\n",
       " 'named': 36,\n",
       " '*-1': 37,\n",
       " 'this': 38,\n",
       " 'British': 39,\n",
       " 'industrial': 40,\n",
       " 'conglomerate': 41,\n",
       " 'A': 42,\n",
       " 'form': 43,\n",
       " 'asbestos': 44,\n",
       " 'once': 45,\n",
       " 'used': 46,\n",
       " '*': 47,\n",
       " 'to': 48,\n",
       " 'make': 49,\n",
       " 'Kent': 50,\n",
       " 'cigarette': 51,\n",
       " 'filters': 52,\n",
       " 'has': 53,\n",
       " 'caused': 54,\n",
       " 'high': 55,\n",
       " 'percentage': 56,\n",
       " 'cancer': 57,\n",
       " 'deaths': 58,\n",
       " 'among': 59,\n",
       " 'workers': 60,\n",
       " 'exposed': 61,\n",
       " 'it': 62,\n",
       " 'more': 63,\n",
       " 'than': 64,\n",
       " '30': 65,\n",
       " 'ago': 66,\n",
       " 'researchers': 67,\n",
       " 'reported': 68,\n",
       " '0': 69,\n",
       " '*T*-1': 70,\n",
       " 'The': 71,\n",
       " 'fiber': 72,\n",
       " 'crocidolite': 73,\n",
       " 'unusually': 74,\n",
       " 'resilient': 75,\n",
       " 'enters': 76,\n",
       " 'lungs': 77,\n",
       " 'with': 78,\n",
       " 'even': 79,\n",
       " 'brief': 80,\n",
       " 'exposures': 81,\n",
       " 'causing': 82,\n",
       " 'symptoms': 83,\n",
       " 'that': 84,\n",
       " 'show': 85,\n",
       " 'up': 86,\n",
       " 'decades': 87,\n",
       " 'later': 88,\n",
       " 'said': 89,\n",
       " '*T*-2': 90,\n",
       " 'Lorillard': 91,\n",
       " 'Inc.': 92,\n",
       " 'unit': 93,\n",
       " 'New': 94,\n",
       " 'York-based': 95,\n",
       " 'Loews': 96,\n",
       " 'Corp.': 97,\n",
       " 'makes': 98,\n",
       " 'cigarettes': 99,\n",
       " 'stopped': 100,\n",
       " 'using': 101,\n",
       " 'in': 102,\n",
       " 'its': 103,\n",
       " 'Micronite': 104,\n",
       " '1956': 105,\n",
       " 'Although': 106,\n",
       " 'preliminary': 107,\n",
       " 'findings': 108,\n",
       " 'were': 109,\n",
       " '*-2': 110,\n",
       " 'year': 111,\n",
       " 'latest': 112,\n",
       " 'results': 113,\n",
       " 'appear': 114,\n",
       " 'today': 115,\n",
       " \"'s\": 116,\n",
       " 'England': 117,\n",
       " 'Journal': 118,\n",
       " 'Medicine': 119,\n",
       " 'forum': 120,\n",
       " 'likely': 121,\n",
       " 'bring': 122,\n",
       " 'new': 123,\n",
       " 'attention': 124,\n",
       " 'problem': 125,\n",
       " 'spokewoman': 126,\n",
       " '``': 127,\n",
       " 'This': 128,\n",
       " 'an': 129,\n",
       " 'story': 130,\n",
       " 'We': 131,\n",
       " \"'re\": 132,\n",
       " 'talking': 133,\n",
       " 'about': 134,\n",
       " 'before': 135,\n",
       " 'anyone': 136,\n",
       " 'heard': 137,\n",
       " 'having': 138,\n",
       " 'any': 139,\n",
       " 'questionable': 140,\n",
       " 'properties': 141,\n",
       " 'There': 142,\n",
       " 'no': 143,\n",
       " 'our': 144,\n",
       " 'products': 145,\n",
       " 'now': 146,\n",
       " \"''\": 147,\n",
       " 'Neither': 148,\n",
       " 'nor': 149,\n",
       " 'who': 150,\n",
       " '*T*-3': 151,\n",
       " 'studied': 152,\n",
       " 'aware': 153,\n",
       " 'research': 154,\n",
       " 'on': 155,\n",
       " 'smokers': 156,\n",
       " 'have': 157,\n",
       " 'useful': 158,\n",
       " 'information': 159,\n",
       " 'whether': 160,\n",
       " 'users': 161,\n",
       " 'are': 162,\n",
       " 'at': 163,\n",
       " 'risk': 164,\n",
       " 'James': 165,\n",
       " 'A.': 166,\n",
       " 'Talcott': 167,\n",
       " 'Boston': 168,\n",
       " 'Dana-Farber': 169,\n",
       " 'Cancer': 170,\n",
       " 'Institute': 171,\n",
       " 'Dr.': 172,\n",
       " 'led': 173,\n",
       " 'team': 174,\n",
       " 'from': 175,\n",
       " 'National': 176,\n",
       " 'medical': 177,\n",
       " 'schools': 178,\n",
       " 'Harvard': 179,\n",
       " 'University': 180,\n",
       " 'spokeswoman': 181,\n",
       " 'very': 182,\n",
       " 'modest': 183,\n",
       " 'amounts': 184,\n",
       " 'making': 185,\n",
       " 'paper': 186,\n",
       " 'for': 187,\n",
       " 'early': 188,\n",
       " '1950s': 189,\n",
       " 'replaced': 190,\n",
       " 'different': 191,\n",
       " 'type': 192,\n",
       " 'filter': 193,\n",
       " 'From': 194,\n",
       " '1953': 195,\n",
       " '1955': 196,\n",
       " '9.8': 197,\n",
       " 'billion': 198,\n",
       " 'sold': 199,\n",
       " '*-3': 200,\n",
       " 'company': 201,\n",
       " 'Among': 202,\n",
       " '33': 203,\n",
       " 'men': 204,\n",
       " '*T*-4': 205,\n",
       " 'worked': 206,\n",
       " 'closely': 207,\n",
       " 'substance': 208,\n",
       " '28': 209,\n",
       " '*ICH*-1': 210,\n",
       " 'died': 211,\n",
       " '--': 212,\n",
       " 'three': 213,\n",
       " 'times': 214,\n",
       " 'expected': 215,\n",
       " 'number': 216,\n",
       " 'Four': 217,\n",
       " 'five': 218,\n",
       " 'surviving': 219,\n",
       " 'asbestos-related': 220,\n",
       " 'diseases': 221,\n",
       " 'including': 222,\n",
       " 'recently': 223,\n",
       " 'diagnosed': 224,\n",
       " 'total': 225,\n",
       " '18': 226,\n",
       " 'malignant': 227,\n",
       " 'mesothelioma': 228,\n",
       " 'lung': 229,\n",
       " 'asbestosis': 230,\n",
       " 'far': 231,\n",
       " 'higher': 232,\n",
       " '*?*': 233,\n",
       " 'morbidity': 234,\n",
       " 'rate': 235,\n",
       " 'striking': 236,\n",
       " 'finding': 237,\n",
       " 'those': 238,\n",
       " 'us': 239,\n",
       " '*T*-5': 240,\n",
       " 'study': 241,\n",
       " 'West': 242,\n",
       " 'Groton': 243,\n",
       " 'Mass.': 244,\n",
       " 'factory': 245,\n",
       " 'appears': 246,\n",
       " 'be': 247,\n",
       " 'highest': 248,\n",
       " 'Western': 249,\n",
       " 'industrialized': 250,\n",
       " 'countries': 251,\n",
       " 'he': 252,\n",
       " 'plant': 253,\n",
       " 'which': 254,\n",
       " 'owned': 255,\n",
       " '*-4': 256,\n",
       " 'by': 257,\n",
       " 'Hollingsworth': 258,\n",
       " '&': 259,\n",
       " 'Vose': 260,\n",
       " 'Co.': 261,\n",
       " 'under': 262,\n",
       " 'contract': 263,\n",
       " '*ICH*-2': 264,\n",
       " 'probably': 265,\n",
       " 'support': 266,\n",
       " '*T*-6': 267,\n",
       " 'argue': 268,\n",
       " 'U.S.': 269,\n",
       " 'should': 270,\n",
       " 'regulate': 271,\n",
       " 'class': 272,\n",
       " 'stringently': 273,\n",
       " 'common': 274,\n",
       " 'kind': 275,\n",
       " 'chrysotile': 276,\n",
       " 'found': 277,\n",
       " 'most': 278,\n",
       " 'other': 279,\n",
       " 'buildings': 280,\n",
       " 'one': 281,\n",
       " 'few': 282,\n",
       " 'nations': 283,\n",
       " '*T*-7': 284,\n",
       " 'does': 285,\n",
       " \"n't\": 286,\n",
       " 'standard': 287,\n",
       " 'regulation': 288,\n",
       " 'smooth': 289,\n",
       " 'needle-like': 290,\n",
       " 'fibers': 291,\n",
       " 'such': 292,\n",
       " 'classified': 293,\n",
       " '*-5': 294,\n",
       " 'amphobiles': 295,\n",
       " 'according': 296,\n",
       " 'Brooke': 297,\n",
       " 'T.': 298,\n",
       " 'Mossman': 299,\n",
       " 'professor': 300,\n",
       " 'pathlogy': 301,\n",
       " 'Vermont': 302,\n",
       " 'College': 303,\n",
       " 'More': 304,\n",
       " 'curly': 305,\n",
       " 'easily': 306,\n",
       " 'rejected': 307,\n",
       " 'body': 308,\n",
       " 'explained': 309,\n",
       " 'In': 310,\n",
       " 'July': 311,\n",
       " 'Environmental': 312,\n",
       " 'Protection': 313,\n",
       " 'Agency': 314,\n",
       " 'imposed': 315,\n",
       " 'gradual': 316,\n",
       " 'ban': 317,\n",
       " 'virtually': 318,\n",
       " 'all': 319,\n",
       " 'uses': 320,\n",
       " 'By': 321,\n",
       " '1997': 322,\n",
       " 'almost': 323,\n",
       " 'remaining': 324,\n",
       " 'cancer-causing': 325,\n",
       " 'outlawed': 326,\n",
       " '*-6': 327,\n",
       " 'About': 328,\n",
       " '160': 329,\n",
       " '*T*-8': 330,\n",
       " 'made': 331,\n",
       " '*-7': 332,\n",
       " 'Areas': 333,\n",
       " 'particularly': 334,\n",
       " 'dusty': 335,\n",
       " 'where': 336,\n",
       " '*-8': 337,\n",
       " 'Workers': 338,\n",
       " 'dumped': 339,\n",
       " 'large': 340,\n",
       " 'burlap': 341,\n",
       " 'sacks': 342,\n",
       " 'imported': 343,\n",
       " 'material': 344,\n",
       " 'into': 345,\n",
       " 'huge': 346,\n",
       " 'bin': 347,\n",
       " 'poured': 348,\n",
       " 'cotton': 349,\n",
       " 'acetate': 350,\n",
       " 'mechanically': 351,\n",
       " 'mixed': 352,\n",
       " 'dry': 353,\n",
       " 'process': 354,\n",
       " 'described': 355,\n",
       " 'clouds': 356,\n",
       " 'blue': 357,\n",
       " 'dust': 358,\n",
       " 'hung': 359,\n",
       " 'over': 360,\n",
       " 'parts': 361,\n",
       " 'though': 362,\n",
       " 'exhaust': 363,\n",
       " 'fans': 364,\n",
       " 'ventilated': 365,\n",
       " 'area': 366,\n",
       " 'question': 367,\n",
       " 'some': 368,\n",
       " 'managers': 369,\n",
       " 'contracted': 370,\n",
       " 'Darrell': 371,\n",
       " 'Phillips': 372,\n",
       " 'vice': 373,\n",
       " 'president': 374,\n",
       " 'human': 375,\n",
       " 'resources': 376,\n",
       " 'But': 377,\n",
       " 'you': 378,\n",
       " 'recognize': 379,\n",
       " 'these': 380,\n",
       " 'events': 381,\n",
       " 'took': 382,\n",
       " 'place': 383,\n",
       " '35': 384,\n",
       " 'It': 385,\n",
       " 'bearing': 386,\n",
       " 'work': 387,\n",
       " 'force': 388,\n",
       " 'Yields': 389,\n",
       " 'money-market': 390,\n",
       " 'mutual': 391,\n",
       " 'funds': 392,\n",
       " 'continued': 393,\n",
       " 'slide': 394,\n",
       " 'amid': 395,\n",
       " 'signs': 396,\n",
       " 'portfolio': 397,\n",
       " 'expect': 398,\n",
       " 'further': 399,\n",
       " 'declines': 400,\n",
       " 'interest': 401,\n",
       " 'rates': 402,\n",
       " 'average': 403,\n",
       " 'seven-day': 404,\n",
       " 'compound': 405,\n",
       " 'yield': 406,\n",
       " '400': 407,\n",
       " 'taxable': 408,\n",
       " 'tracked': 409,\n",
       " 'IBC': 410,\n",
       " 'Money': 411,\n",
       " 'Fund': 412,\n",
       " 'Report': 413,\n",
       " 'eased': 414,\n",
       " 'fraction': 415,\n",
       " 'point': 416,\n",
       " '8.45': 417,\n",
       " '%': 418,\n",
       " '8.47': 419,\n",
       " 'week': 420,\n",
       " 'ended': 421,\n",
       " 'Tuesday': 422,\n",
       " 'Compound': 423,\n",
       " 'yields': 424,\n",
       " 'assume': 425,\n",
       " 'reinvestment': 426,\n",
       " 'dividends': 427,\n",
       " 'current': 428,\n",
       " 'continues': 429,\n",
       " 'Average': 430,\n",
       " 'maturity': 431,\n",
       " \"'\": 432,\n",
       " 'investments': 433,\n",
       " 'lengthened': 434,\n",
       " 'day': 435,\n",
       " '41': 436,\n",
       " 'days': 437,\n",
       " 'longest': 438,\n",
       " 'since': 439,\n",
       " 'August': 440,\n",
       " 'Donoghue': 441,\n",
       " 'Longer': 442,\n",
       " 'maturities': 443,\n",
       " 'thought': 444,\n",
       " 'indicate': 445,\n",
       " 'declining': 446,\n",
       " 'because': 447,\n",
       " 'they': 448,\n",
       " 'permit': 449,\n",
       " 'retain': 450,\n",
       " 'relatively': 451,\n",
       " 'longer': 452,\n",
       " 'period': 453,\n",
       " 'Shorter': 454,\n",
       " 'considered': 455,\n",
       " '*-9': 456,\n",
       " 'sign': 457,\n",
       " 'rising': 458,\n",
       " 'can': 459,\n",
       " 'capture': 460,\n",
       " 'sooner': 461,\n",
       " 'open': 462,\n",
       " 'only': 463,\n",
       " 'institutions': 464,\n",
       " 'stronger': 465,\n",
       " 'indicator': 466,\n",
       " 'watch': 467,\n",
       " 'market': 468,\n",
       " 'reached': 469,\n",
       " 'Nevertheless': 470,\n",
       " 'Brenda': 471,\n",
       " 'Malizia': 472,\n",
       " 'Negus': 473,\n",
       " 'editor': 474,\n",
       " 'may': 475,\n",
       " 'blip': 476,\n",
       " 'again': 477,\n",
       " 'down': 478,\n",
       " 'recent': 479,\n",
       " 'rises': 480,\n",
       " 'short-term': 481,\n",
       " 'six-month': 482,\n",
       " 'Treasury': 483,\n",
       " 'bills': 484,\n",
       " 'Monday': 485,\n",
       " 'auction': 486,\n",
       " 'example': 487,\n",
       " 'rose': 488,\n",
       " '8.04': 489,\n",
       " '7.90': 490,\n",
       " 'Despite': 491,\n",
       " 'investors': 492,\n",
       " 'continue': 493,\n",
       " 'pour': 494,\n",
       " 'cash': 495,\n",
       " 'money': 496,\n",
       " 'Assets': 497,\n",
       " 'grew': 498,\n",
       " '$': 499,\n",
       " '1.5': 500,\n",
       " '*U*': 501,\n",
       " 'during': 502,\n",
       " '352.7': 503,\n",
       " 'Typically': 504,\n",
       " 'money-fund': 505,\n",
       " 'beat': 506,\n",
       " 'comparable': 507,\n",
       " 'vary': 508,\n",
       " 'go': 509,\n",
       " 'after': 510,\n",
       " 'top': 511,\n",
       " 'currently': 512,\n",
       " 'yielding': 513,\n",
       " 'well': 514,\n",
       " '9': 515,\n",
       " 'Dreyfus': 516,\n",
       " 'World-Wide': 517,\n",
       " 'Dollar': 518,\n",
       " 'top-yielding': 519,\n",
       " 'fund': 520,\n",
       " 'had': 521,\n",
       " '9.37': 522,\n",
       " '9.45': 523,\n",
       " 'earlier': 524,\n",
       " 'invests': 525,\n",
       " 'heavily': 526,\n",
       " 'dollar-denominated': 527,\n",
       " 'securities': 528,\n",
       " 'overseas': 529,\n",
       " 'waiving': 530,\n",
       " 'management': 531,\n",
       " 'fees': 532,\n",
       " '*T*-9': 533,\n",
       " 'boosts': 534,\n",
       " 'simple': 535,\n",
       " '8.12': 536,\n",
       " '8.14': 537,\n",
       " '30-day': 538,\n",
       " 'fell': 539,\n",
       " '8.19': 540,\n",
       " '8.22': 541,\n",
       " ';': 542,\n",
       " 'slid': 543,\n",
       " '8.53': 544,\n",
       " '8.56': 545,\n",
       " 'J.P.': 546,\n",
       " 'Bolduc': 547,\n",
       " 'W.R.': 548,\n",
       " 'Grace': 549,\n",
       " '*T*-10': 550,\n",
       " 'holds': 551,\n",
       " '83.4': 552,\n",
       " 'energy-services': 553,\n",
       " 'elected': 554,\n",
       " '*-10': 555,\n",
       " 'He': 556,\n",
       " 'succeeds': 557,\n",
       " 'Terrence': 558,\n",
       " 'D.': 559,\n",
       " 'Daniels': 560,\n",
       " 'formerly': 561,\n",
       " '*T*-11': 562,\n",
       " 'resigned': 563,\n",
       " 'Energy': 564,\n",
       " 'seven': 565,\n",
       " 'seats': 566,\n",
       " 'Pacific': 567,\n",
       " 'First': 568,\n",
       " 'Financial': 569,\n",
       " 'shareholders': 570,\n",
       " 'approved': 571,\n",
       " 'acquisition': 572,\n",
       " 'Royal': 573,\n",
       " 'Trustco': 574,\n",
       " 'Ltd.': 575,\n",
       " 'Toronto': 576,\n",
       " '27': 577,\n",
       " 'share': 578,\n",
       " 'or': 579,\n",
       " '212': 580,\n",
       " 'million': 581,\n",
       " 'thrift': 582,\n",
       " 'holding': 583,\n",
       " 'expects': 584,\n",
       " 'obtain': 585,\n",
       " 'regulatory': 586,\n",
       " 'approval': 587,\n",
       " 'complete': 588,\n",
       " 'transaction': 589,\n",
       " 'year-end': 590,\n",
       " 'McDermott': 591,\n",
       " 'International': 592,\n",
       " 'Babcock': 593,\n",
       " 'Wilcox': 594,\n",
       " 'completed': 595,\n",
       " 'sale': 596,\n",
       " 'Bailey': 597,\n",
       " 'Controls': 598,\n",
       " 'Operations': 599,\n",
       " 'Finmeccanica': 600,\n",
       " 'S.p': 601,\n",
       " '295': 602,\n",
       " 'Italian': 603,\n",
       " 'state-owned': 604,\n",
       " 'interests': 605,\n",
       " 'mechanical': 606,\n",
       " 'engineering': 607,\n",
       " 'industry': 608,\n",
       " 'based': 609,\n",
       " 'Wickliffe': 610,\n",
       " 'Ohio': 611,\n",
       " 'computerized': 612,\n",
       " 'controls': 613,\n",
       " 'systems': 614,\n",
       " 'employs': 615,\n",
       " '2,700': 616,\n",
       " 'people': 617,\n",
       " 'annual': 618,\n",
       " 'revenue': 619,\n",
       " '370': 620,\n",
       " 'federal': 621,\n",
       " 'government': 622,\n",
       " 'suspended': 623,\n",
       " 'sales': 624,\n",
       " 'savings': 625,\n",
       " 'bonds': 626,\n",
       " 'Congress': 627,\n",
       " 'lifted': 628,\n",
       " 'ceiling': 629,\n",
       " 'debt': 630,\n",
       " 'Until': 631,\n",
       " 'acts': 632,\n",
       " 'authority': 633,\n",
       " 'issue': 634,\n",
       " 'obligations': 635,\n",
       " 'borrowing': 636,\n",
       " 'dropped': 637,\n",
       " 'midnight': 638,\n",
       " '2.80': 639,\n",
       " 'trillion': 640,\n",
       " '2.87': 641,\n",
       " 'Legislation': 642,\n",
       " 'lift': 643,\n",
       " 'ensnarled': 644,\n",
       " '*-11': 645,\n",
       " 'fight': 646,\n",
       " 'cutting': 647,\n",
       " 'capital-gains': 648,\n",
       " 'taxes': 649,\n",
       " 'House': 650,\n",
       " 'voted': 651,\n",
       " 'raise': 652,\n",
       " '3.1': 653,\n",
       " 'but': 654,\n",
       " 'Senate': 655,\n",
       " 'act': 656,\n",
       " 'until': 657,\n",
       " 'next': 658,\n",
       " 'earliest': 659,\n",
       " 'default': 660,\n",
       " 'if': 661,\n",
       " 'then': 662,\n",
       " 'Clark': 663,\n",
       " 'J.': 664,\n",
       " 'Vitulli': 665,\n",
       " '*-12': 666,\n",
       " 'senior': 667,\n",
       " '*RNR*-1': 668,\n",
       " 'general': 669,\n",
       " 'manager': 670,\n",
       " 'marketing': 671,\n",
       " 'arm': 672,\n",
       " 'Japanese': 673,\n",
       " 'auto': 674,\n",
       " 'maker': 675,\n",
       " 'Mazda': 676,\n",
       " 'Motor': 677,\n",
       " 'Corp': 678,\n",
       " 'position': 679,\n",
       " 'oversee': 680,\n",
       " 'service': 681,\n",
       " 'operations': 682,\n",
       " 'Previously': 683,\n",
       " '43': 684,\n",
       " 'Chrysler': 685,\n",
       " 'division': 686,\n",
       " 'been': 687,\n",
       " 'executive': 688,\n",
       " '20': 689,\n",
       " 'When': 690,\n",
       " 'time': 691,\n",
       " 'their': 692,\n",
       " 'biannual': 693,\n",
       " 'powwow': 694,\n",
       " 'nation': 695,\n",
       " 'manufacturing': 696,\n",
       " 'titans': 697,\n",
       " 'typically': 698,\n",
       " 'jet': 699,\n",
       " 'off': 700,\n",
       " 'sunny': 701,\n",
       " 'confines': 702,\n",
       " 'resort': 703,\n",
       " 'towns': 704,\n",
       " 'like': 705,\n",
       " 'Boca': 706,\n",
       " 'Raton': 707,\n",
       " 'Hot': 708,\n",
       " 'Springs': 709,\n",
       " 'Not': 710,\n",
       " 'Association': 711,\n",
       " 'Manufacturers': 712,\n",
       " 'settled': 713,\n",
       " 'Hoosier': 714,\n",
       " 'capital': 715,\n",
       " 'Indianapolis': 716,\n",
       " 'fall': 717,\n",
       " 'meeting': 718,\n",
       " 'And': 719,\n",
       " 'city': 720,\n",
       " 'decided': 721,\n",
       " 'treat': 722,\n",
       " 'guests': 723,\n",
       " 'royalty': 724,\n",
       " 'rock': 725,\n",
       " 'stars': 726,\n",
       " 'owners': 727,\n",
       " 'idea': 728,\n",
       " 'course': 729,\n",
       " ':': 730,\n",
       " 'prove': 731,\n",
       " '125': 732,\n",
       " 'corporate': 733,\n",
       " 'decision': 734,\n",
       " 'makers': 735,\n",
       " 'buckle': 736,\n",
       " 'Rust': 737,\n",
       " 'Belt': 738,\n",
       " 'so': 739,\n",
       " 'rusty': 740,\n",
       " 'good': 741,\n",
       " 'expand': 742,\n",
       " 'On': 743,\n",
       " 'receiving': 744,\n",
       " 'end': 745,\n",
       " 'message': 746,\n",
       " 'officials': 747,\n",
       " 'giants': 748,\n",
       " 'Du': 749,\n",
       " 'Pont': 750,\n",
       " 'Maytag': 751,\n",
       " 'along': 752,\n",
       " 'lesser': 753,\n",
       " 'knowns': 754,\n",
       " 'Trojan': 755,\n",
       " 'Steel': 756,\n",
       " 'Valley': 757,\n",
       " 'Queen': 758,\n",
       " 'Cheese': 759,\n",
       " 'Factory': 760,\n",
       " 'For': 761,\n",
       " 'starters': 762,\n",
       " 'executives': 763,\n",
       " 'joined': 764,\n",
       " 'Mayor': 765,\n",
       " 'William': 766,\n",
       " 'H.': 767,\n",
       " 'Hudnut': 768,\n",
       " 'III': 769,\n",
       " 'evening': 770,\n",
       " 'Symphony': 771,\n",
       " 'Orchestra': 772,\n",
       " 'guest': 773,\n",
       " 'pianist-comedian': 774,\n",
       " 'Victor': 775,\n",
       " 'Borge': 776,\n",
       " 'Champagne': 777,\n",
       " 'dessert': 778,\n",
       " 'followed': 779,\n",
       " 'morning': 780,\n",
       " 'police': 781,\n",
       " 'escort': 782,\n",
       " 'busloads': 783,\n",
       " 'wives': 784,\n",
       " 'raced': 785,\n",
       " 'Speedway': 786,\n",
       " 'unimpeded': 787,\n",
       " 'traffic': 788,\n",
       " 'red': 789,\n",
       " 'lights': 790,\n",
       " 'governor': 791,\n",
       " 'could': 792,\n",
       " 'lieutenant': 793,\n",
       " 'welcomed': 794,\n",
       " 'special': 795,\n",
       " 'buffet': 796,\n",
       " 'breakfast': 797,\n",
       " 'held': 798,\n",
       " 'museum': 799,\n",
       " 'food': 800,\n",
       " 'drinks': 801,\n",
       " 'banned': 802,\n",
       " 'everyday': 803,\n",
       " 'visitors': 804,\n",
       " 'Then': 805,\n",
       " 'honor': 806,\n",
       " 'speedway': 807,\n",
       " 'hauled': 808,\n",
       " 'out': 809,\n",
       " 'four': 810,\n",
       " 'drivers': 811,\n",
       " 'crews': 812,\n",
       " 'official': 813,\n",
       " '500': 814,\n",
       " 'announcer': 815,\n",
       " '10-lap': 816,\n",
       " 'exhibition': 817,\n",
       " 'race': 818,\n",
       " 'After': 819,\n",
       " 'Fortune': 820,\n",
       " 'drooled': 821,\n",
       " 'schoolboys': 822,\n",
       " 'cars': 823,\n",
       " 'No': 824,\n",
       " 'dummies': 825,\n",
       " 'pointed': 826,\n",
       " 'still': 827,\n",
       " 'space': 828,\n",
       " 'machines': 829,\n",
       " 'another': 830,\n",
       " 'sponsor': 831,\n",
       " 'name': 832,\n",
       " 'two': 833,\n",
       " 'Back': 834,\n",
       " 'downtown': 835,\n",
       " 'execs': 836,\n",
       " 'squeezed': 837,\n",
       " 'meetings': 838,\n",
       " 'hotel': 839,\n",
       " 'boarding': 840,\n",
       " 'buses': 841,\n",
       " 'dinner': 842,\n",
       " 'dancing': 843,\n",
       " 'block': 844,\n",
       " 'away': 845,\n",
       " 'Under': 846,\n",
       " 'moons': 847,\n",
       " 'renovated': 848,\n",
       " 'Indiana': 849,\n",
       " 'Roof': 850,\n",
       " 'ballroom': 851,\n",
       " 'nine': 852,\n",
       " 'hottest': 853,\n",
       " 'chefs': 854,\n",
       " 'town': 855,\n",
       " 'fed': 856,\n",
       " 'them': 857,\n",
       " 'duckling': 858,\n",
       " 'mousseline': 859,\n",
       " 'lobster': 860,\n",
       " 'consomme': 861,\n",
       " 'veal': 862,\n",
       " 'mignon': 863,\n",
       " 'chocolate': 864,\n",
       " 'terrine': 865,\n",
       " 'raspberry': 866,\n",
       " 'sauce': 867,\n",
       " 'Knowing': 868,\n",
       " 'tasty': 869,\n",
       " 'free': 870,\n",
       " 'meal': 871,\n",
       " 'when': 872,\n",
       " 'eat': 873,\n",
       " 'gave': 874,\n",
       " 'standing': 875,\n",
       " 'ovation': 876,\n",
       " 'CEOs': 877,\n",
       " 'say': 878,\n",
       " 'red-carpet': 879,\n",
       " 'treatment': 880,\n",
       " 'tempts': 881,\n",
       " 'return': 882,\n",
       " 'heartland': 883,\n",
       " 'future': 884,\n",
       " 'looking': 885,\n",
       " 'forward': 886,\n",
       " 'winter': 887,\n",
       " 'February': 888,\n",
       " 'South': 889,\n",
       " 'Korea': 890,\n",
       " 'registered': 891,\n",
       " 'trade': 892,\n",
       " 'deficit': 893,\n",
       " '101': 894,\n",
       " 'October': 895,\n",
       " 'reflecting': 896,\n",
       " 'country': 897,\n",
       " 'economic': 898,\n",
       " 'sluggishness': 899,\n",
       " 'figures': 900,\n",
       " 'released': 901,\n",
       " 'Wednesday': 902,\n",
       " 'Preliminary': 903,\n",
       " 'tallies': 904,\n",
       " 'Trade': 905,\n",
       " 'Industry': 906,\n",
       " 'Ministry': 907,\n",
       " 'showed': 908,\n",
       " 'fifth': 909,\n",
       " 'monthly': 910,\n",
       " 'setback': 911,\n",
       " 'casting': 912,\n",
       " 'cloud': 913,\n",
       " 'export-oriented': 914,\n",
       " 'economy': 915,\n",
       " 'Exports': 916,\n",
       " 'stood': 917,\n",
       " '5.29': 918,\n",
       " 'mere': 919,\n",
       " '0.7': 920,\n",
       " 'increase': 921,\n",
       " 'while': 922,\n",
       " 'imports': 923,\n",
       " 'increased': 924,\n",
       " 'sharply': 925,\n",
       " '5.39': 926,\n",
       " 'last': 927,\n",
       " 'boom': 928,\n",
       " '*T*-12': 929,\n",
       " 'began': 930,\n",
       " '1986': 931,\n",
       " 'prolonged': 932,\n",
       " 'labor': 933,\n",
       " 'disputes': 934,\n",
       " 'conflicts': 935,\n",
       " 'sluggish': 936,\n",
       " 'exports': 937,\n",
       " 'Government': 938,\n",
       " 'would': 939,\n",
       " 'remain': 940,\n",
       " 'target': 941,\n",
       " '68': 942,\n",
       " 'gloomy': 943,\n",
       " 'forecast': 944,\n",
       " 'recorded': 945,\n",
       " 'surplus': 946,\n",
       " '71': 947,\n",
       " 'January': 948,\n",
       " 'accumulated': 949,\n",
       " '4': 950,\n",
       " 'same': 951,\n",
       " '50.45': 952,\n",
       " 'Imports': 953,\n",
       " '50.38': 954,\n",
       " '19': 955,\n",
       " 'Newsweek': 956,\n",
       " 'trying': 957,\n",
       " 'keep': 958,\n",
       " 'pace': 959,\n",
       " 'rival': 960,\n",
       " 'Time': 961,\n",
       " 'magazine': 962,\n",
       " 'announced': 963,\n",
       " 'advertising': 964,\n",
       " '1990': 965,\n",
       " 'introduce': 966,\n",
       " 'incentive': 967,\n",
       " 'plan': 968,\n",
       " 'advertisers': 969,\n",
       " 'ad': 970,\n",
       " 'Washington': 971,\n",
       " 'Post': 972,\n",
       " 'second': 973,\n",
       " 'offered': 974,\n",
       " 'Plans': 975,\n",
       " '*T*-13': 976,\n",
       " 'give': 977,\n",
       " 'discounts': 978,\n",
       " 'maintaining': 979,\n",
       " 'increasing': 980,\n",
       " 'spending': 981,\n",
       " 'become': 982,\n",
       " 'permanent': 983,\n",
       " 'fixtures': 984,\n",
       " 'news': 985,\n",
       " 'weeklies': 986,\n",
       " 'underscore': 987,\n",
       " 'fierce': 988,\n",
       " 'competition': 989,\n",
       " 'between': 990,\n",
       " 'Warner': 991,\n",
       " 'Mortimer': 992,\n",
       " 'B.': 993,\n",
       " 'Zuckerman': 994,\n",
       " 'News': 995,\n",
       " 'World': 996,\n",
       " 'Alan': 997,\n",
       " 'Spoon': 998,\n",
       " '5': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOUN': 0,\n",
       " '.': 1,\n",
       " 'NUM': 2,\n",
       " 'ADJ': 3,\n",
       " 'VERB': 4,\n",
       " 'DET': 5,\n",
       " 'ADP': 6,\n",
       " 'CONJ': 7,\n",
       " 'X': 8,\n",
       " 'ADV': 9,\n",
       " 'PRT': 10,\n",
       " 'PRON': 11}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0,\n",
       " 'i': 1,\n",
       " 'e': 2,\n",
       " 'r': 3,\n",
       " 'V': 4,\n",
       " 'n': 5,\n",
       " 'k': 6,\n",
       " ',': 7,\n",
       " '6': 8,\n",
       " '1': 9,\n",
       " 'y': 10,\n",
       " 'a': 11,\n",
       " 's': 12,\n",
       " 'o': 13,\n",
       " 'l': 14,\n",
       " 'd': 15,\n",
       " 'w': 16,\n",
       " 'j': 17,\n",
       " 't': 18,\n",
       " 'h': 19,\n",
       " 'b': 20,\n",
       " 'x': 21,\n",
       " 'c': 22,\n",
       " 'u': 23,\n",
       " 'v': 24,\n",
       " 'N': 25,\n",
       " '.': 26,\n",
       " '2': 27,\n",
       " '9': 28,\n",
       " 'M': 29,\n",
       " 'm': 30,\n",
       " 'f': 31,\n",
       " 'E': 32,\n",
       " 'D': 33,\n",
       " 'p': 34,\n",
       " 'g': 35,\n",
       " 'R': 36,\n",
       " 'A': 37,\n",
       " '5': 38,\n",
       " 'C': 39,\n",
       " 'G': 40,\n",
       " 'F': 41,\n",
       " 'L': 42,\n",
       " '*': 43,\n",
       " '-': 44,\n",
       " 'B': 45,\n",
       " 'K': 46,\n",
       " '3': 47,\n",
       " '0': 48,\n",
       " 'T': 49,\n",
       " 'I': 50,\n",
       " 'Y': 51,\n",
       " \"'\": 52,\n",
       " 'J': 53,\n",
       " '`': 54,\n",
       " 'W': 55,\n",
       " 'q': 56,\n",
       " 'H': 57,\n",
       " 'U': 58,\n",
       " '8': 59,\n",
       " '4': 60,\n",
       " '?': 61,\n",
       " 'z': 62,\n",
       " '&': 63,\n",
       " 'S': 64,\n",
       " '7': 65,\n",
       " '%': 66,\n",
       " '$': 67,\n",
       " ';': 68,\n",
       " 'O': 69,\n",
       " ':': 70,\n",
       " 'Q': 71,\n",
       " 'Z': 72,\n",
       " '\\\\': 73,\n",
       " '/': 74,\n",
       " 'X': 75,\n",
       " '@': 76,\n",
       " '!': 77,\n",
       " '#': 78}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 12408\n",
      "Unique words: 12\n",
      "Unique words: 79\n"
     ]
    }
   ],
   "source": [
    "word_vocab_size = len(word_to_idx)\n",
    "tag_vocab_size = len(tag_to_idx)\n",
    "char_vocab_size = len(char_to_idx)\n",
    "\n",
    "print(\"Unique words: {}\".format(word_vocab_size))\n",
    "print(\"Unique words: {}\".format(tag_vocab_size))\n",
    "print(\"Unique words: {}\".format(char_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_DIM = 1024\n",
    "CHAR_EMBEDDING_DIM = 128\n",
    "WORD_HIDDEN_DIM = 1024\n",
    "CHAR_HIDDEN_DIM = 1024\n",
    "EPOCH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualLSTMTagger(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, word_hidden_dim, char_embedding_dim, \\\n",
    "                char_hidden_dim, word_vocab_size, char_vocab_size, tag_vocab_size):\n",
    "        super(DualLSTMTagger, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)\n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, word_hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(word_hidden_dim, tag_vocab_size)\n",
    "        \n",
    "    def forward(self, sentence, words):\n",
    "        embeds = self.word_embedding(sentence)\n",
    "        char_hidden_final = []\n",
    "        for word in words:\n",
    "            char_embeds = self.char_embedding(word)\n",
    "            _, (char_hidden, char_cell_state) = self.char_lstm(char_embeds.view(len(word), 1, -1))\n",
    "            word_char_hidden_state = char_hidden.view(-1)\n",
    "            char_hidden_final.append(word_char_hidden_state)\n",
    "\n",
    "        char_hidden_final = torch.stack(tuple(char_hidden_final))\n",
    "        combined = torch.cat((embeds, char_hidden_final), 1)\n",
    "        lstm_out, _ = self.lstm(combined.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a check on the model before training.\n",
      " Sentences: \n",
      "everybodyeatthefood.Ikeptlookingoutthewindow,tryingtofindtheoneIwaswaitingfor.\n",
      "[('everybody', 'NOUN'), ('eat', 'NOUN'), ('the', 'NOUN'), ('food', 'NUM'), ('.', 'ADJ'), ('I', 'NOUN'), ('kept', 'ADV'), ('looking', 'NUM'), ('out', 'VERB'), ('the', 'NOUN'), ('window', 'ADV'), (',', 'ADV'), ('trying', 'DET'), ('to', '.'), ('find', 'CONJ'), ('the', 'CONJ'), ('one', 'DET'), ('I', 'CONJ'), ('was', 'NOUN'), ('waiting', 'DET'), ('for', 'CONJ'), ('.', 'ADJ')]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-630d33f3a140>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  words = [ torch.tensor(sequence_to_idx(s[0], char_to_idx), \\\n",
      "<ipython-input-33-630d33f3a140>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = tagged_sentence[:20]\n",
    "\n",
    "model = DualLSTMTagger(WORD_EMBEDDING_DIM, WORD_HIDDEN_DIM, CHAR_EMBEDDING_DIM,\n",
    "                      CHAR_HIDDEN_DIM, word_vocab_size, char_vocab_size, tag_vocab_size)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "#test sentence\n",
    "seq = 'everybody eat the food .  I kept looking out the window , \\\n",
    "trying to find the one I was waiting for .'.split()\n",
    "\n",
    "print('Running a check on the model before training.\\n Sentences: \\n{}'.format(''.join(seq)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    words = [ torch.tensor(sequence_to_idx(s[0], char_to_idx), \\\n",
    "                          dtype = torch.long) for s in seq ]\n",
    "    sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), \\\n",
    "                           dtype = torch.long)\n",
    "    \n",
    "    tag_scores = model(sentence, words)\n",
    "    _, indices = torch.max(tag_scores, 1)\n",
    "    ret = []\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        for key, value in tag_to_idx.items():\n",
    "            if indices[i] == value:\n",
    "                ret.append((seq[i], key))\n",
    "    print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-dd7f8ca75f2a>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  words = [ torch.tensor(sequence_to_idx(s[0], char_to_idx), \\\n",
      "<ipython-input-34-dd7f8ca75f2a>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence = torch.tensor(sequence_to_idx(sentence, word_to_idx), \\\n",
      "<ipython-input-34-dd7f8ca75f2a>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(sequence_to_idx(targets, tag_to_idx), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Running:\t Iteration 1 Complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-dd7f8ca75f2a>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  acc += torch.mean(torch.tensor(targets == indices, dtype=torch.float))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed, Loss : 0.09988049417734146, \tAccuracy: 0.5399650931358337\n",
      "Epoch 2 completed, Loss : 0.07337447628378868, \tAccuracy: 0.6582813858985901\n"
     ]
    }
   ],
   "source": [
    "print('Training started')\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "interval = round(len(train) / 100)\n",
    "e_interval = round(EPOCH/10)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    acc = 0\n",
    "    loss = 0\n",
    "    i = 0\n",
    "    \n",
    "    for sentence_tag in train:\n",
    "        i += 1\n",
    "        words = [ torch.tensor(sequence_to_idx(s[0], char_to_idx), \\\n",
    "                              dtype = torch.long) for s in sentence_tag]\n",
    "        sentence = [s[0] for s in sentence_tag]\n",
    "        sentence = torch.tensor(sequence_to_idx(sentence, word_to_idx), \\\n",
    "                               dtype=torch.long)\n",
    "        targets = [s[1] for s in sentence_tag]\n",
    "        targets = torch.tensor(sequence_to_idx(targets, tag_to_idx), \\\n",
    "                              dtype=torch.long)\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence, words)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss.item()\n",
    "        _, indices = torch.max(tag_scores, 1)\n",
    "        acc += torch.mean(torch.tensor(targets == indices, dtype=torch.float))\n",
    "        # if i % interval == 0\n",
    "        print(\"Epoch {} Running:\\t Iteration {} Complete\".format(epoch + 1, i), end = '\\r', flush=True)\n",
    "    loss = loss / len(train)\n",
    "    acc = acc / len(train)\n",
    "    loss_list.append(float(loss))\n",
    "    accuracy_list.append(float(acc))\n",
    "    print(\"Epoch {} completed, Loss : {}, \\tAccuracy: {}\".format(epoch + 1, np.mean(loss_list), np.mean(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('everybody', 'PRON'), ('eat', 'PRON'), ('the', 'DET'), ('food', 'NOUN'), ('.', '.'), ('I', 'NUM'), ('kept', '.'), ('looking', 'VERB'), ('out', 'VERB'), ('the', 'DET'), ('window', 'NOUN'), (',', '.'), ('trying', 'VERB'), ('to', 'PRT'), ('find', 'DET'), ('the', 'DET'), ('one', 'DET'), ('I', 'NOUN'), ('was', 'VERB'), ('waiting', 'DET'), ('for', 'ADP'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-e5832f77e96d>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  words = [ torch.tensor(sequence_to_idx(s[0], char_to_idx), \\\n",
      "<ipython-input-35-e5832f77e96d>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), \\\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    words = [ torch.tensor(sequence_to_idx(s[0], char_to_idx), \\\n",
    "                          dtype = torch.long) for s in seq ]\n",
    "    sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), \\\n",
    "                           dtype = torch.long)\n",
    "    \n",
    "    tag_scores = model(sentence, words)\n",
    "    _, indices = torch.max(tag_scores, 1)\n",
    "    ret = []\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        for key, value in tag_to_idx.items():\n",
    "            if indices[i] == value:\n",
    "                ret.append((seq[i], key))\n",
    "    print(ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
